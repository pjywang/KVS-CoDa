colnames(ROB.TAB) <- c("Prop. Included", "Prop_Numerator",
"Prop_Denominator")
row.names(ROB.TAB) <- x.nam
# BAL.resume
BAL.resume <- matrix(0,nrow =n.fold*n.iter , ncol = ncol(x))
colnames(BAL.resume) <- colnames(x)
n.fold=5
n.iter=10
# BAL.resume
BAL.resume <- matrix(0,nrow =n.fold*n.iter , ncol = ncol(x))
colnames(BAL.resume) <- colnames(x)
# Build a table with the response variable and covariates for correction
if (!is.null(covar)){ dat <- data.frame(cbind(numy, covar))
} else { dat <-data.frame(numy)}
covar=NULL
# Build a table with the response variable and covariates for correction
if (!is.null(covar)){ dat <- data.frame(cbind(numy, covar))
} else { dat <-data.frame(numy)}
# Define log-transformed data with the zero-replacement made
logc <- log(cmultRepl2(x, zero.rep = zero.rep))
# CROSS - VALIDATION groups
CV.groups <- c()
for (i in 1:n.iter){
Outer_cv = mod$StratifiedKFold(n_splits = as.integer(n.fold),
shuffle = T, random_state=as.integer(i))
if(classy=="numeric"){
Outer_cv = mod$KFold(n_splits = as.integer(n.fold),
shuffle = T, random_state=as.integer(i))
}
sep = Outer_cv$split(x, y)
sep <- iterate(sep)
for (j in 1:n.fold){
CV.groups <- rbind(CV.groups, sep[[j]][[1]])
}
}
CV.groups <- CV.groups + 1
suppressMessages(library(pROC))
cv.MSE <- function(k){
#-------------------------------------#
# Necessary matrices for the function
#-------------------------------------#
# Folds associated to a "complete FOLD"
CV.group<-CV.groups[((k-1)*n.fold + 1):(k*n.fold),]
# Build objects
# Table with all the variables included
Bal.List <- list()
# Matrix for MSE values (or ACC "Accuracy")
ACC.Mat <- matrix(0,nrow = maxV-1, ncol = nrow(CV.group))
# For each fold
for (i in 1:nrow(CV.group)){
# Define training data.set
train.idx<-CV.group[i,]
# Training dataset (x, y and covar)
x.train<-logc[train.idx,]
x.test<-logc[-train.idx,]
y.train<-y[train.idx]
y.test<-y[-train.idx]
covar.train<-covar[train.idx,]
covar.test<-covar[-train.idx,]
# Compute the balances for the training data set
BAL <- selbal.aux(x.train, y.train, th.imp = 0, covar = covar.train,
logit.acc, logt=F, maxV = maxV)
# Variables included in the balance (as NUMERATOR | DENOMINATOR)
Bal.List[[i]]<-BAL
# A matrix for predictions for Y
PRED.y <- matrix(0, nrow = length(y.test), ncol = nrow(BAL) - 1)
# For each number of variables (2:nrow(BAL))
for (l in 2:min(maxV,nrow(BAL))){
# Data frame for train data
df <-data.frame(Y = y.train, B = bal.value(BAL[1:l,],x.train))
# Data frame for test data
df.test <- data.frame(Y = y.test, B = bal.value(BAL[1:l,],x.test))
if(!is.null(covar)){
df <- cbind(df,cov=covar.train)
df.test <- cbind(df.test,cov=covar.test)
}
# Regression model for test data
FIT <- glm(Y ~ ., data= df, family = f.class)
# Predictions
PRED.y[,l-1] <- predict(FIT,df.test, type="response")
}
#------------------------------------------------------------------------------#
# FUNCTION: Measure the error value
#------------------------------------------------------------------------------#
ACC.eval <- function(y, pred, classy, logit.acc=NULL){
if (classy == "numeric"){
ACC <- apply(pred, 2, function(x) mean((y-x)^2))
}else{
if (logit.acc == "AUC"){
# Load library
library(pROC)
ACC <- apply(pred, 2, function(x) auc(y,x, quiet=TRUE))
} else if(logit.acc == "Rsq"){
ACC <- apply(pred, 2, function(x) cor (y, x)^2)
} else if (logit.acc == "Tjur"){
ACC <- apply(pred, 2, function(x) mean(x[y==1]) - mean(x[y==0]))
} else if (logit.acc == "Dev"){
ACC<-apply(pred, 2, function(x) 1-(deviance(glm(y ~ x, data= df, family = binomial()))/glm(y~1, family=binomial())[[10]]) )  # proportion of explained deviance
}
}
return(ACC)
}
#------------------------------------------------------------------------------#
# Run ACC.eval function
R <- ACC.eval(numy[-train.idx], PRED.y, classy=classy, logit.acc)
# Add the information
ACC.Mat[,i] <- c(R, rep(R[length(R)], maxV-length(R)-1))
} # End of i
return(list(Bal.List, ACC.Mat))
}
# Build a parallelization scenario
suppressMessages(library(foreach))
suppressMessages(library(doParallel))
# Number of cores of the computer but one
no_cores <- detectCores() - 2
# Register the number of cores
registerDoParallel(no_cores)
# Define the function comb
comb <- function(x, ...) {
lapply(seq_along(x),
function(i) c(x[[i]], lapply(list(...), function(y) y[[i]])))
}
# CV - procedure computed in parallel
INTEREST <- foreach(h=1:n.iter,
.export=c("logit.cor", "rowM","selbal.aux", "bal.value",
"logit.acc", "cmultRepl","cmultRepl2"),
.combine='comb',
.multicombine=TRUE,
.init=list(list(), list())) %dopar% {
tryCatch({cv.MSE(h)}, error = function(e){
return(paste0("Error: ", e))
})
# cv.MSE(h)
}
logit.acc="AUC"
# CV - procedure computed in parallel
INTEREST <- foreach(h=1:n.iter,
.export=c("logit.cor", "rowM","selbal.aux", "bal.value",
"logit.acc", "cmultRepl","cmultRepl2"),
.combine='comb',
.multicombine=TRUE,
.init=list(list(), list())) %dopar% {
tryCatch({cv.MSE(h)}, error = function(e){
return(paste0("Error: ", e))
})
# cv.MSE(h)
}
# Stop the parallelization
stopImplicitCluster()
# Build a parallelization scenario
suppressMessages(library(foreach))
suppressMessages(library(doParallel))
# Number of cores of the computer but one
no_cores <- detectCores() - 2
# Number of cores of the computer but one
no_cores <- min(detectCores() - 2, n.iter)
# Register the number of cores
registerDoParallel(no_cores)
# Define the function comb
comb <- function(x, ...) {
lapply(seq_along(x),
function(i) c(x[[i]], lapply(list(...), function(y) y[[i]])))
}
# CV - procedure computed in parallel
INTEREST <- foreach(h=1:n.iter,
.export=c("logit.cor", "rowM","selbal.aux", "bal.value",
"logit.acc", "cmultRepl","cmultRepl2"),
.combine='comb',
.multicombine=TRUE,
.init=list(list(), list())) %dopar% {
tryCatch({cv.MSE(h)}, error = function(e){
return(paste0("Error: ", e))
})
# cv.MSE(h)
}
# Stop the parallelization
stopImplicitCluster()
# Number of cores of the computer but one
no_cores <- detectCores() - 2
# Register the number of cores
registerDoParallel(no_cores)
cv.MSE(1)
maxV=10
cv.MSE(1)
install.packages('CMA')
cv.MSE(1)
if (!require("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install("CMA")
cv.MSE(1)
# Build a parallelization scenario
suppressMessages(library(foreach))
suppressMessages(library(doParallel))
# Number of cores of the computer but one
no_cores <- detectCores() - 2
# Register the number of cores
registerDoParallel(no_cores)
# Define the function comb
comb <- function(x, ...) {
lapply(seq_along(x),
function(i) c(x[[i]], lapply(list(...), function(y) y[[i]])))
}
# CV - procedure computed in parallel
INTEREST <- foreach(h=1:n.iter,
.export=c("logit.cor", "rowM","selbal.aux", "bal.value",
"logit.acc", "cmultRepl","cmultRepl2"),
.combine='comb',
.multicombine=TRUE,
.init=list(list(), list())) %dopar% {
tryCatch({cv.MSE(h)}, error = function(e){
return(paste0("Error: ", e))
})
# cv.MSE(h)
}
View(INTEREST)
View(INTEREST)
# Stop the parallelization
stopImplicitCluster()
rm(list=ls())
library(knitr) # rbookdown, kable
library(glmnet) # glmnet
# library(selbal) # selbal
library(ggplot2) # draw selbal
library(gridExtra) # grid.arrange
library(UpSetR) # upset
library(ggforce) # selbal-like plot
library(grid) # grid.draw
library(readxl) # Read excel files: install.packages("readxl")
library(MLmetrics) # To use F1_score function: install.packages("MLmetrics")
library(pracma) # to use logspace, logseq function
source(file = './CoDA-Penalized-Regression/R/functions_coda_penalized_regression.R')
source(file = './CoDA-Penalized-Regression/R/functions.R')
source(file = './selbal/R/Selbal_Functions.R')
source(file = './selbal/R/data.R')
library(reticulate)
# use_python("C:/Users/Junyoung Park/anaconda3/python.exe")
use_python("/home/users/pjywang/miniconda3/envs/R/bin/python3.11")
pd <- import("pandas")
np <- import("numpy")
mod <- import("sklearn.model_selection")
source(file = './tools.R')
filepath = "C:/Users/Junyoung Park/OneDrive/문서/Stats/CoDA/Experiments/DimReduction/datasets/COMBO_data/"
count <- read.csv(paste0(filepath, "complete_data/GeneraCounts.csv"), header = F)
tax_table <- t(read.csv(paste0(filepath, "complete_data/GeneraPhylo.csv"), header = F))
y <- read.csv(paste0(filepath, "BMI.csv"), header = F)
y <- y$V1
x <- t(count)
colnames(x) <- rownames(count)
x <- zero_rep(x, method="xmin", val=0.5)
selbal_bmi <- myselbal.cv(x = x, y = y, n.fold = 5, n.iter = 10,
covar = NULL, maxV = 10)
View(selbal_bmi)
selbal_bmi[["cv.accuracy"]]
selbal_bmi[["accuracy.nvar"]][["data"]][["mean"]]
source(file = './CoDA-Penalized-Regression/R/functions_coda_penalized_regression.R')
source(file = './CoDA-Penalized-Regression/R/functions.R')
source(file = './selbal/R/Selbal_Functions.R')
source(file = './selbal/R/data.R')
# To read *.pickle file in R
library(reticulate)
# NEED TO RECONCILE PANDAS VERSION.. to 1.5.2 to all my computers!!
pd <- import("pandas")
pd$show_versions
i=1
filename = paste0(dirname, "200_100_log5_", i - 1, ".pickle")
pickle_data <- pd$read_pickle(filename)
# Directory that synthetic data are stored
dirname = "./Synthetic_data/"
filename = paste0(dirname, "200_100_log5_", i - 1, ".pickle")
pickle_data <- pd$read_pickle(filename)
pickle_data$Y[pickle_data$Y < 0] = 0
View(pickle_data)
for (i in 1:50){
filename = paste0(dirname, "200_100_log5_", i - 1, ".pickle")
pickle_data <- pd$read_pickle(filename)
pickle_data$Y[pickle_data$Y < 0] = 0
cat('(', nrow(pickle_data$X), ncol(pickle_data$X), ')\n', sep = "")
}
# imports
library(pracma) # to use logspace, logseq function
if(!require(parallel)) {
install.packages("parallel")
}
library(parallel)
if(!require(foreach)) {
install.packages("foreach")
}
library(foreach)
if(!require(doParallel)) {
install.packages("doParallel")
}
library(doParallel)
numCores <- parallel::detectCores() - 1
colnames(pickle_data$X)
# Parallel computation
numCores <- detectCores() - 2
len_lamseq = 3
lamseq = logseq(0.03, 0.19, 12 * len_lamseq)
# Parallel computation
numCores <- detectCores() - 2
myCluster <- makeCluster(numCores)
registerDoParallel(myCluster)
results <- foreach(i = 1:50,
.multicombine = TRUE ) %:%
foreach(j = 1:8) %dopar% {
pickle_data <- pd$read_pickle(filename)
pickle_data$Y[pickle_data$Y < 0] = 0
# 0.5x_min replacement
pickle_data$X[pickle_data$X == 0] = 0.5
findlam = j * 5
lamseq_j = lamseq[((8-j) * len_lamseq + 1 ): ((13-j) * len_lamseq)]
lambdaRange_codalasso(X = pickle_data$X, y = pickle_data$Y,
lambdaSeq = lamseq_j,
findlambda = findlam, verbose = F)
}
results <- foreach(i = 1:50,
.multicombine = TRUE,
.export = 'lambdaRange_codalasso') %:%
foreach(j = 1:8) %dopar% {
pickle_data <- pd$read_pickle(filename)
pickle_data$Y[pickle_data$Y < 0] = 0
# 0.5x_min replacement
pickle_data$X[pickle_data$X == 0] = 0.5
findlam = j * 5
lamseq_j = lamseq[((8-j) * len_lamseq + 1 ): ((13-j) * len_lamseq)]
lambdaRange_codalasso(X = pickle_data$X, y = pickle_data$Y,
lambdaSeq = lamseq_j,
findlambda = findlam, verbose = F)
}
stopCluster(myCluster)
registerDoParallel(myCluster)
results <- foreach(i = 1:50,
.multicombine = TRUE,
.export = 'lambdaRange_codalasso') %:%
foreach(j = 1:8) %dopar% {
filename = paste0(dirname, "200_100_log5_", i - 1, ".pickle")
pickle_data <- pd$read_pickle(filename)
pickle_data$Y[pickle_data$Y < 0] = 0
# 0.5x_min replacement
pickle_data$X[pickle_data$X == 0] = 0.5
findlam = j * 5
lamseq_j = lamseq[((8-j) * len_lamseq + 1 ): ((13-j) * len_lamseq)]
lambdaRange_codalasso(X = pickle_data$X, y = pickle_data$Y,
lambdaSeq = lamseq_j,
findlambda = findlam, verbose = F)
}
stopCluster(myCluster)
# Parallel computation
numCores <- detectCores() - 2
myCluster <- makeCluster(numCores)
registerDoParallel(myCluster)
results <- foreach(i = 1:50,
.multicombine = TRUE) %:%
foreach(j = 1:8) %dopar% {
filename = paste0(dirname, "200_100_log5_", i - 1, ".pickle")
pickle_data <- pd$read_pickle(filename)
pickle_data$Y[pickle_data$Y < 0] = 0
# 0.5x_min replacement
pickle_data$X[pickle_data$X == 0] = 0.5
findlam = j * 5
lamseq_j = lamseq[((8-j) * len_lamseq + 1 ): ((13-j) * len_lamseq)]
lambdaRange_codalasso(X = pickle_data$X, y = pickle_data$Y,
lambdaSeq = lamseq_j,
findlambda = findlam, verbose = F)
}
stopCluster(myCluster)
# imports
library(pracma) # to use logspace, logseq function
# For parallel computation
library(foreach)
library(doParallel)
setwd("/home/users/pjywang/Desktop/DimReduction/ConstrainedLASSO")
source(file = './CoDA-Penalized-Regression/R/functions_coda_penalized_regression.R')
source(file = './CoDA-Penalized-Regression/R/functions.R')
source(file = './selbal/R/Selbal_Functions.R')
source(file = './selbal/R/data.R')
# To read *.pickle file in R
library(reticulate)
# use_python("C:/Users/Junyoung Park/anaconda3/python.exe")
use_python("/home/users/pjywang/miniconda3/envs/R/bin/python3.11")
# PANDAS VERSION: 1.5.2 (necessary)
pd <- import("pandas")
# Directory that synthetic data are stored
dirname = "./Synthetic_data/"
len_lamseq = 3
lamseq = logseq(0.03, 0.19, 12 * len_lamseq)
# Parallel computation
numCores <- detectCores() - 2
myCluster <- makeCluster(numCores)
registerDoParallel(myCluster)
results <- foreach(i = 1:50,
.multicombine = TRUE) %:%
foreach(j = 1:8) %dopar% {
filename <- paste0(dirname, "200_100_log5_", i - 1, ".pickle")
pickle_data <- pd$read_pickle(filename)
pickle_data$Y[pickle_data$Y < 0] <- 0
# 0.5x_min replacement
pickle_data$X[pickle_data$X == 0] <- 0.5
findlam <- j * 5
lamseq_j <- lamseq[((8-j) * len_lamseq + 1 ): ((13-j) * len_lamseq)]
lambdaRange_codalasso(X = pickle_data$X, y = pickle_data$Y,
lambdaSeq = lamseq_j,
findlambda = findlam, verbose = F)
}
stopCluster(myCluster)
rm(list=ls())
# imports
library(pracma) # to use logspace, logseq function
# For parallel computation
library(foreach)
library(doParallel)
setwd("/home/users/pjywang/Desktop/DimReduction/ConstrainedLASSO")
source(file = './CoDA-Penalized-Regression/R/functions_coda_penalized_regression.R')
source(file = './CoDA-Penalized-Regression/R/functions.R')
source(file = './selbal/R/Selbal_Functions.R')
source(file = './selbal/R/data.R')
# To read *.pickle file in R
library(reticulate)
# use_python("C:/Users/Junyoung Park/anaconda3/python.exe")
use_python("/home/users/pjywang/miniconda3/envs/R/bin/python3.11")
# PANDAS VERSION: 1.5.2 (necessary)
pd <- import("pandas")
data_list <- list()
for (i in 1:50){
filename <- paste0(dirname, "200_100_log5_", i - 1, ".pickle")
pickle_data <- pd$read_pickle(filename)
pickle_data$Y[pickle_data$Y < 0] <- 0
# 0.5x_min replacement
pickle_data$X[pickle_data$X == 0] <- 0.5
data_list[[i]] = pickle_data
}
# Directory that synthetic data are stored
dirname = "./Synthetic_data/"
data_list <- list()
for (i in 1:50){
filename <- paste0(dirname, "200_100_log5_", i - 1, ".pickle")
pickle_data <- pd$read_pickle(filename)
pickle_data$Y[pickle_data$Y < 0] <- 0
# 0.5x_min replacement
pickle_data$X[pickle_data$X == 0] <- 0.5
data_list[[i]] = pickle_data
}
View(data_list)
len_lamseq = 3
lamseq = logseq(0.03, 0.19, 12 * len_lamseq)
len_lamseq = 3
lamseq = logseq(0.03, 0.19, 12 * len_lamseq)
# Parallel computation setting
numCores <- detectCores() - 2
myCluster <- makeCluster(numCores)
registerDoParallel(myCluster)
results <- foreach(i = 1:50,
.multicombine = TRUE) %:%
foreach(j = 1:8) %dopar% {
findlam <- j * 5
lamseq_j <- lamseq[((8-j) * len_lamseq + 1 ): ((13-j) * len_lamseq)]
lambdaRange_codalasso(X = pickle_data$X, y = pickle_data$Y,
lambdaSeq = lamseq_j,
findlambda = findlam, verbose = F)
}
stopCluster(myCluster)
len_lamseq = 3
lamseq = logseq(0.03, 0.19, 12 * len_lamseq)
# Parallel computation setting
numCores <- detectCores() - 2
myCluster <- makeCluster(numCores)
registerDoParallel(myCluster)
results <- foreach(i = 1:10,
.multicombine = TRUE) %:%
foreach(j = 1:3) %dopar% {
findlam <- j * 5
lamseq_j <- lamseq[((8-j) * len_lamseq + 1 ): ((13-j) * len_lamseq)]
X <- data_list[[i]]$X
Y <- data_list[[i]]$Y
lambdaRange_codalasso(X = X, y = Y, lambdaSeq = lamseq_j,
findlambda = findlam, verbose = F)
}
View(results)
stopCluster(myCluster)
library(knitr) # rbookdown, kable
library(glmnet) # glmnet
# library(selbal) # selbal
library(ggplot2) # draw selbal
library(gridExtra) # grid.arrange
library(UpSetR) # upset
library(ggforce) # selbal-like plot
library(grid) # grid.draw
library(readxl) # Read excel files: install.packages("readxl")
library(MLmetrics) # To use F1_score function: install.packages("MLmetrics")
library(pracma) # to use logspace, logseq function
source(file = './CoDA-Penalized-Regression/R/functions_coda_penalized_regression.R')
source(file = './CoDA-Penalized-Regression/R/functions.R')
source(file = './selbal/R/Selbal_Functions.R')
source(file = './selbal/R/data.R')
library(reticulate)
pd <- import("pandas")
np <- import("numpy")
mod <- import("sklearn.model_selection")
source(file = './tools.R')
# Load and preprocess
filepath = "C:/Users/user/OneDrive/문서/Stats/CoDA/Experiments/DimReduction/datasets/COMBO_data/"
count <- read.csv(paste0(filepath, "complete_data/GeneraCounts.csv"), header = F)
tax_table <- t(read.csv(paste0(filepath, "complete_data/GeneraPhylo.csv"), header = F))
y <- read.csv(paste0(filepath, "BMI.csv"), header = F)
y <- y$V1
x <- t(count)
colnames(x) <- rownames(count)
x <- zero_rep(x, method="xmin", val=0.5)
selbal_bmi <- myselbal.cv(x = x, y = y, n.fold = 5, n.iter = 10,
covar = NULL, maxV = 10)
View(selbal_bmi)
selbal_bmi[["accuracy.nvar"]][["data"]]$se / sqrt(10)
