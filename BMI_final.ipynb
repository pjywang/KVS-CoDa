{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6efd6086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d686c12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-23 00:53:08.280584: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-23 00:53:08.417272: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-23 00:53:09.980308: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/python/3.7.2-tf/lib:/opt/gcc/5.4.0/lib64:/home/apps/cuda-10.2/lib64:/home/apps/cuda-10.2/lib:/home/apps/cuda-10.2/extras/CUPTI/lib64:/home/apps/intel/mpi/2021.5.1/libfabric/lib:/home/apps/intel/mpi/2021.5.1/lib/release:/home/apps/intel/mpi/2021.5.1/lib:/home/apps/intel/mkl/2022.0.2/lib/intel64:/home/apps/intel/compiler/2022.0.2/linux/lib/oclfpga/host/linux64/lib:/home/apps/intel/compiler/2022.0.2/linux/lib:/home/apps/intel/compiler/2022.0.2/linux/lib/x64:/home/apps/intel/compiler/2022.0.2/linux/compiler/lib/intel64_lin:/home/apps/intel/tbb/2021.5.1/lib/intel64/gcc4.8:/home/apps/intel/debugger/2021.5.0/gdb/intel64/lib:/home/apps/intel/debugger/2021.5.0/libipt/intel64/lib:/home/apps/intel/debugger/2021.5.0/dep/lib:/home/apps/cuda-12.1/lib64:/home/apps/cuda-12.1/extras/CUPTI/lib64:/opt/intel/mkl/8.1.1/lib/em64t:/opt/intel/mkl/8.1.1/lib/em64t:/home/users/pjywang/miniconda3/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib\n",
      "2023-05-23 00:53:09.980425: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/python/3.7.2-tf/lib:/opt/gcc/5.4.0/lib64:/home/apps/cuda-10.2/lib64:/home/apps/cuda-10.2/lib:/home/apps/cuda-10.2/extras/CUPTI/lib64:/home/apps/intel/mpi/2021.5.1/libfabric/lib:/home/apps/intel/mpi/2021.5.1/lib/release:/home/apps/intel/mpi/2021.5.1/lib:/home/apps/intel/mkl/2022.0.2/lib/intel64:/home/apps/intel/compiler/2022.0.2/linux/lib/oclfpga/host/linux64/lib:/home/apps/intel/compiler/2022.0.2/linux/lib:/home/apps/intel/compiler/2022.0.2/linux/lib/x64:/home/apps/intel/compiler/2022.0.2/linux/compiler/lib/intel64_lin:/home/apps/intel/tbb/2021.5.1/lib/intel64/gcc4.8:/home/apps/intel/debugger/2021.5.0/gdb/intel64/lib:/home/apps/intel/debugger/2021.5.0/libipt/intel64/lib:/home/apps/intel/debugger/2021.5.0/dep/lib:/home/apps/cuda-12.1/lib64:/home/apps/cuda-12.1/extras/CUPTI/lib64:/opt/intel/mkl/8.1.1/lib/em64t:/opt/intel/mkl/8.1.1/lib/em64t:/home/users/pjywang/miniconda3/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib:/home/users/pjywang/miniconda3/envs/DimRed/lib/:/lib\n",
      "2023-05-23 00:53:09.980436: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12500391619598031885\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 23367450624\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 9175531477664728787\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:18:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 23367450624\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 13899808925466518644\n",
      "physical_device_desc: \"device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:51:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 2144165316\n",
      ", name: \"/device:GPU:2\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 23367450624\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 15749649558754070239\n",
      "physical_device_desc: \"device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:8a:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 1651660799\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-23 00:53:12.300956: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-23 00:53:15.608600: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:0 with 22284 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:18:00.0, compute capability: 8.6\n",
      "2023-05-23 00:53:15.609028: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:1 with 22284 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:51:00.0, compute capability: 8.6\n",
      "2023-05-23 00:53:15.609301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:2 with 22284 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:8a:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import ccm_coda, kernel\n",
    "import kernel\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle, os\n",
    "\n",
    "from tools import * # we use cv_score, zero_replacement functions here\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6845c3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import composition_stats as cp\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold, StratifiedKFold, train_test_split, LeaveOneOut\n",
    "from sklearn.svm import SVC, LinearSVC, SVR\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import accuracy_score, get_scorer, r2_score, mean_squared_error\n",
    "\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f32ab44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d6ff769",
   "metadata": {},
   "source": [
    "## COMBO dataset\n",
    "Dataset downloaded from https://c-lasso.readthedocs.io/en/latest/auto_examples/plot_example_COMBO.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f214d8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OTU_1</th>\n",
       "      <th>OTU_2</th>\n",
       "      <th>OTU_3</th>\n",
       "      <th>OTU_4</th>\n",
       "      <th>OTU_5</th>\n",
       "      <th>OTU_6</th>\n",
       "      <th>OTU_7</th>\n",
       "      <th>OTU_8</th>\n",
       "      <th>OTU_9</th>\n",
       "      <th>OTU_10</th>\n",
       "      <th>...</th>\n",
       "      <th>OTU_78</th>\n",
       "      <th>OTU_79</th>\n",
       "      <th>OTU_80</th>\n",
       "      <th>OTU_81</th>\n",
       "      <th>OTU_82</th>\n",
       "      <th>OTU_83</th>\n",
       "      <th>OTU_84</th>\n",
       "      <th>OTU_85</th>\n",
       "      <th>OTU_86</th>\n",
       "      <th>OTU_87</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "      <td>...</td>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "      <td>Bacteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Bacteroidetes</td>\n",
       "      <td>Bacteroidetes</td>\n",
       "      <td>Bacteroidetes</td>\n",
       "      <td>...</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Proteobacteria</td>\n",
       "      <td>Synergistetes</td>\n",
       "      <td>Synergistetes</td>\n",
       "      <td>Verrucomicrobia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Actinobacteria</td>\n",
       "      <td>Bacteroidia</td>\n",
       "      <td>Bacteroidia</td>\n",
       "      <td>Bacteroidia</td>\n",
       "      <td>...</td>\n",
       "      <td>Betaproteobacteria</td>\n",
       "      <td>Betaproteobacteria</td>\n",
       "      <td>Deltaproteobacteria</td>\n",
       "      <td>Epsilonproteobacteria</td>\n",
       "      <td>Gammaproteobacteria</td>\n",
       "      <td>Gammaproteobacteria</td>\n",
       "      <td>Gammaproteobacteria</td>\n",
       "      <td>Synergistia</td>\n",
       "      <td>Synergistia</td>\n",
       "      <td>Verrucomicrobiae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Coriobacteriales</td>\n",
       "      <td>Coriobacteriales</td>\n",
       "      <td>Coriobacteriales</td>\n",
       "      <td>Coriobacteriales</td>\n",
       "      <td>Coriobacteriales</td>\n",
       "      <td>Coriobacteriales</td>\n",
       "      <td>Coriobacteriales</td>\n",
       "      <td>Bacteroidales</td>\n",
       "      <td>Bacteroidales</td>\n",
       "      <td>Bacteroidales</td>\n",
       "      <td>...</td>\n",
       "      <td>Burkholderiales</td>\n",
       "      <td>Neisseriales</td>\n",
       "      <td>Desulfovibrionales</td>\n",
       "      <td>Campylobacterales</td>\n",
       "      <td>Aeromonadales</td>\n",
       "      <td>Pseudomonadales</td>\n",
       "      <td>Xanthomonadales</td>\n",
       "      <td>Synergistales</td>\n",
       "      <td>Synergistales</td>\n",
       "      <td>Verrucomicrobiales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Coriobacteriaceae</td>\n",
       "      <td>Coriobacteriaceae</td>\n",
       "      <td>Coriobacteriaceae</td>\n",
       "      <td>Coriobacteriaceae</td>\n",
       "      <td>Coriobacteriaceae</td>\n",
       "      <td>Coriobacteriaceae</td>\n",
       "      <td>Coriobacteriaceae</td>\n",
       "      <td>Bacteroidaceae</td>\n",
       "      <td>Porphyromonadaceae</td>\n",
       "      <td>Porphyromonadaceae</td>\n",
       "      <td>...</td>\n",
       "      <td>Oxalobacteraceae</td>\n",
       "      <td>Neisseriaceae</td>\n",
       "      <td>Desulfovibrionaceae</td>\n",
       "      <td>Campylobacteraceae</td>\n",
       "      <td>Succinivibrionaceae</td>\n",
       "      <td>Pseudomonadaceae</td>\n",
       "      <td>Xanthomonadaceae</td>\n",
       "      <td>Synergistaceae</td>\n",
       "      <td>Synergistaceae</td>\n",
       "      <td>Verrucomicrobiaceae</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Asaccharobacter</td>\n",
       "      <td>Atopobium</td>\n",
       "      <td>Collinsella</td>\n",
       "      <td>Eggerthella</td>\n",
       "      <td>Gordonibacter</td>\n",
       "      <td>Olsenella</td>\n",
       "      <td>Slackia</td>\n",
       "      <td>Bacteroides</td>\n",
       "      <td>Barnesiella</td>\n",
       "      <td>Butyricimonas</td>\n",
       "      <td>...</td>\n",
       "      <td>Oxalobacter</td>\n",
       "      <td>Neisseria</td>\n",
       "      <td>Desulfovibrio</td>\n",
       "      <td>Campylobacter</td>\n",
       "      <td>Succinivibrio</td>\n",
       "      <td>Pseudomonas</td>\n",
       "      <td>Stenotrophomonas</td>\n",
       "      <td>Cloacibacillus</td>\n",
       "      <td>Pyramidobacter</td>\n",
       "      <td>Akkermansia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 87 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0               OTU_1               OTU_2               OTU_3  \\\n",
       "1            Bacteria            Bacteria            Bacteria   \n",
       "2      Actinobacteria      Actinobacteria      Actinobacteria   \n",
       "3      Actinobacteria      Actinobacteria      Actinobacteria   \n",
       "4    Coriobacteriales    Coriobacteriales    Coriobacteriales   \n",
       "5   Coriobacteriaceae   Coriobacteriaceae   Coriobacteriaceae   \n",
       "6     Asaccharobacter           Atopobium         Collinsella   \n",
       "\n",
       "0               OTU_4               OTU_5               OTU_6  \\\n",
       "1            Bacteria            Bacteria            Bacteria   \n",
       "2      Actinobacteria      Actinobacteria      Actinobacteria   \n",
       "3      Actinobacteria      Actinobacteria      Actinobacteria   \n",
       "4    Coriobacteriales    Coriobacteriales    Coriobacteriales   \n",
       "5   Coriobacteriaceae   Coriobacteriaceae   Coriobacteriaceae   \n",
       "6         Eggerthella       Gordonibacter           Olsenella   \n",
       "\n",
       "0               OTU_7            OTU_8                OTU_9  \\\n",
       "1            Bacteria         Bacteria             Bacteria   \n",
       "2      Actinobacteria    Bacteroidetes        Bacteroidetes   \n",
       "3      Actinobacteria      Bacteroidia          Bacteroidia   \n",
       "4    Coriobacteriales    Bacteroidales        Bacteroidales   \n",
       "5   Coriobacteriaceae   Bacteroidaceae   Porphyromonadaceae   \n",
       "6             Slackia      Bacteroides          Barnesiella   \n",
       "\n",
       "0               OTU_10  ...               OTU_78               OTU_79  \\\n",
       "1             Bacteria  ...             Bacteria             Bacteria   \n",
       "2        Bacteroidetes  ...       Proteobacteria       Proteobacteria   \n",
       "3          Bacteroidia  ...   Betaproteobacteria   Betaproteobacteria   \n",
       "4        Bacteroidales  ...      Burkholderiales         Neisseriales   \n",
       "5   Porphyromonadaceae  ...     Oxalobacteraceae        Neisseriaceae   \n",
       "6        Butyricimonas  ...          Oxalobacter            Neisseria   \n",
       "\n",
       "0                OTU_80                  OTU_81                OTU_82  \\\n",
       "1              Bacteria                Bacteria              Bacteria   \n",
       "2        Proteobacteria          Proteobacteria        Proteobacteria   \n",
       "3   Deltaproteobacteria   Epsilonproteobacteria   Gammaproteobacteria   \n",
       "4    Desulfovibrionales       Campylobacterales         Aeromonadales   \n",
       "5   Desulfovibrionaceae      Campylobacteraceae   Succinivibrionaceae   \n",
       "6         Desulfovibrio           Campylobacter         Succinivibrio   \n",
       "\n",
       "0                OTU_83                OTU_84           OTU_85  \\\n",
       "1              Bacteria              Bacteria         Bacteria   \n",
       "2        Proteobacteria        Proteobacteria    Synergistetes   \n",
       "3   Gammaproteobacteria   Gammaproteobacteria      Synergistia   \n",
       "4       Pseudomonadales       Xanthomonadales    Synergistales   \n",
       "5      Pseudomonadaceae      Xanthomonadaceae   Synergistaceae   \n",
       "6           Pseudomonas      Stenotrophomonas   Cloacibacillus   \n",
       "\n",
       "0           OTU_86                OTU_87  \n",
       "1         Bacteria              Bacteria  \n",
       "2    Synergistetes       Verrucomicrobia  \n",
       "3      Synergistia      Verrucomicrobiae  \n",
       "4    Synergistales    Verrucomicrobiales  \n",
       "5   Synergistaceae   Verrucomicrobiaceae  \n",
       "6   Pyramidobacter           Akkermansia  \n",
       "\n",
       "[6 rows x 87 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = pd.read_csv('./datasets/COMBO_data/complete_data/GeneraCounts.csv', header=None)\n",
    "count = count.transpose()\n",
    "\n",
    "tax_table = pd.read_csv('./datasets/COMBO_data/complete_data/GeneraPhylo.csv', header=None)\n",
    "tax_table = tax_table.transpose()\n",
    "tax_table.columns = tax_table.iloc[0]\n",
    "tax_table.drop(tax_table.index[0], inplace=True)\n",
    "tax_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3655cbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero ratio: 0.7209051724137931\n"
     ]
    }
   ],
   "source": [
    "# BMI as response\n",
    "Y = pd.read_csv('./datasets/COMBO_data/BMI.csv', header=None)\n",
    "Y = np.array(Y).ravel()\n",
    "\n",
    "# Zero ratio, make composotional\n",
    "X = np.array(count, dtype=float)\n",
    "print(\"zero ratio:\", np.sum(X == 0) / X.size)\n",
    "X = X / np.sum(X, axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970bc208",
   "metadata": {},
   "source": [
    "To make comparison with GBM zero replacement, we delete columns with only one nonzero value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d81971d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 77)\n",
      "zero ratio: 0.6860119047619048\n"
     ]
    }
   ],
   "source": [
    "valid_idx = np.sum(X > 0, axis=0) > 1\n",
    "\n",
    "X = np.array(count.iloc[:, valid_idx], dtype=float)\n",
    "print(X.shape)\n",
    "print(\"zero ratio:\", np.sum(X == 0) / X.size)\n",
    "X = X / np.sum(X, axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a03c8d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "956bb7ba",
   "metadata": {},
   "source": [
    "## CV-MSE computation in conjunction with Kernel Ridge Regression\n",
    "Rewrite this section using the Section \"Using Column-deleted Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df0e5e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1 th cv experiment...\n",
      "\n",
      " Fold 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-23 00:03:28.581812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22284 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:18:00.0, compute capability: 8.6\n",
      "2023-05-23 00:03:28.582592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22284 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:51:00.0, compute capability: 8.6\n",
      "2023-05-23 00:03:28.582748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22284 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:8a:00.0, compute capability: 8.6\n",
      "2023-05-23 00:03:32.357906: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-05-23 00:03:32.383086: I tensorflow/core/util/cuda_solvers.cc:179] Creating GpuSolver handles for stream 0x46b35560\n",
      "2023-05-23 00:03:40.077517: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x2b8bd148db30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-05-23 00:03:40.077835: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2023-05-23 00:03:40.077841: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (1): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2023-05-23 00:03:40.077846: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (2): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2023-05-23 00:03:40.658929: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration 100 loss 229.63062\n",
      "iteration 200 loss 227.80814\n",
      "iteration 300 loss 227.47948\n",
      "iteration 400 loss 227.3699\n",
      "iteration 500 loss 227.32782\n",
      "iteration 600 loss 227.30269\n",
      "iteration 700 loss 227.28696\n",
      "iteration 800 loss 227.27676\n",
      "iteration 900 loss 227.2699\n",
      "iteration 1000 loss 227.2653\n",
      "iteration 1100 loss 227.2621\n",
      "iteration 1200 loss 227.25987\n",
      "iteration 1300 loss 227.2583\n",
      "iteration 1400 loss 227.25723\n",
      "iteration 1500 loss 227.25644\n",
      "iteration 1600 loss 227.25589\n",
      "iteration 1700 loss 227.25548\n",
      "iteration 1800 loss 227.25522\n",
      "iteration 1900 loss 227.25504\n",
      "iteration 2000 loss 227.25484\n",
      "iteration 2100 loss 227.25476\n",
      "iteration 2200 loss 227.2547\n",
      "iteration 2300 loss 227.25467\n",
      "iteration 2400 loss 227.25462\n",
      "iteration 2500 loss 227.2546\n",
      "iteration 2600 loss 227.25458\n",
      "iteration 2700 loss 227.25458\n",
      "iteration 2800 loss 227.25456\n",
      "iteration 2900 loss 227.25455\n",
      "iteration 3000 loss 227.25455\n",
      "iteration 3100 loss 227.25455\n",
      "iteration 3200 loss 227.25453\n",
      "iteration 3300 loss 227.25455\n",
      "iteration 3400 loss 227.25455\n",
      "iteration 3500 loss 227.25453\n",
      "iteration 3600 loss 227.25455\n",
      "iteration 3700 loss 227.25455\n",
      "iteration 3800 loss 227.25453\n",
      "iteration 3900 loss 227.25455\n",
      "iteration 4000 loss 227.25453\n",
      "iteration 4100 loss 227.25452\n",
      "iteration 4200 loss 227.25456\n",
      "iteration 4300 loss 227.25455\n",
      "iteration 4400 loss 227.25455\n",
      "iteration 4500 loss 227.25455\n",
      "iteration 4600 loss 227.25452\n",
      "iteration 4700 loss 227.25455\n",
      "iteration 4800 loss 227.25453\n",
      "iteration 4900 loss 227.25452\n",
      "iteration 5000 loss 227.25453Train finished\n",
      "Selected features are: [15 48 51]\n",
      "Selected weights are: [0.9973805  0.50386924 0.9878563 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0018567206447542097, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 41.232183314677236\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 324.61777\n",
      "iteration 200 loss 317.46585\n",
      "iteration 300 loss 317.43692\n",
      "iteration 400 loss 317.42853\n",
      "iteration 500 loss 317.42346\n",
      "iteration 600 loss 317.42038\n",
      "iteration 700 loss 317.41846\n",
      "iteration 800 loss 317.41724\n",
      "iteration 900 loss 317.41647\n",
      "iteration 1000 loss 317.41602\n",
      "iteration 1100 loss 317.41565\n",
      "iteration 1200 loss 317.41547\n",
      "iteration 1300 loss 317.41534\n",
      "iteration 1400 loss 317.41522\n",
      "iteration 1500 loss 317.41516\n",
      "iteration 1600 loss 317.41516\n",
      "iteration 1700 loss 317.4152\n",
      "iteration 1800 loss 317.4151\n",
      "iteration 1900 loss 317.41516\n",
      "iteration 2000 loss 317.4151\n",
      "iteration 2100 loss 317.4151\n",
      "iteration 2200 loss 317.41516\n",
      "iteration 2300 loss 317.4151\n",
      "iteration 2400 loss 317.41513\n",
      "iteration 2500 loss 317.41507\n",
      "iteration 2600 loss 317.4151\n",
      "iteration 2700 loss 317.4151\n",
      "iteration 2800 loss 317.4151\n",
      "iteration 2900 loss 317.4151\n",
      "iteration 3000 loss 317.41513\n",
      "iteration 3100 loss 317.41513\n",
      "iteration 3200 loss 317.4151\n",
      "iteration 3300 loss 317.4151\n",
      "iteration 3400 loss 317.4151\n",
      "iteration 3500 loss 317.4151\n",
      "iteration 3600 loss 317.41504\n",
      "iteration 3700 loss 317.4151\n",
      "iteration 3800 loss 317.41507\n",
      "iteration 3900 loss 317.4151\n",
      "iteration 4000 loss 317.4151\n",
      "iteration 4100 loss 317.4151\n",
      "iteration 4200 loss 317.4151\n",
      "iteration 4300 loss 317.41513\n",
      "iteration 4400 loss 317.4151\n",
      "iteration 4500 loss 317.4151\n",
      "iteration 4600 loss 317.4151\n",
      "iteration 4700 loss 317.4151\n",
      "iteration 4800 loss 317.4151\n",
      "iteration 4900 loss 317.4151\n",
      "iteration 5000 loss 317.4151Train finished\n",
      "Selected features are: [15 40 48]\n",
      "Selected weights are: [0.9952151  0.45924565 0.8914435 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0015879029593663843, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 9.673790146519107\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 256.26715\n",
      "iteration 200 loss 254.7512\n",
      "iteration 300 loss 254.58098\n",
      "iteration 400 loss 254.581\n",
      "iteration 500 loss 254.581\n",
      "iteration 600 loss 254.581\n",
      "iteration 700 loss 254.581\n",
      "iteration 800 loss 254.58098\n",
      "iteration 900 loss 254.58098\n",
      "iteration 1000 loss 254.581\n",
      "iteration 1100 loss 254.581\n",
      "iteration 1200 loss 254.58098\n",
      "iteration 1300 loss 254.58098\n",
      "iteration 1400 loss 254.58101\n",
      "iteration 1500 loss 254.58096\n",
      "iteration 1600 loss 254.58096\n",
      "iteration 1700 loss 254.581\n",
      "iteration 1800 loss 254.581\n",
      "iteration 1900 loss 254.581\n",
      "iteration 2000 loss 254.581\n",
      "iteration 2100 loss 254.581\n",
      "iteration 2200 loss 254.581\n",
      "iteration 2300 loss 254.58101\n",
      "iteration 2400 loss 254.581\n",
      "iteration 2500 loss 254.581\n",
      "iteration 2600 loss 254.58098\n",
      "iteration 2700 loss 254.58098\n",
      "iteration 2800 loss 254.581\n",
      "iteration 2900 loss 254.581\n",
      "iteration 3000 loss 254.58101\n",
      "iteration 3100 loss 254.581\n",
      "iteration 3200 loss 254.581\n",
      "iteration 3300 loss 254.58098\n",
      "iteration 3400 loss 254.58098\n",
      "iteration 3500 loss 254.581\n",
      "iteration 3600 loss 254.581\n",
      "iteration 3700 loss 254.58101\n",
      "iteration 3800 loss 254.581\n",
      "iteration 3900 loss 254.581\n",
      "iteration 4000 loss 254.58098\n",
      "iteration 4100 loss 254.58098\n",
      "iteration 4200 loss 254.581\n",
      "iteration 4300 loss 254.581\n",
      "iteration 4400 loss 254.58101\n",
      "iteration 4500 loss 254.581\n",
      "iteration 4600 loss 254.581\n",
      "iteration 4700 loss 254.58098\n",
      "iteration 4800 loss 254.58098\n",
      "iteration 4900 loss 254.581\n",
      "iteration 5000 loss 254.581Train finished\n",
      "Selected features are: [ 8 15 48]\n",
      "Selected weights are: [0.68004066 0.9958014  0.6848576 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.001443768176763274, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 32.49369026821117\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 240.44139\n",
      "iteration 200 loss 237.54382\n",
      "iteration 300 loss 233.97527\n",
      "iteration 400 loss 233.45819\n",
      "iteration 500 loss 233.25407\n",
      "iteration 600 loss 233.09485\n",
      "iteration 700 loss 233.01773\n",
      "iteration 800 loss 232.97723\n",
      "iteration 900 loss 232.9754\n",
      "iteration 1000 loss 232.97472\n",
      "iteration 1100 loss 232.97456\n",
      "iteration 1200 loss 232.97444\n",
      "iteration 1300 loss 232.97437\n",
      "iteration 1400 loss 232.97433\n",
      "iteration 1500 loss 232.97424\n",
      "iteration 1600 loss 232.97424\n",
      "iteration 1700 loss 232.97423\n",
      "iteration 1800 loss 232.97424\n",
      "iteration 1900 loss 232.97418\n",
      "iteration 2000 loss 232.97421\n",
      "iteration 2100 loss 232.97421\n",
      "iteration 2200 loss 232.97418\n",
      "iteration 2300 loss 232.9742\n",
      "iteration 2400 loss 232.97423\n",
      "iteration 2500 loss 232.97421\n",
      "iteration 2600 loss 232.97421\n",
      "iteration 2700 loss 232.97421\n",
      "iteration 2800 loss 232.97421\n",
      "iteration 2900 loss 232.97424\n",
      "iteration 3000 loss 232.97421\n",
      "iteration 3100 loss 232.97421\n",
      "iteration 3200 loss 232.97421\n",
      "iteration 3300 loss 232.97421\n",
      "iteration 3400 loss 232.9742\n",
      "iteration 3500 loss 232.97421\n",
      "iteration 3600 loss 232.9742\n",
      "iteration 3700 loss 232.9742\n",
      "iteration 3800 loss 232.97421\n",
      "iteration 3900 loss 232.97421\n",
      "iteration 4000 loss 232.9742\n",
      "iteration 4100 loss 232.97418\n",
      "iteration 4200 loss 232.9742\n",
      "iteration 4300 loss 232.9742\n",
      "iteration 4400 loss 232.9742\n",
      "iteration 4500 loss 232.9742\n",
      "iteration 4600 loss 232.9742\n",
      "iteration 4700 loss 232.9742\n",
      "iteration 4800 loss 232.9742\n",
      "iteration 4900 loss 232.9742\n",
      "iteration 5000 loss 232.9742Train finished\n",
      "Selected features are: [15 40 48]\n",
      "Selected weights are: [0.99729866 0.58616024 0.88271815]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0014669699399233551, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 48.22404999473088\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 317.7611\n",
      "iteration 200 loss 314.47562\n",
      "iteration 300 loss 314.0735\n",
      "iteration 400 loss 313.61108\n",
      "iteration 500 loss 313.36426\n",
      "iteration 600 loss 313.0613\n",
      "iteration 700 loss 312.57404\n",
      "iteration 800 loss 312.29822\n",
      "iteration 900 loss 312.29144\n",
      "iteration 1000 loss 312.2895\n",
      "iteration 1100 loss 312.28943\n",
      "iteration 1200 loss 312.28937\n",
      "iteration 1300 loss 312.28925\n",
      "iteration 1400 loss 312.28928\n",
      "iteration 1500 loss 312.28928\n",
      "iteration 1600 loss 312.28928\n",
      "iteration 1700 loss 312.28928\n",
      "iteration 1800 loss 312.28928\n",
      "iteration 1900 loss 312.28928\n",
      "iteration 2000 loss 312.28928\n",
      "iteration 2100 loss 312.28928\n",
      "iteration 2200 loss 312.28928\n",
      "iteration 2300 loss 312.28928\n",
      "iteration 2400 loss 312.28928\n",
      "iteration 2500 loss 312.28928\n",
      "iteration 2600 loss 312.28928\n",
      "iteration 2700 loss 312.28928\n",
      "iteration 2800 loss 312.28928\n",
      "iteration 2900 loss 312.28928\n",
      "iteration 3000 loss 312.28928\n",
      "iteration 3100 loss 312.28928\n",
      "iteration 3200 loss 312.28928\n",
      "iteration 3300 loss 312.28928\n",
      "iteration 3400 loss 312.28928\n",
      "iteration 3500 loss 312.28928\n",
      "iteration 3600 loss 312.28928\n",
      "iteration 3700 loss 312.28928\n",
      "iteration 3800 loss 312.28928\n",
      "iteration 3900 loss 312.28928\n",
      "iteration 4000 loss 312.28928\n",
      "iteration 4100 loss 312.28928\n",
      "iteration 4200 loss 312.28928\n",
      "iteration 4300 loss 312.28928\n",
      "iteration 4400 loss 312.28928\n",
      "iteration 4500 loss 312.28928\n",
      "iteration 4600 loss 312.28928\n",
      "iteration 4700 loss 312.28928\n",
      "iteration 4800 loss 312.28928\n",
      "iteration 4900 loss 312.28928\n",
      "iteration 5000 loss 312.28928Train finished\n",
      "Selected features are: [33 51 56]\n",
      "Selected weights are: [0.9981215 0.9981215 0.9981215]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.02594502429891858, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 12.222685986999302\n",
      "\n",
      " 2 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 258.27417\n",
      "iteration 200 loss 258.1054\n",
      "iteration 300 loss 254.85236\n",
      "iteration 400 loss 251.29982\n",
      "iteration 500 loss 251.03442\n",
      "iteration 600 loss 250.94095\n",
      "iteration 700 loss 250.87038\n",
      "iteration 800 loss 250.82516\n",
      "iteration 900 loss 250.79256\n",
      "iteration 1000 loss 250.76845\n",
      "iteration 1100 loss 250.75748\n",
      "iteration 1200 loss 250.75256\n",
      "iteration 1300 loss 250.74825\n",
      "iteration 1400 loss 250.74443\n",
      "iteration 1500 loss 250.74106\n",
      "iteration 1600 loss 250.738\n",
      "iteration 1700 loss 250.73532\n",
      "iteration 1800 loss 250.7329\n",
      "iteration 1900 loss 250.73218\n",
      "iteration 2000 loss 250.73196\n",
      "iteration 2100 loss 250.73181\n",
      "iteration 2200 loss 250.73172\n",
      "iteration 2300 loss 250.73164\n",
      "iteration 2400 loss 250.73158\n",
      "iteration 2500 loss 250.73154\n",
      "iteration 2600 loss 250.7315\n",
      "iteration 2700 loss 250.73149\n",
      "iteration 2800 loss 250.73145\n",
      "iteration 2900 loss 250.73145\n",
      "iteration 3000 loss 250.73141\n",
      "iteration 3100 loss 250.73145\n",
      "iteration 3200 loss 250.73141\n",
      "iteration 3300 loss 250.7314\n",
      "iteration 3400 loss 250.7314\n",
      "iteration 3500 loss 250.7314\n",
      "iteration 3600 loss 250.73141\n",
      "iteration 3700 loss 250.73141\n",
      "iteration 3800 loss 250.73143\n",
      "iteration 3900 loss 250.73141\n",
      "iteration 4000 loss 250.7314\n",
      "iteration 4100 loss 250.73138\n",
      "iteration 4200 loss 250.73138\n",
      "iteration 4300 loss 250.7314\n",
      "iteration 4400 loss 250.73141\n",
      "iteration 4500 loss 250.73143\n",
      "iteration 4600 loss 250.7314\n",
      "iteration 4700 loss 250.73143\n",
      "iteration 4800 loss 250.73138\n",
      "iteration 4900 loss 250.73141\n",
      "iteration 5000 loss 250.73141Train finished\n",
      "Selected features are: [15 40 48]\n",
      "Selected weights are: [0.9987697 0.5167237 0.7853181]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0014903502343102527, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 40.53326772882942\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 302.221\n",
      "iteration 200 loss 299.62415\n",
      "iteration 300 loss 299.37476\n",
      "iteration 400 loss 299.32047\n",
      "iteration 500 loss 299.30685\n",
      "iteration 600 loss 299.3001\n",
      "iteration 700 loss 299.29526\n",
      "iteration 800 loss 299.291\n",
      "iteration 900 loss 299.28693\n",
      "iteration 1000 loss 299.28403\n",
      "iteration 1100 loss 299.2818\n",
      "iteration 1200 loss 299.2815\n",
      "iteration 1300 loss 299.2815\n",
      "iteration 1400 loss 299.2815\n",
      "iteration 1500 loss 299.2815\n",
      "iteration 1600 loss 299.28146\n",
      "iteration 1700 loss 299.2815\n",
      "iteration 1800 loss 299.2815\n",
      "iteration 1900 loss 299.2815\n",
      "iteration 2000 loss 299.2815\n",
      "iteration 2100 loss 299.2815\n",
      "iteration 2200 loss 299.2815\n",
      "iteration 2300 loss 299.28146\n",
      "iteration 2400 loss 299.28146\n",
      "iteration 2500 loss 299.2815\n",
      "iteration 2600 loss 299.2815\n",
      "iteration 2700 loss 299.2815\n",
      "iteration 2800 loss 299.28146\n",
      "iteration 2900 loss 299.2815\n",
      "iteration 3000 loss 299.2815\n",
      "iteration 3100 loss 299.2815\n",
      "iteration 3200 loss 299.2815\n",
      "iteration 3300 loss 299.2815\n",
      "iteration 3400 loss 299.2815\n",
      "iteration 3500 loss 299.2815\n",
      "iteration 3600 loss 299.2815\n",
      "iteration 3700 loss 299.2815\n",
      "iteration 3800 loss 299.2815\n",
      "iteration 3900 loss 299.2815\n",
      "iteration 4000 loss 299.2815\n",
      "iteration 4100 loss 299.2815\n",
      "iteration 4200 loss 299.2815\n",
      "iteration 4300 loss 299.2815\n",
      "iteration 4400 loss 299.2815\n",
      "iteration 4500 loss 299.28146\n",
      "iteration 4600 loss 299.2815\n",
      "iteration 4700 loss 299.2815\n",
      "iteration 4800 loss 299.28146\n",
      "iteration 4900 loss 299.28146\n",
      "iteration 5000 loss 299.28146Train finished\n",
      "Selected features are: [11 46 56]\n",
      "Selected weights are: [0.38270488 0.9283514  0.99831396]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.002963715022785046, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 17.599559492826714\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 272.4447\n",
      "iteration 200 loss 267.20154\n",
      "iteration 300 loss 266.91537\n",
      "iteration 400 loss 266.89954\n",
      "iteration 500 loss 266.8913\n",
      "iteration 600 loss 266.8864\n",
      "iteration 700 loss 266.8835\n",
      "iteration 800 loss 266.8817\n",
      "iteration 900 loss 266.8806\n",
      "iteration 1000 loss 266.87994\n",
      "iteration 1100 loss 266.8795\n",
      "iteration 1200 loss 266.87924\n",
      "iteration 1300 loss 266.8791\n",
      "iteration 1400 loss 266.87897\n",
      "iteration 1500 loss 266.87885\n",
      "iteration 1600 loss 266.87885\n",
      "iteration 1700 loss 266.8788\n",
      "iteration 1800 loss 266.8788\n",
      "iteration 1900 loss 266.8788\n",
      "iteration 2000 loss 266.87878\n",
      "iteration 2100 loss 266.87878\n",
      "iteration 2200 loss 266.87878\n",
      "iteration 2300 loss 266.87878\n",
      "iteration 2400 loss 266.87875\n",
      "iteration 2500 loss 266.87875\n",
      "iteration 2600 loss 266.87878\n",
      "iteration 2700 loss 266.87875\n",
      "iteration 2800 loss 266.87875\n",
      "iteration 2900 loss 266.87878\n",
      "iteration 3000 loss 266.87878\n",
      "iteration 3100 loss 266.87875\n",
      "iteration 3200 loss 266.87872\n",
      "iteration 3300 loss 266.87878\n",
      "iteration 3400 loss 266.87875\n",
      "iteration 3500 loss 266.87875\n",
      "iteration 3600 loss 266.87875\n",
      "iteration 3700 loss 266.87875\n",
      "iteration 3800 loss 266.87872\n",
      "iteration 3900 loss 266.87872\n",
      "iteration 4000 loss 266.87875\n",
      "iteration 4100 loss 266.87875\n",
      "iteration 4200 loss 266.87875\n",
      "iteration 4300 loss 266.87875\n",
      "iteration 4400 loss 266.87872\n",
      "iteration 4500 loss 266.87872\n",
      "iteration 4600 loss 266.87875\n",
      "iteration 4700 loss 266.87872\n",
      "iteration 4800 loss 266.87875\n",
      "iteration 4900 loss 266.87875\n",
      "iteration 5000 loss 266.87875Train finished\n",
      "Selected features are: [14 15 48]\n",
      "Selected weights are: [0.47230282 0.9954896  0.95663774]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0008761054848978785, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 30.017143207060933\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 275.67038\n",
      "iteration 200 loss 273.80273\n",
      "iteration 300 loss 273.62766\n",
      "iteration 400 loss 273.60806\n",
      "iteration 500 loss 273.5995\n",
      "iteration 600 loss 273.5957\n",
      "iteration 700 loss 273.594\n",
      "iteration 800 loss 273.5932\n",
      "iteration 900 loss 273.59283\n",
      "iteration 1000 loss 273.5927\n",
      "iteration 1100 loss 273.59262\n",
      "iteration 1200 loss 273.59262\n",
      "iteration 1300 loss 273.5926\n",
      "iteration 1400 loss 273.59262\n",
      "iteration 1500 loss 273.59262\n",
      "iteration 1600 loss 273.5926\n",
      "iteration 1700 loss 273.59262\n",
      "iteration 1800 loss 273.59262\n",
      "iteration 1900 loss 273.59265\n",
      "iteration 2000 loss 273.59262\n",
      "iteration 2100 loss 273.59265\n",
      "iteration 2200 loss 273.59265\n",
      "iteration 2300 loss 273.59265\n",
      "iteration 2400 loss 273.59265\n",
      "iteration 2500 loss 273.59262\n",
      "iteration 2600 loss 273.59265\n",
      "iteration 2700 loss 273.59262\n",
      "iteration 2800 loss 273.59262\n",
      "iteration 2900 loss 273.59265\n",
      "iteration 3000 loss 273.5926\n",
      "iteration 3100 loss 273.59262\n",
      "iteration 3200 loss 273.5926\n",
      "iteration 3300 loss 273.59262\n",
      "iteration 3400 loss 273.59265\n",
      "iteration 3500 loss 273.59262\n",
      "iteration 3600 loss 273.59262\n",
      "iteration 3700 loss 273.59262\n",
      "iteration 3800 loss 273.59262\n",
      "iteration 3900 loss 273.59262\n",
      "iteration 4000 loss 273.59262\n",
      "iteration 4100 loss 273.59262\n",
      "iteration 4200 loss 273.59262\n",
      "iteration 4300 loss 273.59262\n",
      "iteration 4400 loss 273.59262\n",
      "iteration 4500 loss 273.59262\n",
      "iteration 4600 loss 273.59262\n",
      "iteration 4700 loss 273.59262\n",
      "iteration 4800 loss 273.59262\n",
      "iteration 4900 loss 273.59262\n",
      "iteration 5000 loss 273.59262Train finished\n",
      "Selected features are: [15 40 48]\n",
      "Selected weights are: [0.99490154 0.4821732  0.9753623 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0014471670102451752, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 23.433120557889563\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 267.3592\n",
      "iteration 200 loss 264.30658\n",
      "iteration 300 loss 263.93628\n",
      "iteration 400 loss 263.63385\n",
      "iteration 500 loss 263.39362\n",
      "iteration 600 loss 263.23468\n",
      "iteration 700 loss 263.15433\n",
      "iteration 800 loss 263.1231\n",
      "iteration 900 loss 263.10593\n",
      "iteration 1000 loss 263.1029\n",
      "iteration 1100 loss 263.10217\n",
      "iteration 1200 loss 263.102\n",
      "iteration 1300 loss 263.10193\n",
      "iteration 1400 loss 263.10187\n",
      "iteration 1500 loss 263.10193\n",
      "iteration 1600 loss 263.10193\n",
      "iteration 1700 loss 263.10193\n",
      "iteration 1800 loss 263.10193\n",
      "iteration 1900 loss 263.10193\n",
      "iteration 2000 loss 263.10193\n",
      "iteration 2100 loss 263.10193\n",
      "iteration 2200 loss 263.10193\n",
      "iteration 2300 loss 263.10193\n",
      "iteration 2400 loss 263.10193\n",
      "iteration 2500 loss 263.10193\n",
      "iteration 2600 loss 263.10193\n",
      "iteration 2700 loss 263.10193\n",
      "iteration 2800 loss 263.10193\n",
      "iteration 2900 loss 263.10193\n",
      "iteration 3000 loss 263.10193\n",
      "iteration 3100 loss 263.10193\n",
      "iteration 3200 loss 263.10193\n",
      "iteration 3300 loss 263.10193\n",
      "iteration 3400 loss 263.10193\n",
      "iteration 3500 loss 263.10193\n",
      "iteration 3600 loss 263.10193\n",
      "iteration 3700 loss 263.10193\n",
      "iteration 3800 loss 263.10193\n",
      "iteration 3900 loss 263.10193\n",
      "iteration 4000 loss 263.10193\n",
      "iteration 4100 loss 263.10193\n",
      "iteration 4200 loss 263.10193\n",
      "iteration 4300 loss 263.10193\n",
      "iteration 4400 loss 263.10193\n",
      "iteration 4500 loss 263.10193\n",
      "iteration 4600 loss 263.10193\n",
      "iteration 4700 loss 263.10193\n",
      "iteration 4800 loss 263.10193\n",
      "iteration 4900 loss 263.10193\n",
      "iteration 5000 loss 263.10193Train finished\n",
      "Selected features are: [33 51 56]\n",
      "Selected weights are: [0.99875486 0.9113539  0.99875486]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.02940795650688572, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 32.297911921507946\n",
      "\n",
      " 3 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 294.1175\n",
      "iteration 200 loss 290.16046\n",
      "iteration 300 loss 290.12634\n",
      "iteration 400 loss 290.111\n",
      "iteration 500 loss 290.10358\n",
      "iteration 600 loss 290.09995\n",
      "iteration 700 loss 290.09814\n",
      "iteration 800 loss 290.09723\n",
      "iteration 900 loss 290.0967\n",
      "iteration 1000 loss 290.0965\n",
      "iteration 1100 loss 290.09637\n",
      "iteration 1200 loss 290.0963\n",
      "iteration 1300 loss 290.0962\n",
      "iteration 1400 loss 290.09622\n",
      "iteration 1500 loss 290.09622\n",
      "iteration 1600 loss 290.0962\n",
      "iteration 1700 loss 290.0962\n",
      "iteration 1800 loss 290.0962\n",
      "iteration 1900 loss 290.0962\n",
      "iteration 2000 loss 290.0962\n",
      "iteration 2100 loss 290.09622\n",
      "iteration 2200 loss 290.0962\n",
      "iteration 2300 loss 290.09616\n",
      "iteration 2400 loss 290.0962\n",
      "iteration 2500 loss 290.0962\n",
      "iteration 2600 loss 290.0962\n",
      "iteration 2700 loss 290.09616\n",
      "iteration 2800 loss 290.0962\n",
      "iteration 2900 loss 290.0962\n",
      "iteration 3000 loss 290.0962\n",
      "iteration 3100 loss 290.0962\n",
      "iteration 3200 loss 290.0962\n",
      "iteration 3300 loss 290.0962\n",
      "iteration 3400 loss 290.0962\n",
      "iteration 3500 loss 290.0962\n",
      "iteration 3600 loss 290.09616\n",
      "iteration 3700 loss 290.09616\n",
      "iteration 3800 loss 290.09616\n",
      "iteration 3900 loss 290.09616\n",
      "iteration 4000 loss 290.09616\n",
      "iteration 4100 loss 290.09616\n",
      "iteration 4200 loss 290.09616\n",
      "iteration 4300 loss 290.09616\n",
      "iteration 4400 loss 290.09616\n",
      "iteration 4500 loss 290.09616\n",
      "iteration 4600 loss 290.09616\n",
      "iteration 4700 loss 290.09616\n",
      "iteration 4800 loss 290.09616\n",
      "iteration 4900 loss 290.09616\n",
      "iteration 5000 loss 290.09616Train finished\n",
      "Selected features are: [14 15 48]\n",
      "Selected weights are: [0.42882878 0.9937561  0.940204  ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0007260801041075735, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 19.46685175769618\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 247.14047\n",
      "iteration 200 loss 238.954\n",
      "iteration 300 loss 238.67422\n",
      "iteration 400 loss 238.66444\n",
      "iteration 500 loss 238.65765\n",
      "iteration 600 loss 238.65298\n",
      "iteration 700 loss 238.6498\n",
      "iteration 800 loss 238.64767\n",
      "iteration 900 loss 238.64624\n",
      "iteration 1000 loss 238.64526\n",
      "iteration 1100 loss 238.64467\n",
      "iteration 1200 loss 238.64426\n",
      "iteration 1300 loss 238.64395\n",
      "iteration 1400 loss 238.64378\n",
      "iteration 1500 loss 238.64368\n",
      "iteration 1600 loss 238.64359\n",
      "iteration 1700 loss 238.64354\n",
      "iteration 1800 loss 238.64352\n",
      "iteration 1900 loss 238.6435\n",
      "iteration 2000 loss 238.64346\n",
      "iteration 2100 loss 238.64346\n",
      "iteration 2200 loss 238.6435\n",
      "iteration 2300 loss 238.64348\n",
      "iteration 2400 loss 238.6435\n",
      "iteration 2500 loss 238.6435\n",
      "iteration 2600 loss 238.64346\n",
      "iteration 2700 loss 238.64345\n",
      "iteration 2800 loss 238.64343\n",
      "iteration 2900 loss 238.64346\n",
      "iteration 3000 loss 238.64346\n",
      "iteration 3100 loss 238.64346\n",
      "iteration 3200 loss 238.64348\n",
      "iteration 3300 loss 238.64346\n",
      "iteration 3400 loss 238.64346\n",
      "iteration 3500 loss 238.64346\n",
      "iteration 3600 loss 238.64348\n",
      "iteration 3700 loss 238.64348\n",
      "iteration 3800 loss 238.64345\n",
      "iteration 3900 loss 238.64346\n",
      "iteration 4000 loss 238.64346\n",
      "iteration 4100 loss 238.64346\n",
      "iteration 4200 loss 238.64346\n",
      "iteration 4300 loss 238.64348\n",
      "iteration 4400 loss 238.64348\n",
      "iteration 4500 loss 238.6435\n",
      "iteration 4600 loss 238.64346\n",
      "iteration 4700 loss 238.64348\n",
      "iteration 4800 loss 238.64348\n",
      "iteration 4900 loss 238.64346\n",
      "iteration 5000 loss 238.64348Train finished\n",
      "Selected features are: [15 40 48]\n",
      "Selected weights are: [0.9955936  0.48914966 0.538813  ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0014640513769544825, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 41.2058218286505\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 305.33716\n",
      "iteration 200 loss 304.21246\n",
      "iteration 300 loss 303.87848\n",
      "iteration 400 loss 303.7846\n",
      "iteration 500 loss 303.75174\n",
      "iteration 600 loss 303.735\n",
      "iteration 700 loss 303.72412\n",
      "iteration 800 loss 303.71692\n",
      "iteration 900 loss 303.712\n",
      "iteration 1000 loss 303.70987\n",
      "iteration 1100 loss 303.70975\n",
      "iteration 1200 loss 303.7097\n",
      "iteration 1300 loss 303.70972\n",
      "iteration 1400 loss 303.70972\n",
      "iteration 1500 loss 303.70975\n",
      "iteration 1600 loss 303.70975\n",
      "iteration 1700 loss 303.70975\n",
      "iteration 1800 loss 303.70975\n",
      "iteration 1900 loss 303.70975\n",
      "iteration 2000 loss 303.70975\n",
      "iteration 2100 loss 303.70975\n",
      "iteration 2200 loss 303.70975\n",
      "iteration 2300 loss 303.70975\n",
      "iteration 2400 loss 303.70975\n",
      "iteration 2500 loss 303.70975\n",
      "iteration 2600 loss 303.70975\n",
      "iteration 2700 loss 303.70975\n",
      "iteration 2800 loss 303.70975\n",
      "iteration 2900 loss 303.70975\n",
      "iteration 3000 loss 303.70975\n",
      "iteration 3100 loss 303.70975\n",
      "iteration 3200 loss 303.70975\n",
      "iteration 3300 loss 303.70975\n",
      "iteration 3400 loss 303.70975\n",
      "iteration 3500 loss 303.70975\n",
      "iteration 3600 loss 303.70975\n",
      "iteration 3700 loss 303.70975\n",
      "iteration 3800 loss 303.70975\n",
      "iteration 3900 loss 303.70975\n",
      "iteration 4000 loss 303.70975\n",
      "iteration 4100 loss 303.70975\n",
      "iteration 4200 loss 303.70975\n",
      "iteration 4300 loss 303.70975\n",
      "iteration 4400 loss 303.70975\n",
      "iteration 4500 loss 303.70975\n",
      "iteration 4600 loss 303.70975\n",
      "iteration 4700 loss 303.70975\n",
      "iteration 4800 loss 303.70975\n",
      "iteration 4900 loss 303.70975\n",
      "iteration 5000 loss 303.70975Train finished\n",
      "Selected features are: [14 15 48]\n",
      "Selected weights are: [0.46779424 0.99715656 0.99715656]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0008787693696906085, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 13.183448810550995\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 300.81433\n",
      "iteration 200 loss 296.67133\n",
      "iteration 300 loss 295.25027\n",
      "iteration 400 loss 294.30084\n",
      "iteration 500 loss 294.27527\n",
      "iteration 600 loss 294.26624\n",
      "iteration 700 loss 294.2632\n",
      "iteration 800 loss 294.2622\n",
      "iteration 900 loss 294.26187\n",
      "iteration 1000 loss 294.26178\n",
      "iteration 1100 loss 294.26172\n",
      "iteration 1200 loss 294.26178\n",
      "iteration 1300 loss 294.26178\n",
      "iteration 1400 loss 294.26172\n",
      "iteration 1500 loss 294.26172\n",
      "iteration 1600 loss 294.26172\n",
      "iteration 1700 loss 294.26175\n",
      "iteration 1800 loss 294.26175\n",
      "iteration 1900 loss 294.26175\n",
      "iteration 2000 loss 294.26172\n",
      "iteration 2100 loss 294.26172\n",
      "iteration 2200 loss 294.26172\n",
      "iteration 2300 loss 294.26172\n",
      "iteration 2400 loss 294.26172\n",
      "iteration 2500 loss 294.26172\n",
      "iteration 2600 loss 294.26172\n",
      "iteration 2700 loss 294.26172\n",
      "iteration 2800 loss 294.26172\n",
      "iteration 2900 loss 294.26172\n",
      "iteration 3000 loss 294.26172\n",
      "iteration 3100 loss 294.26172\n",
      "iteration 3200 loss 294.26172\n",
      "iteration 3300 loss 294.26172\n",
      "iteration 3400 loss 294.26172\n",
      "iteration 3500 loss 294.26172\n",
      "iteration 3600 loss 294.26172\n",
      "iteration 3700 loss 294.26172\n",
      "iteration 3800 loss 294.26172\n",
      "iteration 3900 loss 294.26172\n",
      "iteration 4000 loss 294.26172\n",
      "iteration 4100 loss 294.26172\n",
      "iteration 4200 loss 294.26172\n",
      "iteration 4300 loss 294.26172\n",
      "iteration 4400 loss 294.26172\n",
      "iteration 4500 loss 294.26172\n",
      "iteration 4600 loss 294.26172\n",
      "iteration 4700 loss 294.26172\n",
      "iteration 4800 loss 294.26172\n",
      "iteration 4900 loss 294.26172\n",
      "iteration 5000 loss 294.26172Train finished\n",
      "Selected features are: [33 51 56]\n",
      "Selected weights are: [0.88085026 0.8522424  0.9956049 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.025123768989348604, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 19.048567616445037\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 220.14745\n",
      "iteration 200 loss 216.01904\n",
      "iteration 300 loss 215.36156\n",
      "iteration 400 loss 215.20114\n",
      "iteration 500 loss 215.11746\n",
      "iteration 600 loss 215.03778\n",
      "iteration 700 loss 214.99043\n",
      "iteration 800 loss 214.94939\n",
      "iteration 900 loss 214.91136\n",
      "iteration 1000 loss 214.87549\n",
      "iteration 1100 loss 214.84103\n",
      "iteration 1200 loss 214.81169\n",
      "iteration 1300 loss 214.79478\n",
      "iteration 1400 loss 214.78537\n",
      "iteration 1500 loss 214.77843\n",
      "iteration 1600 loss 214.77188\n",
      "iteration 1700 loss 214.76555\n",
      "iteration 1800 loss 214.75935\n",
      "iteration 1900 loss 214.75322\n",
      "iteration 2000 loss 214.74721\n",
      "iteration 2100 loss 214.74124\n",
      "iteration 2200 loss 214.73535\n",
      "iteration 2300 loss 214.72949\n",
      "iteration 2400 loss 214.7237\n",
      "iteration 2500 loss 214.71803\n",
      "iteration 2600 loss 214.7124\n",
      "iteration 2700 loss 214.70735\n",
      "iteration 2800 loss 214.70703\n",
      "iteration 2900 loss 214.70697\n",
      "iteration 3000 loss 214.70688\n",
      "iteration 3100 loss 214.70682\n",
      "iteration 3200 loss 214.70679\n",
      "iteration 3300 loss 214.70676\n",
      "iteration 3400 loss 214.70676\n",
      "iteration 3500 loss 214.7067\n",
      "iteration 3600 loss 214.70673\n",
      "iteration 3700 loss 214.7067\n",
      "iteration 3800 loss 214.70671\n",
      "iteration 3900 loss 214.70668\n",
      "iteration 4000 loss 214.70667\n",
      "iteration 4100 loss 214.70668\n",
      "iteration 4200 loss 214.7067\n",
      "iteration 4300 loss 214.70668\n",
      "iteration 4400 loss 214.70667\n",
      "iteration 4500 loss 214.70667\n",
      "iteration 4600 loss 214.70667\n",
      "iteration 4700 loss 214.70667\n",
      "iteration 4800 loss 214.70665\n",
      "iteration 4900 loss 214.70668\n",
      "iteration 5000 loss 214.70665Train finished\n",
      "Selected features are: [15 33 51]\n",
      "Selected weights are: [0.9979888 0.9979888 0.672567 ]\n",
      "Best params obtained by cv: {'alpha': 0.0001, 'coef0': 1, 'degree': 3, 'gamma': 0.002869537749099246, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 53.50692934734077\n",
      "\n",
      " 4 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 232.28592\n",
      "iteration 200 loss 230.74507\n",
      "iteration 300 loss 228.48691\n",
      "iteration 400 loss 227.59311\n",
      "iteration 500 loss 227.3857\n",
      "iteration 600 loss 227.314\n",
      "iteration 700 loss 227.26534\n",
      "iteration 800 loss 227.23752\n",
      "iteration 900 loss 227.21835\n",
      "iteration 1000 loss 227.20111\n",
      "iteration 1100 loss 227.18652\n",
      "iteration 1200 loss 227.17575\n",
      "iteration 1300 loss 227.16478\n",
      "iteration 1400 loss 227.15349\n",
      "iteration 1500 loss 227.14192\n",
      "iteration 1600 loss 227.13011\n",
      "iteration 1700 loss 227.11801\n",
      "iteration 1800 loss 227.10564\n",
      "iteration 1900 loss 227.09303\n",
      "iteration 2000 loss 227.08015\n",
      "iteration 2100 loss 227.06705\n",
      "iteration 2200 loss 227.05368\n",
      "iteration 2300 loss 227.0401\n",
      "iteration 2400 loss 227.0347\n",
      "iteration 2500 loss 227.0347\n",
      "iteration 2600 loss 227.0347\n",
      "iteration 2700 loss 227.0347\n",
      "iteration 2800 loss 227.0347\n",
      "iteration 2900 loss 227.0347\n",
      "iteration 3000 loss 227.0347\n",
      "iteration 3100 loss 227.0347\n",
      "iteration 3200 loss 227.0347\n",
      "iteration 3300 loss 227.0347\n",
      "iteration 3400 loss 227.0347\n",
      "iteration 3500 loss 227.0347\n",
      "iteration 3600 loss 227.0347\n",
      "iteration 3700 loss 227.0347\n",
      "iteration 3800 loss 227.0347\n",
      "iteration 3900 loss 227.0347\n",
      "iteration 4000 loss 227.0347\n",
      "iteration 4100 loss 227.0347\n",
      "iteration 4200 loss 227.0347\n",
      "iteration 4300 loss 227.0347\n",
      "iteration 4400 loss 227.0347\n",
      "iteration 4500 loss 227.0347\n",
      "iteration 4600 loss 227.0347\n",
      "iteration 4700 loss 227.0347\n",
      "iteration 4800 loss 227.0347\n",
      "iteration 4900 loss 227.0347\n",
      "iteration 5000 loss 227.0347Train finished\n",
      "Selected features are: [ 7 11 49]\n",
      "Selected weights are: [0.9990938 0.9990938 0.9990938]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 9.349212803939532e-05, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 48.215203338051104\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 314.21698\n",
      "iteration 200 loss 309.833\n",
      "iteration 300 loss 308.9439\n",
      "iteration 400 loss 308.536\n",
      "iteration 500 loss 308.5355\n",
      "iteration 600 loss 308.53537\n",
      "iteration 700 loss 308.53534\n",
      "iteration 800 loss 308.53534\n",
      "iteration 900 loss 308.53534\n",
      "iteration 1000 loss 308.53534\n",
      "iteration 1100 loss 308.5353\n",
      "iteration 1200 loss 308.5353\n",
      "iteration 1300 loss 308.53528\n",
      "iteration 1400 loss 308.5353\n",
      "iteration 1500 loss 308.53528\n",
      "iteration 1600 loss 308.5353\n",
      "iteration 1700 loss 308.53528\n",
      "iteration 1800 loss 308.53528\n",
      "iteration 1900 loss 308.53528\n",
      "iteration 2000 loss 308.53528\n",
      "iteration 2100 loss 308.53528\n",
      "iteration 2200 loss 308.53528\n",
      "iteration 2300 loss 308.53528\n",
      "iteration 2400 loss 308.53528\n",
      "iteration 2500 loss 308.53528\n",
      "iteration 2600 loss 308.53528\n",
      "iteration 2700 loss 308.53528\n",
      "iteration 2800 loss 308.53528\n",
      "iteration 2900 loss 308.53528\n",
      "iteration 3000 loss 308.53528\n",
      "iteration 3100 loss 308.53528\n",
      "iteration 3200 loss 308.53528\n",
      "iteration 3300 loss 308.53528\n",
      "iteration 3400 loss 308.53528\n",
      "iteration 3500 loss 308.53528\n",
      "iteration 3600 loss 308.53528\n",
      "iteration 3700 loss 308.53528\n",
      "iteration 3800 loss 308.53528\n",
      "iteration 3900 loss 308.53528\n",
      "iteration 4000 loss 308.53528\n",
      "iteration 4100 loss 308.53528\n",
      "iteration 4200 loss 308.53528\n",
      "iteration 4300 loss 308.53528\n",
      "iteration 4400 loss 308.53528\n",
      "iteration 4500 loss 308.53528\n",
      "iteration 4600 loss 308.53528\n",
      "iteration 4700 loss 308.53528\n",
      "iteration 4800 loss 308.53528\n",
      "iteration 4900 loss 308.53528\n",
      "iteration 5000 loss 308.53528Train finished\n",
      "Selected features are: [46 51 56]\n",
      "Selected weights are: [0.67665684 0.67129964 0.9970006 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.006534878287697462, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 13.267425745579635\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 280.7942\n",
      "iteration 200 loss 275.45874\n",
      "iteration 300 loss 274.87897\n",
      "iteration 400 loss 274.73807\n",
      "iteration 500 loss 274.7378\n",
      "iteration 600 loss 274.7378\n",
      "iteration 700 loss 274.73773\n",
      "iteration 800 loss 274.73776\n",
      "iteration 900 loss 274.73776\n",
      "iteration 1000 loss 274.73776\n",
      "iteration 1100 loss 274.73776\n",
      "iteration 1200 loss 274.7378\n",
      "iteration 1300 loss 274.7378\n",
      "iteration 1400 loss 274.7378\n",
      "iteration 1500 loss 274.7378\n",
      "iteration 1600 loss 274.7378\n",
      "iteration 1700 loss 274.7378\n",
      "iteration 1800 loss 274.7378\n",
      "iteration 1900 loss 274.7378\n",
      "iteration 2000 loss 274.7378\n",
      "iteration 2100 loss 274.7378\n",
      "iteration 2200 loss 274.7378\n",
      "iteration 2300 loss 274.7378\n",
      "iteration 2400 loss 274.7378\n",
      "iteration 2500 loss 274.7378\n",
      "iteration 2600 loss 274.7378\n",
      "iteration 2700 loss 274.7378\n",
      "iteration 2800 loss 274.7378\n",
      "iteration 2900 loss 274.7378\n",
      "iteration 3000 loss 274.7378\n",
      "iteration 3100 loss 274.7378\n",
      "iteration 3200 loss 274.7378\n",
      "iteration 3300 loss 274.7378\n",
      "iteration 3400 loss 274.7378\n",
      "iteration 3500 loss 274.7378\n",
      "iteration 3600 loss 274.7378\n",
      "iteration 3700 loss 274.7378\n",
      "iteration 3800 loss 274.7378\n",
      "iteration 3900 loss 274.7378\n",
      "iteration 4000 loss 274.7378\n",
      "iteration 4100 loss 274.7378\n",
      "iteration 4200 loss 274.7378\n",
      "iteration 4300 loss 274.7378\n",
      "iteration 4400 loss 274.7378\n",
      "iteration 4500 loss 274.7378\n",
      "iteration 4600 loss 274.7378\n",
      "iteration 4700 loss 274.7378\n",
      "iteration 4800 loss 274.7378\n",
      "iteration 4900 loss 274.7378\n",
      "iteration 5000 loss 274.7378Train finished\n",
      "Selected features are: [46 51 56]\n",
      "Selected weights are: [0.8090416  0.60179543 0.9969257 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.006619017086109188, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 27.38444941836649\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 288.1507\n",
      "iteration 200 loss 287.29797\n",
      "iteration 300 loss 287.00226\n",
      "iteration 400 loss 286.828\n",
      "iteration 500 loss 286.78415\n",
      "iteration 600 loss 286.77322\n",
      "iteration 700 loss 286.76752\n",
      "iteration 800 loss 286.76425\n",
      "iteration 900 loss 286.7624\n",
      "iteration 1000 loss 286.7614\n",
      "iteration 1100 loss 286.7608\n",
      "iteration 1200 loss 286.76047\n",
      "iteration 1300 loss 286.7602\n",
      "iteration 1400 loss 286.76007\n",
      "iteration 1500 loss 286.76\n",
      "iteration 1600 loss 286.75995\n",
      "iteration 1700 loss 286.76\n",
      "iteration 1800 loss 286.75995\n",
      "iteration 1900 loss 286.75995\n",
      "iteration 2000 loss 286.75995\n",
      "iteration 2100 loss 286.75995\n",
      "iteration 2200 loss 286.75995\n",
      "iteration 2300 loss 286.75992\n",
      "iteration 2400 loss 286.75995\n",
      "iteration 2500 loss 286.75995\n",
      "iteration 2600 loss 286.75995\n",
      "iteration 2700 loss 286.75995\n",
      "iteration 2800 loss 286.75995\n",
      "iteration 2900 loss 286.75992\n",
      "iteration 3000 loss 286.75995\n",
      "iteration 3100 loss 286.75995\n",
      "iteration 3200 loss 286.75998\n",
      "iteration 3300 loss 286.75998\n",
      "iteration 3400 loss 286.75995\n",
      "iteration 3500 loss 286.75998\n",
      "iteration 3600 loss 286.75995\n",
      "iteration 3700 loss 286.75995\n",
      "iteration 3800 loss 286.75995\n",
      "iteration 3900 loss 286.75992\n",
      "iteration 4000 loss 286.75992\n",
      "iteration 4100 loss 286.75992\n",
      "iteration 4200 loss 286.75992\n",
      "iteration 4300 loss 286.75992\n",
      "iteration 4400 loss 286.75992\n",
      "iteration 4500 loss 286.75992\n",
      "iteration 4600 loss 286.75992\n",
      "iteration 4700 loss 286.75992\n",
      "iteration 4800 loss 286.75992\n",
      "iteration 4900 loss 286.75992\n",
      "iteration 5000 loss 286.75992Train finished\n",
      "Selected features are: [14 15 48]\n",
      "Selected weights are: [0.52296203 0.9973224  0.9037805 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.000994246259587257, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 20.075254916784967\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 251.2398\n",
      "iteration 200 loss 249.44385\n",
      "iteration 300 loss 249.21857\n",
      "iteration 400 loss 249.21417\n",
      "iteration 500 loss 249.21268\n",
      "iteration 600 loss 249.21194\n",
      "iteration 700 loss 249.21164\n",
      "iteration 800 loss 249.21149\n",
      "iteration 900 loss 249.21146\n",
      "iteration 1000 loss 249.21144\n",
      "iteration 1100 loss 249.21141\n",
      "iteration 1200 loss 249.21143\n",
      "iteration 1300 loss 249.2114\n",
      "iteration 1400 loss 249.21143\n",
      "iteration 1500 loss 249.21141\n",
      "iteration 1600 loss 249.21143\n",
      "iteration 1700 loss 249.2114\n",
      "iteration 1800 loss 249.21141\n",
      "iteration 1900 loss 249.2114\n",
      "iteration 2000 loss 249.21141\n",
      "iteration 2100 loss 249.2114\n",
      "iteration 2200 loss 249.2114\n",
      "iteration 2300 loss 249.21141\n",
      "iteration 2400 loss 249.2114\n",
      "iteration 2500 loss 249.2114\n",
      "iteration 2600 loss 249.2114\n",
      "iteration 2700 loss 249.2114\n",
      "iteration 2800 loss 249.2114\n",
      "iteration 2900 loss 249.2114\n",
      "iteration 3000 loss 249.2114\n",
      "iteration 3100 loss 249.2114\n",
      "iteration 3200 loss 249.2114\n",
      "iteration 3300 loss 249.2114\n",
      "iteration 3400 loss 249.2114\n",
      "iteration 3500 loss 249.2114\n",
      "iteration 3600 loss 249.2114\n",
      "iteration 3700 loss 249.2114\n",
      "iteration 3800 loss 249.2114\n",
      "iteration 3900 loss 249.2114\n",
      "iteration 4000 loss 249.2114\n",
      "iteration 4100 loss 249.2114\n",
      "iteration 4200 loss 249.2114\n",
      "iteration 4300 loss 249.2114\n",
      "iteration 4400 loss 249.2114\n",
      "iteration 4500 loss 249.2114\n",
      "iteration 4600 loss 249.2114\n",
      "iteration 4700 loss 249.2114\n",
      "iteration 4800 loss 249.2114\n",
      "iteration 4900 loss 249.2114\n",
      "iteration 5000 loss 249.2114Train finished\n",
      "Selected features are: [15 40 48]\n",
      "Selected weights are: [0.9945954  0.6618238  0.86608356]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0014461105296533897, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 34.53707127671423\n",
      "\n",
      " 5 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 280.0482\n",
      "iteration 200 loss 275.71014\n",
      "iteration 300 loss 275.34512\n",
      "iteration 400 loss 275.31293\n",
      "iteration 500 loss 275.30157\n",
      "iteration 600 loss 275.29697\n",
      "iteration 700 loss 275.29544\n",
      "iteration 800 loss 275.29486\n",
      "iteration 900 loss 275.29462\n",
      "iteration 1000 loss 275.29446\n",
      "iteration 1100 loss 275.29437\n",
      "iteration 1200 loss 275.29425\n",
      "iteration 1300 loss 275.2942\n",
      "iteration 1400 loss 275.29413\n",
      "iteration 1500 loss 275.29407\n",
      "iteration 1600 loss 275.29407\n",
      "iteration 1700 loss 275.29407\n",
      "iteration 1800 loss 275.294\n",
      "iteration 1900 loss 275.29395\n",
      "iteration 2000 loss 275.29395\n",
      "iteration 2100 loss 275.29395\n",
      "iteration 2200 loss 275.2939\n",
      "iteration 2300 loss 275.29388\n",
      "iteration 2400 loss 275.29388\n",
      "iteration 2500 loss 275.29388\n",
      "iteration 2600 loss 275.29385\n",
      "iteration 2700 loss 275.29385\n",
      "iteration 2800 loss 275.29385\n",
      "iteration 2900 loss 275.29382\n",
      "iteration 3000 loss 275.2938\n",
      "iteration 3100 loss 275.29376\n",
      "iteration 3200 loss 275.29373\n",
      "iteration 3300 loss 275.29376\n",
      "iteration 3400 loss 275.2937\n",
      "iteration 3500 loss 275.2937\n",
      "iteration 3600 loss 275.29367\n",
      "iteration 3700 loss 275.2937\n",
      "iteration 3800 loss 275.29364\n",
      "iteration 3900 loss 275.29364\n",
      "iteration 4000 loss 275.29358\n",
      "iteration 4100 loss 275.29358\n",
      "iteration 4200 loss 275.29358\n",
      "iteration 4300 loss 275.29355\n",
      "iteration 4400 loss 275.29352\n",
      "iteration 4500 loss 275.2935\n",
      "iteration 4600 loss 275.29352\n",
      "iteration 4700 loss 275.29346\n",
      "iteration 4800 loss 275.2935\n",
      "iteration 4900 loss 275.29346\n",
      "iteration 5000 loss 275.29346Train finished\n",
      "Selected features are: [33 46 56]\n",
      "Selected weights are: [0.35479647 0.99894255 0.99894255]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.011699444006396593, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 27.930345977733698\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 316.92926\n",
      "iteration 200 loss 312.90738\n",
      "iteration 300 loss 312.5865\n",
      "iteration 400 loss 312.57632\n",
      "iteration 500 loss 312.57184\n",
      "iteration 600 loss 312.56958\n",
      "iteration 700 loss 312.56842\n",
      "iteration 800 loss 312.56787\n",
      "iteration 900 loss 312.5675\n",
      "iteration 1000 loss 312.56732\n",
      "iteration 1100 loss 312.5673\n",
      "iteration 1200 loss 312.56726\n",
      "iteration 1300 loss 312.56726\n",
      "iteration 1400 loss 312.56723\n",
      "iteration 1500 loss 312.56723\n",
      "iteration 1600 loss 312.5672\n",
      "iteration 1700 loss 312.5672\n",
      "iteration 1800 loss 312.5672\n",
      "iteration 1900 loss 312.56723\n",
      "iteration 2000 loss 312.5672\n",
      "iteration 2100 loss 312.56726\n",
      "iteration 2200 loss 312.5672\n",
      "iteration 2300 loss 312.56723\n",
      "iteration 2400 loss 312.5672\n",
      "iteration 2500 loss 312.56726\n",
      "iteration 2600 loss 312.56726\n",
      "iteration 2700 loss 312.56723\n",
      "iteration 2800 loss 312.56726\n",
      "iteration 2900 loss 312.56723\n",
      "iteration 3000 loss 312.5672\n",
      "iteration 3100 loss 312.5672\n",
      "iteration 3200 loss 312.56726\n",
      "iteration 3300 loss 312.5672\n",
      "iteration 3400 loss 312.5672\n",
      "iteration 3500 loss 312.5672\n",
      "iteration 3600 loss 312.5672\n",
      "iteration 3700 loss 312.5672\n",
      "iteration 3800 loss 312.5672\n",
      "iteration 3900 loss 312.5672\n",
      "iteration 4000 loss 312.5672\n",
      "iteration 4100 loss 312.5672\n",
      "iteration 4200 loss 312.5672\n",
      "iteration 4300 loss 312.5672\n",
      "iteration 4400 loss 312.5672\n",
      "iteration 4500 loss 312.5672\n",
      "iteration 4600 loss 312.5672\n",
      "iteration 4700 loss 312.5672\n",
      "iteration 4800 loss 312.5672\n",
      "iteration 4900 loss 312.5672\n",
      "iteration 5000 loss 312.5672Train finished\n",
      "Selected features are: [15 40 48]\n",
      "Selected weights are: [0.99647665 0.48436004 0.8472771 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0013490287294279583, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 11.547321489169295\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 228.16507\n",
      "iteration 200 loss 220.01257\n",
      "iteration 300 loss 217.84612\n",
      "iteration 400 loss 217.73883\n",
      "iteration 500 loss 217.72974\n",
      "iteration 600 loss 217.72972\n",
      "iteration 700 loss 217.7297\n",
      "iteration 800 loss 217.72974\n",
      "iteration 900 loss 217.72974\n",
      "iteration 1000 loss 217.72972\n",
      "iteration 1100 loss 217.7297\n",
      "iteration 1200 loss 217.7297\n",
      "iteration 1300 loss 217.7297\n",
      "iteration 1400 loss 217.72972\n",
      "iteration 1500 loss 217.72972\n",
      "iteration 1600 loss 217.72969\n",
      "iteration 1700 loss 217.7297\n",
      "iteration 1800 loss 217.72972\n",
      "iteration 1900 loss 217.72972\n",
      "iteration 2000 loss 217.7297\n",
      "iteration 2100 loss 217.7297\n",
      "iteration 2200 loss 217.7297\n",
      "iteration 2300 loss 217.7297\n",
      "iteration 2400 loss 217.7297\n",
      "iteration 2500 loss 217.7297\n",
      "iteration 2600 loss 217.7297\n",
      "iteration 2700 loss 217.7297\n",
      "iteration 2800 loss 217.7297\n",
      "iteration 2900 loss 217.7297\n",
      "iteration 3000 loss 217.7297\n",
      "iteration 3100 loss 217.7297\n",
      "iteration 3200 loss 217.7297\n",
      "iteration 3300 loss 217.7297\n",
      "iteration 3400 loss 217.7297\n",
      "iteration 3500 loss 217.7297\n",
      "iteration 3600 loss 217.7297\n",
      "iteration 3700 loss 217.7297\n",
      "iteration 3800 loss 217.7297\n",
      "iteration 3900 loss 217.7297\n",
      "iteration 4000 loss 217.7297\n",
      "iteration 4100 loss 217.7297\n",
      "iteration 4200 loss 217.7297\n",
      "iteration 4300 loss 217.7297\n",
      "iteration 4400 loss 217.7297\n",
      "iteration 4500 loss 217.7297\n",
      "iteration 4600 loss 217.7297\n",
      "iteration 4700 loss 217.7297\n",
      "iteration 4800 loss 217.7297\n",
      "iteration 4900 loss 217.7297\n",
      "iteration 5000 loss 217.7297Train finished\n",
      "Selected features are: [ 7 11 46]\n",
      "Selected weights are: [0.9744092  0.6050759  0.99684143]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 9.149122040997293e-05, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 51.68202181301354\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 274.77673\n",
      "iteration 200 loss 270.4214\n",
      "iteration 300 loss 269.8061\n",
      "iteration 400 loss 269.40686\n",
      "iteration 500 loss 269.32452\n",
      "iteration 600 loss 269.32437\n",
      "iteration 700 loss 269.32434\n",
      "iteration 800 loss 269.32434\n",
      "iteration 900 loss 269.32434\n",
      "iteration 1000 loss 269.32434\n",
      "iteration 1100 loss 269.32434\n",
      "iteration 1200 loss 269.32434\n",
      "iteration 1300 loss 269.32434\n",
      "iteration 1400 loss 269.32434\n",
      "iteration 1500 loss 269.32434\n",
      "iteration 1600 loss 269.32434\n",
      "iteration 1700 loss 269.32434\n",
      "iteration 1800 loss 269.32434\n",
      "iteration 1900 loss 269.32434\n",
      "iteration 2000 loss 269.32434\n",
      "iteration 2100 loss 269.32434\n",
      "iteration 2200 loss 269.32434\n",
      "iteration 2300 loss 269.32434\n",
      "iteration 2400 loss 269.32434\n",
      "iteration 2500 loss 269.32434\n",
      "iteration 2600 loss 269.32434\n",
      "iteration 2700 loss 269.32434\n",
      "iteration 2800 loss 269.32434\n",
      "iteration 2900 loss 269.32434\n",
      "iteration 3000 loss 269.32434\n",
      "iteration 3100 loss 269.32434\n",
      "iteration 3200 loss 269.32434\n",
      "iteration 3300 loss 269.32434\n",
      "iteration 3400 loss 269.32434\n",
      "iteration 3500 loss 269.32434\n",
      "iteration 3600 loss 269.32434\n",
      "iteration 3700 loss 269.32434\n",
      "iteration 3800 loss 269.32434\n",
      "iteration 3900 loss 269.32434\n",
      "iteration 4000 loss 269.32434\n",
      "iteration 4100 loss 269.32434\n",
      "iteration 4200 loss 269.32434\n",
      "iteration 4300 loss 269.32434\n",
      "iteration 4400 loss 269.32434\n",
      "iteration 4500 loss 269.32434\n",
      "iteration 4600 loss 269.32434\n",
      "iteration 4700 loss 269.32434\n",
      "iteration 4800 loss 269.32434\n",
      "iteration 4900 loss 269.32434\n",
      "iteration 5000 loss 269.32434Train finished\n",
      "Selected features are: [33 46 56]\n",
      "Selected weights are: [0.67606825 0.7167607  0.9970017 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.008588085826539683, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 29.888602869803936\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 281.38947\n",
      "iteration 200 loss 279.89517\n",
      "iteration 300 loss 279.57104\n",
      "iteration 400 loss 279.4682\n",
      "iteration 500 loss 279.45642\n",
      "iteration 600 loss 279.45016\n",
      "iteration 700 loss 279.44684\n",
      "iteration 800 loss 279.44495\n",
      "iteration 900 loss 279.44397\n",
      "iteration 1000 loss 279.44333\n",
      "iteration 1100 loss 279.44302\n",
      "iteration 1200 loss 279.44278\n",
      "iteration 1300 loss 279.44275\n",
      "iteration 1400 loss 279.44275\n",
      "iteration 1500 loss 279.44272\n",
      "iteration 1600 loss 279.44275\n",
      "iteration 1700 loss 279.4427\n",
      "iteration 1800 loss 279.44272\n",
      "iteration 1900 loss 279.44275\n",
      "iteration 2000 loss 279.4427\n",
      "iteration 2100 loss 279.44275\n",
      "iteration 2200 loss 279.44275\n",
      "iteration 2300 loss 279.44275\n",
      "iteration 2400 loss 279.44272\n",
      "iteration 2500 loss 279.44272\n",
      "iteration 2600 loss 279.44272\n",
      "iteration 2700 loss 279.44275\n",
      "iteration 2800 loss 279.44272\n",
      "iteration 2900 loss 279.44275\n",
      "iteration 3000 loss 279.44275\n",
      "iteration 3100 loss 279.4427\n",
      "iteration 3200 loss 279.44272\n",
      "iteration 3300 loss 279.44272\n",
      "iteration 3400 loss 279.44272\n",
      "iteration 3500 loss 279.44275\n",
      "iteration 3600 loss 279.44272\n",
      "iteration 3700 loss 279.44275\n",
      "iteration 3800 loss 279.4427\n",
      "iteration 3900 loss 279.44272\n",
      "iteration 4000 loss 279.44275\n",
      "iteration 4100 loss 279.4427\n",
      "iteration 4200 loss 279.44275\n",
      "iteration 4300 loss 279.44275\n",
      "iteration 4400 loss 279.44275\n",
      "iteration 4500 loss 279.44272\n",
      "iteration 4600 loss 279.44272\n",
      "iteration 4700 loss 279.44272\n",
      "iteration 4800 loss 279.44275\n",
      "iteration 4900 loss 279.44272\n",
      "iteration 5000 loss 279.44275Train finished\n",
      "Selected features are: [14 15 48]\n",
      "Selected weights are: [0.4311849 0.996225  0.996225 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0008200225640156111, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 23.496671899306445\n",
      "\n",
      " 6 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 233.36258\n",
      "iteration 200 loss 230.43524\n",
      "iteration 300 loss 230.16013\n",
      "iteration 400 loss 230.14304\n",
      "iteration 500 loss 230.1389\n",
      "iteration 600 loss 230.13751\n",
      "iteration 700 loss 230.13684\n",
      "iteration 800 loss 230.1365\n",
      "iteration 900 loss 230.13629\n",
      "iteration 1000 loss 230.13618\n",
      "iteration 1100 loss 230.13611\n",
      "iteration 1200 loss 230.13606\n",
      "iteration 1300 loss 230.13605\n",
      "iteration 1400 loss 230.13603\n",
      "iteration 1500 loss 230.13606\n",
      "iteration 1600 loss 230.13605\n",
      "iteration 1700 loss 230.13605\n",
      "iteration 1800 loss 230.13602\n",
      "iteration 1900 loss 230.13602\n",
      "iteration 2000 loss 230.13602\n",
      "iteration 2100 loss 230.13602\n",
      "iteration 2200 loss 230.13602\n",
      "iteration 2300 loss 230.13602\n",
      "iteration 2400 loss 230.13599\n",
      "iteration 2500 loss 230.13603\n",
      "iteration 2600 loss 230.13603\n",
      "iteration 2700 loss 230.13603\n",
      "iteration 2800 loss 230.13603\n",
      "iteration 2900 loss 230.13603\n",
      "iteration 3000 loss 230.13602\n",
      "iteration 3100 loss 230.13603\n",
      "iteration 3200 loss 230.13602\n",
      "iteration 3300 loss 230.13602\n",
      "iteration 3400 loss 230.13602\n",
      "iteration 3500 loss 230.13602\n",
      "iteration 3600 loss 230.13602\n",
      "iteration 3700 loss 230.13602\n",
      "iteration 3800 loss 230.13602\n",
      "iteration 3900 loss 230.13602\n",
      "iteration 4000 loss 230.13602\n",
      "iteration 4100 loss 230.13602\n",
      "iteration 4200 loss 230.13602\n",
      "iteration 4300 loss 230.13602\n",
      "iteration 4400 loss 230.13602\n",
      "iteration 4500 loss 230.13602\n",
      "iteration 4600 loss 230.13602\n",
      "iteration 4700 loss 230.13602\n",
      "iteration 4800 loss 230.13602\n",
      "iteration 4900 loss 230.13602\n",
      "iteration 5000 loss 230.13602Train finished\n",
      "Selected features are: [15 40 48]\n",
      "Selected weights are: [0.77490664 0.82794666 0.7624665 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0016270711074342357, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 43.888546252595155\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 275.63727\n",
      "iteration 200 loss 270.07968\n",
      "iteration 300 loss 268.6009\n",
      "iteration 400 loss 267.71875\n",
      "iteration 500 loss 267.6962\n",
      "iteration 600 loss 267.69028\n",
      "iteration 700 loss 267.68875\n",
      "iteration 800 loss 267.6883\n",
      "iteration 900 loss 267.68817\n",
      "iteration 1000 loss 267.68817\n",
      "iteration 1100 loss 267.68817\n",
      "iteration 1200 loss 267.68817\n",
      "iteration 1300 loss 267.68817\n",
      "iteration 1400 loss 267.68817\n",
      "iteration 1500 loss 267.68817\n",
      "iteration 1600 loss 267.68817\n",
      "iteration 1700 loss 267.68817\n",
      "iteration 1800 loss 267.68817\n",
      "iteration 1900 loss 267.68817\n",
      "iteration 2000 loss 267.68817\n",
      "iteration 2100 loss 267.68817\n",
      "iteration 2200 loss 267.68817\n",
      "iteration 2300 loss 267.68817\n",
      "iteration 2400 loss 267.68817\n",
      "iteration 2500 loss 267.68817\n",
      "iteration 2600 loss 267.68817\n",
      "iteration 2700 loss 267.68817\n",
      "iteration 2800 loss 267.68817\n",
      "iteration 2900 loss 267.68817\n",
      "iteration 3000 loss 267.68817\n",
      "iteration 3100 loss 267.68817\n",
      "iteration 3200 loss 267.68817\n",
      "iteration 3300 loss 267.68817\n",
      "iteration 3400 loss 267.68817\n",
      "iteration 3500 loss 267.68817\n",
      "iteration 3600 loss 267.68817\n",
      "iteration 3700 loss 267.68817\n",
      "iteration 3800 loss 267.68817\n",
      "iteration 3900 loss 267.68817\n",
      "iteration 4000 loss 267.68817\n",
      "iteration 4100 loss 267.68817\n",
      "iteration 4200 loss 267.68817\n",
      "iteration 4300 loss 267.68817\n",
      "iteration 4400 loss 267.68817\n",
      "iteration 4500 loss 267.68817\n",
      "iteration 4600 loss 267.68817\n",
      "iteration 4700 loss 267.68817\n",
      "iteration 4800 loss 267.68817\n",
      "iteration 4900 loss 267.68817\n",
      "iteration 5000 loss 267.68817Train finished\n",
      "Selected features are: [46 51 56]\n",
      "Selected weights are: [0.85413533 0.642121   0.9959915 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00656227274809116, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 29.7259074288473\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 260.73328\n",
      "iteration 200 loss 254.20752\n",
      "iteration 300 loss 253.5775\n",
      "iteration 400 loss 253.4343\n",
      "iteration 500 loss 253.33871\n",
      "iteration 600 loss 253.27058\n",
      "iteration 700 loss 253.22365\n",
      "iteration 800 loss 253.20078\n",
      "iteration 900 loss 253.1966\n",
      "iteration 1000 loss 253.19406\n",
      "iteration 1100 loss 253.19211\n",
      "iteration 1200 loss 253.19193\n",
      "iteration 1300 loss 253.19193\n",
      "iteration 1400 loss 253.19196\n",
      "iteration 1500 loss 253.19196\n",
      "iteration 1600 loss 253.19199\n",
      "iteration 1700 loss 253.19199\n",
      "iteration 1800 loss 253.19199\n",
      "iteration 1900 loss 253.19202\n",
      "iteration 2000 loss 253.19202\n",
      "iteration 2100 loss 253.19202\n",
      "iteration 2200 loss 253.19202\n",
      "iteration 2300 loss 253.19202\n",
      "iteration 2400 loss 253.19202\n",
      "iteration 2500 loss 253.19202\n",
      "iteration 2600 loss 253.19202\n",
      "iteration 2700 loss 253.19202\n",
      "iteration 2800 loss 253.19202\n",
      "iteration 2900 loss 253.19202\n",
      "iteration 3000 loss 253.19202\n",
      "iteration 3100 loss 253.19202\n",
      "iteration 3200 loss 253.19202\n",
      "iteration 3300 loss 253.19202\n",
      "iteration 3400 loss 253.19202\n",
      "iteration 3500 loss 253.19202\n",
      "iteration 3600 loss 253.19202\n",
      "iteration 3700 loss 253.19202\n",
      "iteration 3800 loss 253.19202\n",
      "iteration 3900 loss 253.19202\n",
      "iteration 4000 loss 253.19202\n",
      "iteration 4100 loss 253.19202\n",
      "iteration 4200 loss 253.19202\n",
      "iteration 4300 loss 253.19202\n",
      "iteration 4400 loss 253.19202\n",
      "iteration 4500 loss 253.19202\n",
      "iteration 4600 loss 253.19202\n",
      "iteration 4700 loss 253.19202\n",
      "iteration 4800 loss 253.19202\n",
      "iteration 4900 loss 253.19202\n",
      "iteration 5000 loss 253.19202Train finished\n",
      "Selected features are: [14 15 48]\n",
      "Selected weights are: [0.47148788 0.9986389  0.9986389 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0007271890502852314, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 37.28462177110316\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 318.3909\n",
      "iteration 200 loss 315.7568\n",
      "iteration 300 loss 315.57996\n",
      "iteration 400 loss 315.5722\n",
      "iteration 500 loss 315.56873\n",
      "iteration 600 loss 315.56714\n",
      "iteration 700 loss 315.5664\n",
      "iteration 800 loss 315.56604\n",
      "iteration 900 loss 315.56586\n",
      "iteration 1000 loss 315.56573\n",
      "iteration 1100 loss 315.56573\n",
      "iteration 1200 loss 315.56573\n",
      "iteration 1300 loss 315.56573\n",
      "iteration 1400 loss 315.56573\n",
      "iteration 1500 loss 315.5657\n",
      "iteration 1600 loss 315.56573\n",
      "iteration 1700 loss 315.56573\n",
      "iteration 1800 loss 315.5657\n",
      "iteration 1900 loss 315.56573\n",
      "iteration 2000 loss 315.56573\n",
      "iteration 2100 loss 315.56573\n",
      "iteration 2200 loss 315.56567\n",
      "iteration 2300 loss 315.56573\n",
      "iteration 2400 loss 315.56573\n",
      "iteration 2500 loss 315.56573\n",
      "iteration 2600 loss 315.56573\n",
      "iteration 2700 loss 315.56573\n",
      "iteration 2800 loss 315.56573\n",
      "iteration 2900 loss 315.56573\n",
      "iteration 3000 loss 315.56573\n",
      "iteration 3100 loss 315.56573\n",
      "iteration 3200 loss 315.56573\n",
      "iteration 3300 loss 315.56573\n",
      "iteration 3400 loss 315.56573\n",
      "iteration 3500 loss 315.56573\n",
      "iteration 3600 loss 315.56573\n",
      "iteration 3700 loss 315.56573\n",
      "iteration 3800 loss 315.56573\n",
      "iteration 3900 loss 315.56573\n",
      "iteration 4000 loss 315.56573\n",
      "iteration 4100 loss 315.56573\n",
      "iteration 4200 loss 315.56573\n",
      "iteration 4300 loss 315.56573\n",
      "iteration 4400 loss 315.56573\n",
      "iteration 4500 loss 315.56573\n",
      "iteration 4600 loss 315.56573\n",
      "iteration 4700 loss 315.56573\n",
      "iteration 4800 loss 315.56573\n",
      "iteration 4900 loss 315.56573\n",
      "iteration 5000 loss 315.56573Train finished\n",
      "Selected features are: [15 40 48]\n",
      "Selected weights are: [0.9949085  0.50492877 0.9320742 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0014471670102451752, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 8.774938849841124\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 292.70792\n",
      "iteration 200 loss 287.55756\n",
      "iteration 300 loss 287.07202\n",
      "iteration 400 loss 286.9615\n",
      "iteration 500 loss 286.9299\n",
      "iteration 600 loss 286.91394\n",
      "iteration 700 loss 286.90256\n",
      "iteration 800 loss 286.89172\n",
      "iteration 900 loss 286.88007\n",
      "iteration 1000 loss 286.86734\n",
      "iteration 1100 loss 286.8568\n",
      "iteration 1200 loss 286.84875\n",
      "iteration 1300 loss 286.8408\n",
      "iteration 1400 loss 286.83255\n",
      "iteration 1500 loss 286.82376\n",
      "iteration 1600 loss 286.81427\n",
      "iteration 1700 loss 286.80377\n",
      "iteration 1800 loss 286.7925\n",
      "iteration 1900 loss 286.78015\n",
      "iteration 2000 loss 286.76666\n",
      "iteration 2100 loss 286.752\n",
      "iteration 2200 loss 286.73694\n",
      "iteration 2300 loss 286.73035\n",
      "iteration 2400 loss 286.72546\n",
      "iteration 2500 loss 286.72186\n",
      "iteration 2600 loss 286.71924\n",
      "iteration 2700 loss 286.71735\n",
      "iteration 2800 loss 286.71594\n",
      "iteration 2900 loss 286.71497\n",
      "iteration 3000 loss 286.71417\n",
      "iteration 3100 loss 286.71368\n",
      "iteration 3200 loss 286.71332\n",
      "iteration 3300 loss 286.713\n",
      "iteration 3400 loss 286.71283\n",
      "iteration 3500 loss 286.71274\n",
      "iteration 3600 loss 286.71265\n",
      "iteration 3700 loss 286.71252\n",
      "iteration 3800 loss 286.7125\n",
      "iteration 3900 loss 286.7125\n",
      "iteration 4000 loss 286.7125\n",
      "iteration 4100 loss 286.71246\n",
      "iteration 4200 loss 286.71246\n",
      "iteration 4300 loss 286.7124\n",
      "iteration 4400 loss 286.71243\n",
      "iteration 4500 loss 286.71243\n",
      "iteration 4600 loss 286.71243\n",
      "iteration 4700 loss 286.71243\n",
      "iteration 4800 loss 286.71243\n",
      "iteration 4900 loss 286.71243\n",
      "iteration 5000 loss 286.71243Train finished\n",
      "Selected features are: [46 56 70]\n",
      "Selected weights are: [0.799146   0.99772274 0.827701  ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.010779666421613687, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 24.032050966438167\n",
      "\n",
      " 7 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 318.12524\n",
      "iteration 200 loss 314.31247\n",
      "iteration 300 loss 313.7022\n",
      "iteration 400 loss 313.15018\n",
      "iteration 500 loss 312.35864\n",
      "iteration 600 loss 312.15875\n",
      "iteration 700 loss 312.1554\n",
      "iteration 800 loss 312.15314\n",
      "iteration 900 loss 312.15314\n",
      "iteration 1000 loss 312.1531\n",
      "iteration 1100 loss 312.15314\n",
      "iteration 1200 loss 312.15317\n",
      "iteration 1300 loss 312.15314\n",
      "iteration 1400 loss 312.15314\n",
      "iteration 1500 loss 312.1531\n",
      "iteration 1600 loss 312.1531\n",
      "iteration 1700 loss 312.1531\n",
      "iteration 1800 loss 312.15314\n",
      "iteration 1900 loss 312.1531\n",
      "iteration 2000 loss 312.15314\n",
      "iteration 2100 loss 312.1531\n",
      "iteration 2200 loss 312.15308\n",
      "iteration 2300 loss 312.1531\n",
      "iteration 2400 loss 312.1531\n",
      "iteration 2500 loss 312.1531\n",
      "iteration 2600 loss 312.1531\n",
      "iteration 2700 loss 312.1531\n",
      "iteration 2800 loss 312.1531\n",
      "iteration 2900 loss 312.1531\n",
      "iteration 3000 loss 312.1531\n",
      "iteration 3100 loss 312.1531\n",
      "iteration 3200 loss 312.1531\n",
      "iteration 3300 loss 312.1531\n",
      "iteration 3400 loss 312.1531\n",
      "iteration 3500 loss 312.1531\n",
      "iteration 3600 loss 312.1531\n",
      "iteration 3700 loss 312.1531\n",
      "iteration 3800 loss 312.1531\n",
      "iteration 3900 loss 312.1531\n",
      "iteration 4000 loss 312.1531\n",
      "iteration 4100 loss 312.1531\n",
      "iteration 4200 loss 312.1531\n",
      "iteration 4300 loss 312.1531\n",
      "iteration 4400 loss 312.1531\n",
      "iteration 4500 loss 312.1531\n",
      "iteration 4600 loss 312.1531\n",
      "iteration 4700 loss 312.1531\n",
      "iteration 4800 loss 312.1531\n",
      "iteration 4900 loss 312.1531\n",
      "iteration 5000 loss 312.1531Train finished\n",
      "Selected features are: [33 51 56]\n",
      "Selected weights are: [0.99770796 0.9518986  0.99770796]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.030860311215155565, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 13.122056674445119\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 273.68472\n",
      "iteration 200 loss 271.40015\n",
      "iteration 300 loss 271.24426\n",
      "iteration 400 loss 271.2339\n",
      "iteration 500 loss 271.22668\n",
      "iteration 600 loss 271.2215\n",
      "iteration 700 loss 271.21783\n",
      "iteration 800 loss 271.21527\n",
      "iteration 900 loss 271.21347\n",
      "iteration 1000 loss 271.21225\n",
      "iteration 1100 loss 271.2114\n",
      "iteration 1200 loss 271.21085\n",
      "iteration 1300 loss 271.21045\n",
      "iteration 1400 loss 271.2102\n",
      "iteration 1500 loss 271.21002\n",
      "iteration 1600 loss 271.2099\n",
      "iteration 1700 loss 271.20987\n",
      "iteration 1800 loss 271.2098\n",
      "iteration 1900 loss 271.20978\n",
      "iteration 2000 loss 271.20975\n",
      "iteration 2100 loss 271.20975\n",
      "iteration 2200 loss 271.20972\n",
      "iteration 2300 loss 271.20972\n",
      "iteration 2400 loss 271.20972\n",
      "iteration 2500 loss 271.20972\n",
      "iteration 2600 loss 271.2097\n",
      "iteration 2700 loss 271.20972\n",
      "iteration 2800 loss 271.20972\n",
      "iteration 2900 loss 271.20972\n",
      "iteration 3000 loss 271.20972\n",
      "iteration 3100 loss 271.20972\n",
      "iteration 3200 loss 271.20972\n",
      "iteration 3300 loss 271.20972\n",
      "iteration 3400 loss 271.20972\n",
      "iteration 3500 loss 271.20972\n",
      "iteration 3600 loss 271.20972\n",
      "iteration 3700 loss 271.20972\n",
      "iteration 3800 loss 271.20972\n",
      "iteration 3900 loss 271.20972\n",
      "iteration 4000 loss 271.20972\n",
      "iteration 4100 loss 271.2097\n",
      "iteration 4200 loss 271.2097\n",
      "iteration 4300 loss 271.20972\n",
      "iteration 4400 loss 271.20972\n",
      "iteration 4500 loss 271.20975\n",
      "iteration 4600 loss 271.20972\n",
      "iteration 4700 loss 271.20972\n",
      "iteration 4800 loss 271.20972\n",
      "iteration 4900 loss 271.20972\n",
      "iteration 5000 loss 271.20972Train finished\n",
      "Selected features are: [15 40 48]\n",
      "Selected weights are: [0.8380884 0.6004096 0.9417015]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0014095709924364188, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 27.900859993733768\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 272.62018\n",
      "iteration 200 loss 260.46677\n",
      "iteration 300 loss 259.72232\n",
      "iteration 400 loss 259.59946\n",
      "iteration 500 loss 259.54913\n",
      "iteration 600 loss 259.54718\n",
      "iteration 700 loss 259.54675\n",
      "iteration 800 loss 259.54663\n",
      "iteration 900 loss 259.54663\n",
      "iteration 1000 loss 259.54657\n",
      "iteration 1100 loss 259.54657\n",
      "iteration 1200 loss 259.54657\n",
      "iteration 1300 loss 259.54657\n",
      "iteration 1400 loss 259.54657\n",
      "iteration 1500 loss 259.5466\n",
      "iteration 1600 loss 259.5466\n",
      "iteration 1700 loss 259.54657\n",
      "iteration 1800 loss 259.5466\n",
      "iteration 1900 loss 259.5466\n",
      "iteration 2000 loss 259.54657\n",
      "iteration 2100 loss 259.54657\n",
      "iteration 2200 loss 259.5466\n",
      "iteration 2300 loss 259.54657\n",
      "iteration 2400 loss 259.54657\n",
      "iteration 2500 loss 259.54657\n",
      "iteration 2600 loss 259.54657\n",
      "iteration 2700 loss 259.54657\n",
      "iteration 2800 loss 259.54657\n",
      "iteration 2900 loss 259.54657\n",
      "iteration 3000 loss 259.54657\n",
      "iteration 3100 loss 259.54657\n",
      "iteration 3200 loss 259.54657\n",
      "iteration 3300 loss 259.54657\n",
      "iteration 3400 loss 259.54657\n",
      "iteration 3500 loss 259.54657\n",
      "iteration 3600 loss 259.54657\n",
      "iteration 3700 loss 259.54657\n",
      "iteration 3800 loss 259.54657\n",
      "iteration 3900 loss 259.54657\n",
      "iteration 4000 loss 259.54657\n",
      "iteration 4100 loss 259.54657\n",
      "iteration 4200 loss 259.54657\n",
      "iteration 4300 loss 259.54657\n",
      "iteration 4400 loss 259.54657\n",
      "iteration 4500 loss 259.54657\n",
      "iteration 4600 loss 259.54657\n",
      "iteration 4700 loss 259.54657\n",
      "iteration 4800 loss 259.54657\n",
      "iteration 4900 loss 259.54657\n",
      "iteration 5000 loss 259.54657Train finished\n",
      "Selected features are: [ 7 46 56]\n",
      "Selected weights are: [0.34603083 0.9309653  0.99744236]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 9.838449167349902e-05, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 33.600634280183826\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 263.67273\n",
      "iteration 200 loss 261.49924\n",
      "iteration 300 loss 261.39175\n",
      "iteration 400 loss 261.34012\n",
      "iteration 500 loss 261.31012\n",
      "iteration 600 loss 261.30457\n",
      "iteration 700 loss 261.30438\n",
      "iteration 800 loss 261.3044\n",
      "iteration 900 loss 261.30444\n",
      "iteration 1000 loss 261.30444\n",
      "iteration 1100 loss 261.3044\n",
      "iteration 1200 loss 261.3044\n",
      "iteration 1300 loss 261.30444\n",
      "iteration 1400 loss 261.3044\n",
      "iteration 1500 loss 261.3044\n",
      "iteration 1600 loss 261.30444\n",
      "iteration 1700 loss 261.3044\n",
      "iteration 1800 loss 261.3044\n",
      "iteration 1900 loss 261.30444\n",
      "iteration 2000 loss 261.3044\n",
      "iteration 2100 loss 261.3044\n",
      "iteration 2200 loss 261.30444\n",
      "iteration 2300 loss 261.3044\n",
      "iteration 2400 loss 261.3044\n",
      "iteration 2500 loss 261.30444\n",
      "iteration 2600 loss 261.3044\n",
      "iteration 2700 loss 261.3044\n",
      "iteration 2800 loss 261.30444\n",
      "iteration 2900 loss 261.3044\n",
      "iteration 3000 loss 261.3044\n",
      "iteration 3100 loss 261.30444\n",
      "iteration 3200 loss 261.3044\n",
      "iteration 3300 loss 261.3044\n",
      "iteration 3400 loss 261.30444\n",
      "iteration 3500 loss 261.3044\n",
      "iteration 3600 loss 261.3044\n",
      "iteration 3700 loss 261.30444\n",
      "iteration 3800 loss 261.3044\n",
      "iteration 3900 loss 261.3044\n",
      "iteration 4000 loss 261.30444\n",
      "iteration 4100 loss 261.3044\n",
      "iteration 4200 loss 261.3044\n",
      "iteration 4300 loss 261.30444\n",
      "iteration 4400 loss 261.3044\n",
      "iteration 4500 loss 261.3044\n",
      "iteration 4600 loss 261.30444\n",
      "iteration 4700 loss 261.3044\n",
      "iteration 4800 loss 261.3044\n",
      "iteration 4900 loss 261.30444\n",
      "iteration 5000 loss 261.3044Train finished\n",
      "Selected features are: [14 15 48]\n",
      "Selected weights are: [0.48111394 0.99488777 0.99488777]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0006997641936776057, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 28.93318270258742\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 254.58797\n",
      "iteration 200 loss 251.07364\n",
      "iteration 300 loss 246.52103\n",
      "iteration 400 loss 245.89496\n",
      "iteration 500 loss 245.55841\n",
      "iteration 600 loss 245.51343\n",
      "iteration 700 loss 245.5036\n",
      "iteration 800 loss 245.50244\n",
      "iteration 900 loss 245.50227\n",
      "iteration 1000 loss 245.50235\n",
      "iteration 1100 loss 245.50233\n",
      "iteration 1200 loss 245.50232\n",
      "iteration 1300 loss 245.50235\n",
      "iteration 1400 loss 245.50229\n",
      "iteration 1500 loss 245.50232\n",
      "iteration 1600 loss 245.50232\n",
      "iteration 1700 loss 245.50233\n",
      "iteration 1800 loss 245.50232\n",
      "iteration 1900 loss 245.5023\n",
      "iteration 2000 loss 245.50232\n",
      "iteration 2100 loss 245.50232\n",
      "iteration 2200 loss 245.50233\n",
      "iteration 2300 loss 245.50232\n",
      "iteration 2400 loss 245.50229\n",
      "iteration 2500 loss 245.5023\n",
      "iteration 2600 loss 245.50227\n",
      "iteration 2700 loss 245.50229\n",
      "iteration 2800 loss 245.50232\n",
      "iteration 2900 loss 245.50232\n",
      "iteration 3000 loss 245.50232\n",
      "iteration 3100 loss 245.50232\n",
      "iteration 3200 loss 245.5023\n",
      "iteration 3300 loss 245.5023\n",
      "iteration 3400 loss 245.50232\n",
      "iteration 3500 loss 245.50232\n",
      "iteration 3600 loss 245.50232\n",
      "iteration 3700 loss 245.50232\n",
      "iteration 3800 loss 245.50233\n",
      "iteration 3900 loss 245.50232\n",
      "iteration 4000 loss 245.50232\n",
      "iteration 4100 loss 245.50232\n",
      "iteration 4200 loss 245.50232\n",
      "iteration 4300 loss 245.5023\n",
      "iteration 4400 loss 245.50232\n",
      "iteration 4500 loss 245.5023\n",
      "iteration 4600 loss 245.50232\n",
      "iteration 4700 loss 245.50232\n",
      "iteration 4800 loss 245.50232\n",
      "iteration 4900 loss 245.50232\n",
      "iteration 5000 loss 245.50232Train finished\n",
      "Selected features are: [ 8 15 48]\n",
      "Selected weights are: [0.76097304 0.9976248  0.6447329 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.001699806208422476, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 41.76872955472099\n",
      "\n",
      " 8 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 275.8221\n",
      "iteration 200 loss 272.79926\n",
      "iteration 300 loss 272.61694\n",
      "iteration 400 loss 272.61343\n",
      "iteration 500 loss 272.61176\n",
      "iteration 600 loss 272.61093\n",
      "iteration 700 loss 272.61047\n",
      "iteration 800 loss 272.61026\n",
      "iteration 900 loss 272.61017\n",
      "iteration 1000 loss 272.61014\n",
      "iteration 1100 loss 272.61014\n",
      "iteration 1200 loss 272.6101\n",
      "iteration 1300 loss 272.61005\n",
      "iteration 1400 loss 272.61008\n",
      "iteration 1500 loss 272.6101\n",
      "iteration 1600 loss 272.61005\n",
      "iteration 1700 loss 272.61008\n",
      "iteration 1800 loss 272.61008\n",
      "iteration 1900 loss 272.6101\n",
      "iteration 2000 loss 272.61005\n",
      "iteration 2100 loss 272.6101\n",
      "iteration 2200 loss 272.61008\n",
      "iteration 2300 loss 272.61008\n",
      "iteration 2400 loss 272.6101\n",
      "iteration 2500 loss 272.6101\n",
      "iteration 2600 loss 272.6101\n",
      "iteration 2700 loss 272.6101\n",
      "iteration 2800 loss 272.61008\n",
      "iteration 2900 loss 272.61008\n",
      "iteration 3000 loss 272.61008\n",
      "iteration 3100 loss 272.61008\n",
      "iteration 3200 loss 272.61008\n",
      "iteration 3300 loss 272.61008\n",
      "iteration 3400 loss 272.61008\n",
      "iteration 3500 loss 272.61008\n",
      "iteration 3600 loss 272.61008\n",
      "iteration 3700 loss 272.61008\n",
      "iteration 3800 loss 272.61008\n",
      "iteration 3900 loss 272.61008\n",
      "iteration 4000 loss 272.61008\n",
      "iteration 4100 loss 272.61008\n",
      "iteration 4200 loss 272.61008\n",
      "iteration 4300 loss 272.61008\n",
      "iteration 4400 loss 272.61008\n",
      "iteration 4500 loss 272.61008\n",
      "iteration 4600 loss 272.61008\n",
      "iteration 4700 loss 272.61008\n",
      "iteration 4800 loss 272.61008\n",
      "iteration 4900 loss 272.61008\n",
      "iteration 5000 loss 272.61008Train finished\n",
      "Selected features are: [15 40 48]\n",
      "Selected weights are: [0.9949732  0.52362096 0.77539414]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0016612522730275134, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 26.514981838602306\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 286.73303\n",
      "iteration 200 loss 281.99023\n",
      "iteration 300 loss 281.94742\n",
      "iteration 400 loss 281.9397\n",
      "iteration 500 loss 281.93573\n",
      "iteration 600 loss 281.93372\n",
      "iteration 700 loss 281.93262\n",
      "iteration 800 loss 281.93207\n",
      "iteration 900 loss 281.93176\n",
      "iteration 1000 loss 281.93164\n",
      "iteration 1100 loss 281.93152\n",
      "iteration 1200 loss 281.93152\n",
      "iteration 1300 loss 281.93146\n",
      "iteration 1400 loss 281.9314\n",
      "iteration 1500 loss 281.93143\n",
      "iteration 1600 loss 281.93146\n",
      "iteration 1700 loss 281.93143\n",
      "iteration 1800 loss 281.93143\n",
      "iteration 1900 loss 281.93146\n",
      "iteration 2000 loss 281.93146\n",
      "iteration 2100 loss 281.93146\n",
      "iteration 2200 loss 281.93146\n",
      "iteration 2300 loss 281.93143\n",
      "iteration 2400 loss 281.93143\n",
      "iteration 2500 loss 281.93143\n",
      "iteration 2600 loss 281.9314\n",
      "iteration 2700 loss 281.9314\n",
      "iteration 2800 loss 281.9314\n",
      "iteration 2900 loss 281.93143\n",
      "iteration 3000 loss 281.9314\n",
      "iteration 3100 loss 281.9314\n",
      "iteration 3200 loss 281.9314\n",
      "iteration 3300 loss 281.9314\n",
      "iteration 3400 loss 281.93143\n",
      "iteration 3500 loss 281.93143\n",
      "iteration 3600 loss 281.9314\n",
      "iteration 3700 loss 281.93143\n",
      "iteration 3800 loss 281.93143\n",
      "iteration 3900 loss 281.9314\n",
      "iteration 4000 loss 281.93143\n",
      "iteration 4100 loss 281.93143\n",
      "iteration 4200 loss 281.9314\n",
      "iteration 4300 loss 281.93143\n",
      "iteration 4400 loss 281.93143\n",
      "iteration 4500 loss 281.9314\n",
      "iteration 4600 loss 281.93143\n",
      "iteration 4700 loss 281.93143\n",
      "iteration 4800 loss 281.9314\n",
      "iteration 4900 loss 281.93143\n",
      "iteration 5000 loss 281.93143Train finished\n",
      "Selected features are: [15 40 48]\n",
      "Selected weights are: [0.9951052  0.52563643 0.943746  ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.001545705744728304, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 23.43326611920098\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 238.88687\n",
      "iteration 200 loss 230.6687\n",
      "iteration 300 loss 230.20114\n",
      "iteration 400 loss 230.14084\n",
      "iteration 500 loss 230.12253\n",
      "iteration 600 loss 230.11214\n",
      "iteration 700 loss 230.10574\n",
      "iteration 800 loss 230.10156\n",
      "iteration 900 loss 230.0987\n",
      "iteration 1000 loss 230.0965\n",
      "iteration 1100 loss 230.09491\n",
      "iteration 1200 loss 230.0936\n",
      "iteration 1300 loss 230.0926\n",
      "iteration 1400 loss 230.09174\n",
      "iteration 1500 loss 230.09106\n",
      "iteration 1600 loss 230.09045\n",
      "iteration 1700 loss 230.08997\n",
      "iteration 1800 loss 230.08957\n",
      "iteration 1900 loss 230.08917\n",
      "iteration 2000 loss 230.08891\n",
      "iteration 2100 loss 230.08862\n",
      "iteration 2200 loss 230.08838\n",
      "iteration 2300 loss 230.08823\n",
      "iteration 2400 loss 230.08803\n",
      "iteration 2500 loss 230.08789\n",
      "iteration 2600 loss 230.08777\n",
      "iteration 2700 loss 230.08768\n",
      "iteration 2800 loss 230.08757\n",
      "iteration 2900 loss 230.0875\n",
      "iteration 3000 loss 230.0874\n",
      "iteration 3100 loss 230.08734\n",
      "iteration 3200 loss 230.08725\n",
      "iteration 3300 loss 230.0872\n",
      "iteration 3400 loss 230.08719\n",
      "iteration 3500 loss 230.08713\n",
      "iteration 3600 loss 230.0871\n",
      "iteration 3700 loss 230.08704\n",
      "iteration 3800 loss 230.087\n",
      "iteration 3900 loss 230.08699\n",
      "iteration 4000 loss 230.08694\n",
      "iteration 4100 loss 230.08698\n",
      "iteration 4200 loss 230.08688\n",
      "iteration 4300 loss 230.08688\n",
      "iteration 4400 loss 230.08688\n",
      "iteration 4500 loss 230.08691\n",
      "iteration 4600 loss 230.08687\n",
      "iteration 4700 loss 230.08685\n",
      "iteration 4800 loss 230.08688\n",
      "iteration 4900 loss 230.08685\n",
      "iteration 5000 loss 230.08687Train finished\n",
      "Selected features are: [ 8 15 48]\n",
      "Selected weights are: [0.76112264 0.99747825 0.6034223 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00149359028673771, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 45.42018120717446\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 281.18713\n",
      "iteration 200 loss 271.58688\n",
      "iteration 300 loss 270.4706\n",
      "iteration 400 loss 270.3343\n",
      "iteration 500 loss 270.28754\n",
      "iteration 600 loss 270.24933\n",
      "iteration 700 loss 270.21753\n",
      "iteration 800 loss 270.1907\n",
      "iteration 900 loss 270.16794\n",
      "iteration 1000 loss 270.1529\n",
      "iteration 1100 loss 270.14865\n",
      "iteration 1200 loss 270.1485\n",
      "iteration 1300 loss 270.1485\n",
      "iteration 1400 loss 270.14844\n",
      "iteration 1500 loss 270.14844\n",
      "iteration 1600 loss 270.14844\n",
      "iteration 1700 loss 270.14844\n",
      "iteration 1800 loss 270.1484\n",
      "iteration 1900 loss 270.14844\n",
      "iteration 2000 loss 270.14844\n",
      "iteration 2100 loss 270.14847\n",
      "iteration 2200 loss 270.14844\n",
      "iteration 2300 loss 270.14844\n",
      "iteration 2400 loss 270.14844\n",
      "iteration 2500 loss 270.14844\n",
      "iteration 2600 loss 270.14844\n",
      "iteration 2700 loss 270.14844\n",
      "iteration 2800 loss 270.14844\n",
      "iteration 2900 loss 270.1484\n",
      "iteration 3000 loss 270.14844\n",
      "iteration 3100 loss 270.14844\n",
      "iteration 3200 loss 270.14844\n",
      "iteration 3300 loss 270.14844\n",
      "iteration 3400 loss 270.14844\n",
      "iteration 3500 loss 270.14844\n",
      "iteration 3600 loss 270.14844\n",
      "iteration 3700 loss 270.14844\n",
      "iteration 3800 loss 270.14844\n",
      "iteration 3900 loss 270.14844\n",
      "iteration 4000 loss 270.14844\n",
      "iteration 4100 loss 270.14844\n",
      "iteration 4200 loss 270.14844\n",
      "iteration 4300 loss 270.14844\n",
      "iteration 4400 loss 270.14844\n",
      "iteration 4500 loss 270.14844\n",
      "iteration 4600 loss 270.14844\n",
      "iteration 4700 loss 270.14844\n",
      "iteration 4800 loss 270.14844\n",
      "iteration 4900 loss 270.14844\n",
      "iteration 5000 loss 270.14844Train finished\n",
      "Selected features are: [15 40 48]\n",
      "Selected weights are: [0.99778026 0.3465815  0.99778026]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.001458643977591523, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 31.278541014830186\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 304.96405\n",
      "iteration 200 loss 296.76434\n",
      "iteration 300 loss 296.35553\n",
      "iteration 400 loss 296.31363\n",
      "iteration 500 loss 296.29953\n",
      "iteration 600 loss 296.29926\n",
      "iteration 700 loss 296.2992\n",
      "iteration 800 loss 296.2992\n",
      "iteration 900 loss 296.2992\n",
      "iteration 1000 loss 296.2992\n",
      "iteration 1100 loss 296.2992\n",
      "iteration 1200 loss 296.2992\n",
      "iteration 1300 loss 296.2992\n",
      "iteration 1400 loss 296.2992\n",
      "iteration 1500 loss 296.2992\n",
      "iteration 1600 loss 296.2992\n",
      "iteration 1700 loss 296.2992\n",
      "iteration 1800 loss 296.2992\n",
      "iteration 1900 loss 296.2992\n",
      "iteration 2000 loss 296.2992\n",
      "iteration 2100 loss 296.2992\n",
      "iteration 2200 loss 296.2992\n",
      "iteration 2300 loss 296.2992\n",
      "iteration 2400 loss 296.2992\n",
      "iteration 2500 loss 296.2992\n",
      "iteration 2600 loss 296.2992\n",
      "iteration 2700 loss 296.2992\n",
      "iteration 2800 loss 296.2992\n",
      "iteration 2900 loss 296.2992\n",
      "iteration 3000 loss 296.2992\n",
      "iteration 3100 loss 296.2992\n",
      "iteration 3200 loss 296.2992\n",
      "iteration 3300 loss 296.2992\n",
      "iteration 3400 loss 296.2992\n",
      "iteration 3500 loss 296.2992\n",
      "iteration 3600 loss 296.2992\n",
      "iteration 3700 loss 296.2992\n",
      "iteration 3800 loss 296.2992\n",
      "iteration 3900 loss 296.2992\n",
      "iteration 4000 loss 296.2992\n",
      "iteration 4100 loss 296.2992\n",
      "iteration 4200 loss 296.2992\n",
      "iteration 4300 loss 296.2992\n",
      "iteration 4400 loss 296.2992\n",
      "iteration 4500 loss 296.2992\n",
      "iteration 4600 loss 296.2992\n",
      "iteration 4700 loss 296.2992\n",
      "iteration 4800 loss 296.2992\n",
      "iteration 4900 loss 296.2992\n",
      "iteration 5000 loss 296.2992Train finished\n",
      "Selected features are: [14 15 48]\n",
      "Selected weights are: [0.4201622  0.99642694 0.99642694]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0010021866361589742, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 17.97213793900409\n",
      "\n",
      " 9 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 283.54065\n",
      "iteration 200 loss 281.9466\n",
      "iteration 300 loss 281.47516\n",
      "iteration 400 loss 281.47098\n",
      "iteration 500 loss 281.47076\n",
      "iteration 600 loss 281.47076\n",
      "iteration 700 loss 281.4707\n",
      "iteration 800 loss 281.4707\n",
      "iteration 900 loss 281.4707\n",
      "iteration 1000 loss 281.4707\n",
      "iteration 1100 loss 281.4707\n",
      "iteration 1200 loss 281.4707\n",
      "iteration 1300 loss 281.4707\n",
      "iteration 1400 loss 281.4707\n",
      "iteration 1500 loss 281.4707\n",
      "iteration 1600 loss 281.4707\n",
      "iteration 1700 loss 281.4707\n",
      "iteration 1800 loss 281.4707\n",
      "iteration 1900 loss 281.4707\n",
      "iteration 2000 loss 281.4707\n",
      "iteration 2100 loss 281.4707\n",
      "iteration 2200 loss 281.4707\n",
      "iteration 2300 loss 281.4707\n",
      "iteration 2400 loss 281.4707\n",
      "iteration 2500 loss 281.4707\n",
      "iteration 2600 loss 281.4707\n",
      "iteration 2700 loss 281.4707\n",
      "iteration 2800 loss 281.4707\n",
      "iteration 2900 loss 281.4707\n",
      "iteration 3000 loss 281.4707\n",
      "iteration 3100 loss 281.4707\n",
      "iteration 3200 loss 281.4707\n",
      "iteration 3300 loss 281.4707\n",
      "iteration 3400 loss 281.4707\n",
      "iteration 3500 loss 281.4707\n",
      "iteration 3600 loss 281.4707\n",
      "iteration 3700 loss 281.4707\n",
      "iteration 3800 loss 281.4707\n",
      "iteration 3900 loss 281.4707\n",
      "iteration 4000 loss 281.4707\n",
      "iteration 4100 loss 281.4707\n",
      "iteration 4200 loss 281.4707\n",
      "iteration 4300 loss 281.4707\n",
      "iteration 4400 loss 281.4707\n",
      "iteration 4500 loss 281.4707\n",
      "iteration 4600 loss 281.4707\n",
      "iteration 4700 loss 281.4707\n",
      "iteration 4800 loss 281.4707\n",
      "iteration 4900 loss 281.4707\n",
      "iteration 5000 loss 281.4707Train finished\n",
      "Selected features are: [14 15 48]\n",
      "Selected weights are: [0.50306284 0.9973036  0.9973036 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0007986787120921267, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 20.80201322155406\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 229.314\n",
      "iteration 200 loss 228.02518\n",
      "iteration 300 loss 225.02245\n",
      "iteration 400 loss 223.34789\n",
      "iteration 500 loss 222.85364\n",
      "iteration 600 loss 222.71677\n",
      "iteration 700 loss 222.66266\n",
      "iteration 800 loss 222.6177\n",
      "iteration 900 loss 222.58588\n",
      "iteration 1000 loss 222.56506\n",
      "iteration 1100 loss 222.55078\n",
      "iteration 1200 loss 222.54224\n",
      "iteration 1300 loss 222.53458\n",
      "iteration 1400 loss 222.52711\n",
      "iteration 1500 loss 222.5194\n",
      "iteration 1600 loss 222.5191\n",
      "iteration 1700 loss 222.51906\n",
      "iteration 1800 loss 222.51903\n",
      "iteration 1900 loss 222.51898\n",
      "iteration 2000 loss 222.51898\n",
      "iteration 2100 loss 222.51895\n",
      "iteration 2200 loss 222.51895\n",
      "iteration 2300 loss 222.51895\n",
      "iteration 2400 loss 222.51897\n",
      "iteration 2500 loss 222.51898\n",
      "iteration 2600 loss 222.51895\n",
      "iteration 2700 loss 222.51898\n",
      "iteration 2800 loss 222.51895\n",
      "iteration 2900 loss 222.51898\n",
      "iteration 3000 loss 222.51895\n",
      "iteration 3100 loss 222.51895\n",
      "iteration 3200 loss 222.51895\n",
      "iteration 3300 loss 222.51897\n",
      "iteration 3400 loss 222.51897\n",
      "iteration 3500 loss 222.51898\n",
      "iteration 3600 loss 222.51895\n",
      "iteration 3700 loss 222.51892\n",
      "iteration 3800 loss 222.51895\n",
      "iteration 3900 loss 222.51895\n",
      "iteration 4000 loss 222.51895\n",
      "iteration 4100 loss 222.51895\n",
      "iteration 4200 loss 222.51895\n",
      "iteration 4300 loss 222.51895\n",
      "iteration 4400 loss 222.51898\n",
      "iteration 4500 loss 222.51897\n",
      "iteration 4600 loss 222.51895\n",
      "iteration 4700 loss 222.51897\n",
      "iteration 4800 loss 222.51898\n",
      "iteration 4900 loss 222.51897\n",
      "iteration 5000 loss 222.51897Train finished\n",
      "Selected features are: [11 46 51]\n",
      "Selected weights are: [0.57384163 0.99927175 0.99927175]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.002267459357141457, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 50.96206054644046\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 256.28418\n",
      "iteration 200 loss 253.0345\n",
      "iteration 300 loss 252.7421\n",
      "iteration 400 loss 252.70981\n",
      "iteration 500 loss 252.6969\n",
      "iteration 600 loss 252.68823\n",
      "iteration 700 loss 252.6802\n",
      "iteration 800 loss 252.67834\n",
      "iteration 900 loss 252.67836\n",
      "iteration 1000 loss 252.67834\n",
      "iteration 1100 loss 252.67836\n",
      "iteration 1200 loss 252.67838\n",
      "iteration 1300 loss 252.67838\n",
      "iteration 1400 loss 252.67839\n",
      "iteration 1500 loss 252.67838\n",
      "iteration 1600 loss 252.67838\n",
      "iteration 1700 loss 252.67838\n",
      "iteration 1800 loss 252.67838\n",
      "iteration 1900 loss 252.67836\n",
      "iteration 2000 loss 252.67836\n",
      "iteration 2100 loss 252.67836\n",
      "iteration 2200 loss 252.67836\n",
      "iteration 2300 loss 252.67836\n",
      "iteration 2400 loss 252.67836\n",
      "iteration 2500 loss 252.67836\n",
      "iteration 2600 loss 252.67836\n",
      "iteration 2700 loss 252.67836\n",
      "iteration 2800 loss 252.67836\n",
      "iteration 2900 loss 252.67836\n",
      "iteration 3000 loss 252.67836\n",
      "iteration 3100 loss 252.67836\n",
      "iteration 3200 loss 252.67836\n",
      "iteration 3300 loss 252.67836\n",
      "iteration 3400 loss 252.67836\n",
      "iteration 3500 loss 252.67836\n",
      "iteration 3600 loss 252.67836\n",
      "iteration 3700 loss 252.67836\n",
      "iteration 3800 loss 252.67836\n",
      "iteration 3900 loss 252.67836\n",
      "iteration 4000 loss 252.67836\n",
      "iteration 4100 loss 252.67836\n",
      "iteration 4200 loss 252.67836\n",
      "iteration 4300 loss 252.67836\n",
      "iteration 4400 loss 252.67836\n",
      "iteration 4500 loss 252.67836\n",
      "iteration 4600 loss 252.67836\n",
      "iteration 4700 loss 252.67836\n",
      "iteration 4800 loss 252.67836\n",
      "iteration 4900 loss 252.67836\n",
      "iteration 5000 loss 252.67836Train finished\n",
      "Selected features are: [11 46 56]\n",
      "Selected weights are: [0.334626   0.93844074 0.9981893 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0028945244976866305, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 36.75989535483275\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 301.0266\n",
      "iteration 200 loss 298.89874\n",
      "iteration 300 loss 298.6165\n",
      "iteration 400 loss 298.528\n",
      "iteration 500 loss 298.52795\n",
      "iteration 600 loss 298.52795\n",
      "iteration 700 loss 298.52795\n",
      "iteration 800 loss 298.52795\n",
      "iteration 900 loss 298.52795\n",
      "iteration 1000 loss 298.52795\n",
      "iteration 1100 loss 298.52795\n",
      "iteration 1200 loss 298.52795\n",
      "iteration 1300 loss 298.52795\n",
      "iteration 1400 loss 298.52795\n",
      "iteration 1500 loss 298.52795\n",
      "iteration 1600 loss 298.52795\n",
      "iteration 1700 loss 298.52795\n",
      "iteration 1800 loss 298.52795\n",
      "iteration 1900 loss 298.52795\n",
      "iteration 2000 loss 298.52795\n",
      "iteration 2100 loss 298.52795\n",
      "iteration 2200 loss 298.52795\n",
      "iteration 2300 loss 298.52795\n",
      "iteration 2400 loss 298.52795\n",
      "iteration 2500 loss 298.52795\n",
      "iteration 2600 loss 298.52795\n",
      "iteration 2700 loss 298.52795\n",
      "iteration 2800 loss 298.52795\n",
      "iteration 2900 loss 298.52795\n",
      "iteration 3000 loss 298.52795\n",
      "iteration 3100 loss 298.52795\n",
      "iteration 3200 loss 298.52795\n",
      "iteration 3300 loss 298.52795\n",
      "iteration 3400 loss 298.52795\n",
      "iteration 3500 loss 298.52795\n",
      "iteration 3600 loss 298.52795\n",
      "iteration 3700 loss 298.52795\n",
      "iteration 3800 loss 298.52795\n",
      "iteration 3900 loss 298.52795\n",
      "iteration 4000 loss 298.52795\n",
      "iteration 4100 loss 298.52795\n",
      "iteration 4200 loss 298.52795\n",
      "iteration 4300 loss 298.52795\n",
      "iteration 4400 loss 298.52795\n",
      "iteration 4500 loss 298.52795\n",
      "iteration 4600 loss 298.52795\n",
      "iteration 4700 loss 298.52795\n",
      "iteration 4800 loss 298.52795\n",
      "iteration 4900 loss 298.52795\n",
      "iteration 5000 loss 298.52795Train finished\n",
      "Selected features are: [15 40 48]\n",
      "Selected weights are: [0.9959135  0.43444833 0.9959135 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.001419367142114569, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 16.083097500394587\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 295.54407\n",
      "iteration 200 loss 289.64392\n",
      "iteration 300 loss 289.26492\n",
      "iteration 400 loss 289.2058\n",
      "iteration 500 loss 289.18567\n",
      "iteration 600 loss 289.17456\n",
      "iteration 700 loss 289.16827\n",
      "iteration 800 loss 289.16458\n",
      "iteration 900 loss 289.1625\n",
      "iteration 1000 loss 289.16125\n",
      "iteration 1100 loss 289.16055\n",
      "iteration 1200 loss 289.16013\n",
      "iteration 1300 loss 289.15982\n",
      "iteration 1400 loss 289.1597\n",
      "iteration 1500 loss 289.15964\n",
      "iteration 1600 loss 289.15958\n",
      "iteration 1700 loss 289.15955\n",
      "iteration 1800 loss 289.15955\n",
      "iteration 1900 loss 289.1595\n",
      "iteration 2000 loss 289.15952\n",
      "iteration 2100 loss 289.15952\n",
      "iteration 2200 loss 289.15952\n",
      "iteration 2300 loss 289.15952\n",
      "iteration 2400 loss 289.1595\n",
      "iteration 2500 loss 289.1595\n",
      "iteration 2600 loss 289.1595\n",
      "iteration 2700 loss 289.15952\n",
      "iteration 2800 loss 289.15952\n",
      "iteration 2900 loss 289.1595\n",
      "iteration 3000 loss 289.1595\n",
      "iteration 3100 loss 289.1595\n",
      "iteration 3200 loss 289.1595\n",
      "iteration 3300 loss 289.1595\n",
      "iteration 3400 loss 289.1595\n",
      "iteration 3500 loss 289.1595\n",
      "iteration 3600 loss 289.1595\n",
      "iteration 3700 loss 289.1595\n",
      "iteration 3800 loss 289.1595\n",
      "iteration 3900 loss 289.1595\n",
      "iteration 4000 loss 289.15952\n",
      "iteration 4100 loss 289.15952\n",
      "iteration 4200 loss 289.15952\n",
      "iteration 4300 loss 289.15952\n",
      "iteration 4400 loss 289.1595\n",
      "iteration 4500 loss 289.1595\n",
      "iteration 4600 loss 289.1595\n",
      "iteration 4700 loss 289.15952\n",
      "iteration 4800 loss 289.1595\n",
      "iteration 4900 loss 289.15952\n",
      "iteration 5000 loss 289.15952Train finished\n",
      "Selected features are: [14 15 48]\n",
      "Selected weights are: [0.46638077 0.9966389  0.9717309 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0007792841244596705, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 20.316675899169656\n",
      "\n",
      " 10 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 279.04572\n",
      "iteration 200 loss 276.08636\n",
      "iteration 300 loss 273.6604\n",
      "iteration 400 loss 273.06418\n",
      "iteration 500 loss 272.7033\n",
      "iteration 600 loss 272.66315\n",
      "iteration 700 loss 272.6439\n",
      "iteration 800 loss 272.6316\n",
      "iteration 900 loss 272.626\n",
      "iteration 1000 loss 272.62354\n",
      "iteration 1100 loss 272.62164\n",
      "iteration 1200 loss 272.6201\n",
      "iteration 1300 loss 272.61865\n",
      "iteration 1400 loss 272.61722\n",
      "iteration 1500 loss 272.6158\n",
      "iteration 1600 loss 272.61566\n",
      "iteration 1700 loss 272.61566\n",
      "iteration 1800 loss 272.61563\n",
      "iteration 1900 loss 272.6157\n",
      "iteration 2000 loss 272.61563\n",
      "iteration 2100 loss 272.61563\n",
      "iteration 2200 loss 272.61566\n",
      "iteration 2300 loss 272.61563\n",
      "iteration 2400 loss 272.61566\n",
      "iteration 2500 loss 272.61563\n",
      "iteration 2600 loss 272.61563\n",
      "iteration 2700 loss 272.6156\n",
      "iteration 2800 loss 272.61563\n",
      "iteration 2900 loss 272.61566\n",
      "iteration 3000 loss 272.61563\n",
      "iteration 3100 loss 272.6156\n",
      "iteration 3200 loss 272.61566\n",
      "iteration 3300 loss 272.61566\n",
      "iteration 3400 loss 272.61563\n",
      "iteration 3500 loss 272.61566\n",
      "iteration 3600 loss 272.61566\n",
      "iteration 3700 loss 272.61563\n",
      "iteration 3800 loss 272.61566\n",
      "iteration 3900 loss 272.61563\n",
      "iteration 4000 loss 272.6156\n",
      "iteration 4100 loss 272.61563\n",
      "iteration 4200 loss 272.6156\n",
      "iteration 4300 loss 272.6156\n",
      "iteration 4400 loss 272.61563\n",
      "iteration 4500 loss 272.6156\n",
      "iteration 4600 loss 272.61563\n",
      "iteration 4700 loss 272.61563\n",
      "iteration 4800 loss 272.6156\n",
      "iteration 4900 loss 272.6156\n",
      "iteration 5000 loss 272.6156Train finished\n",
      "Selected features are: [11 46 51]\n",
      "Selected weights are: [0.4324295 0.9990962 0.9990962]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0022933131110663904, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 31.515822880409466\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 280.03613\n",
      "iteration 200 loss 278.11176\n",
      "iteration 300 loss 277.62903\n",
      "iteration 400 loss 277.56134\n",
      "iteration 500 loss 277.54514\n",
      "iteration 600 loss 277.53613\n",
      "iteration 700 loss 277.53415\n",
      "iteration 800 loss 277.5343\n",
      "iteration 900 loss 277.5343\n",
      "iteration 1000 loss 277.5343\n",
      "iteration 1100 loss 277.5343\n",
      "iteration 1200 loss 277.5343\n",
      "iteration 1300 loss 277.5343\n",
      "iteration 1400 loss 277.5343\n",
      "iteration 1500 loss 277.5343\n",
      "iteration 1600 loss 277.5343\n",
      "iteration 1700 loss 277.5343\n",
      "iteration 1800 loss 277.5343\n",
      "iteration 1900 loss 277.5343\n",
      "iteration 2000 loss 277.5343\n",
      "iteration 2100 loss 277.5343\n",
      "iteration 2200 loss 277.5343\n",
      "iteration 2300 loss 277.5343\n",
      "iteration 2400 loss 277.5343\n",
      "iteration 2500 loss 277.5343\n",
      "iteration 2600 loss 277.5343\n",
      "iteration 2700 loss 277.5343\n",
      "iteration 2800 loss 277.5343\n",
      "iteration 2900 loss 277.5343\n",
      "iteration 3000 loss 277.5343\n",
      "iteration 3100 loss 277.5343\n",
      "iteration 3200 loss 277.5343\n",
      "iteration 3300 loss 277.5343\n",
      "iteration 3400 loss 277.5343\n",
      "iteration 3500 loss 277.5343\n",
      "iteration 3600 loss 277.5343\n",
      "iteration 3700 loss 277.5343\n",
      "iteration 3800 loss 277.5343\n",
      "iteration 3900 loss 277.5343\n",
      "iteration 4000 loss 277.5343\n",
      "iteration 4100 loss 277.5343\n",
      "iteration 4200 loss 277.5343\n",
      "iteration 4300 loss 277.5343\n",
      "iteration 4400 loss 277.5343\n",
      "iteration 4500 loss 277.5343\n",
      "iteration 4600 loss 277.5343\n",
      "iteration 4700 loss 277.5343\n",
      "iteration 4800 loss 277.5343\n",
      "iteration 4900 loss 277.5343\n",
      "iteration 5000 loss 277.5343Train finished\n",
      "Selected features are: [14 15 48]\n",
      "Selected weights are: [0.42539677 0.99696386 0.99696386]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0007682263200436374, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 22.78388731554958\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 195.11453\n",
      "iteration 200 loss 192.83572\n",
      "iteration 300 loss 192.62956\n",
      "iteration 400 loss 192.5753\n",
      "iteration 500 loss 192.5391\n",
      "iteration 600 loss 192.50722\n",
      "iteration 700 loss 192.47919\n",
      "iteration 800 loss 192.45459\n",
      "iteration 900 loss 192.43289\n",
      "iteration 1000 loss 192.41379\n",
      "iteration 1100 loss 192.39691\n",
      "iteration 1200 loss 192.3819\n",
      "iteration 1300 loss 192.36852\n",
      "iteration 1400 loss 192.35657\n",
      "iteration 1500 loss 192.3458\n",
      "iteration 1600 loss 192.33607\n",
      "iteration 1700 loss 192.32953\n",
      "iteration 1800 loss 192.32639\n",
      "iteration 1900 loss 192.32388\n",
      "iteration 2000 loss 192.3219\n",
      "iteration 2100 loss 192.32028\n",
      "iteration 2200 loss 192.31894\n",
      "iteration 2300 loss 192.31778\n",
      "iteration 2400 loss 192.31723\n",
      "iteration 2500 loss 192.31723\n",
      "iteration 2600 loss 192.3172\n",
      "iteration 2700 loss 192.31723\n",
      "iteration 2800 loss 192.31718\n",
      "iteration 2900 loss 192.3172\n",
      "iteration 3000 loss 192.3172\n",
      "iteration 3100 loss 192.3172\n",
      "iteration 3200 loss 192.3172\n",
      "iteration 3300 loss 192.31721\n",
      "iteration 3400 loss 192.31723\n",
      "iteration 3500 loss 192.31718\n",
      "iteration 3600 loss 192.3172\n",
      "iteration 3700 loss 192.3172\n",
      "iteration 3800 loss 192.31723\n",
      "iteration 3900 loss 192.3172\n",
      "iteration 4000 loss 192.31723\n",
      "iteration 4100 loss 192.3172\n",
      "iteration 4200 loss 192.3172\n",
      "iteration 4300 loss 192.3172\n",
      "iteration 4400 loss 192.3172\n",
      "iteration 4500 loss 192.3172\n",
      "iteration 4600 loss 192.31721\n",
      "iteration 4700 loss 192.3172\n",
      "iteration 4800 loss 192.3172\n",
      "iteration 4900 loss 192.3172\n",
      "iteration 5000 loss 192.3172Train finished\n",
      "Selected features are: [15 40 51]\n",
      "Selected weights are: [0.9976446 0.504708  0.9976446]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.002055427507818776, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 60.38781563603584\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 306.55353\n",
      "iteration 200 loss 305.2569\n",
      "iteration 300 loss 304.93607\n",
      "iteration 400 loss 304.91556\n",
      "iteration 500 loss 304.9157\n",
      "iteration 600 loss 304.9157\n",
      "iteration 700 loss 304.9157\n",
      "iteration 800 loss 304.91577\n",
      "iteration 900 loss 304.91574\n",
      "iteration 1000 loss 304.91574\n",
      "iteration 1100 loss 304.91577\n",
      "iteration 1200 loss 304.91574\n",
      "iteration 1300 loss 304.91574\n",
      "iteration 1400 loss 304.91577\n",
      "iteration 1500 loss 304.91574\n",
      "iteration 1600 loss 304.91574\n",
      "iteration 1700 loss 304.91574\n",
      "iteration 1800 loss 304.91577\n",
      "iteration 1900 loss 304.91574\n",
      "iteration 2000 loss 304.91574\n",
      "iteration 2100 loss 304.91577\n",
      "iteration 2200 loss 304.91574\n",
      "iteration 2300 loss 304.91574\n",
      "iteration 2400 loss 304.91574\n",
      "iteration 2500 loss 304.91577\n",
      "iteration 2600 loss 304.91574\n",
      "iteration 2700 loss 304.91574\n",
      "iteration 2800 loss 304.91577\n",
      "iteration 2900 loss 304.91574\n",
      "iteration 3000 loss 304.91574\n",
      "iteration 3100 loss 304.91574\n",
      "iteration 3200 loss 304.91577\n",
      "iteration 3300 loss 304.91574\n",
      "iteration 3400 loss 304.91574\n",
      "iteration 3500 loss 304.91577\n",
      "iteration 3600 loss 304.91574\n",
      "iteration 3700 loss 304.91574\n",
      "iteration 3800 loss 304.91574\n",
      "iteration 3900 loss 304.91577\n",
      "iteration 4000 loss 304.91574\n",
      "iteration 4100 loss 304.91574\n",
      "iteration 4200 loss 304.91577\n",
      "iteration 4300 loss 304.91574\n",
      "iteration 4400 loss 304.91574\n",
      "iteration 4500 loss 304.91574\n",
      "iteration 4600 loss 304.91577\n",
      "iteration 4700 loss 304.91574\n",
      "iteration 4800 loss 304.91574\n",
      "iteration 4900 loss 304.91577\n",
      "iteration 5000 loss 304.91574Train finished\n",
      "Selected features are: [14 15 48]\n",
      "Selected weights are: [0.41501635 0.9956426  0.9956426 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00101920985216718, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 10.884689254021666\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 303.74664\n",
      "iteration 200 loss 296.75323\n",
      "iteration 300 loss 296.4248\n",
      "iteration 400 loss 296.4051\n",
      "iteration 500 loss 296.40143\n",
      "iteration 600 loss 296.39954\n",
      "iteration 700 loss 296.3986\n",
      "iteration 800 loss 296.39807\n",
      "iteration 900 loss 296.39783\n",
      "iteration 1000 loss 296.39764\n",
      "iteration 1100 loss 296.39758\n",
      "iteration 1200 loss 296.39752\n",
      "iteration 1300 loss 296.39752\n",
      "iteration 1400 loss 296.3975\n",
      "iteration 1500 loss 296.39755\n",
      "iteration 1600 loss 296.39752\n",
      "iteration 1700 loss 296.39752\n",
      "iteration 1800 loss 296.39752\n",
      "iteration 1900 loss 296.39752\n",
      "iteration 2000 loss 296.3975\n",
      "iteration 2100 loss 296.39752\n",
      "iteration 2200 loss 296.3975\n",
      "iteration 2300 loss 296.3975\n",
      "iteration 2400 loss 296.3975\n",
      "iteration 2500 loss 296.3975\n",
      "iteration 2600 loss 296.39752\n",
      "iteration 2700 loss 296.39752\n",
      "iteration 2800 loss 296.39752\n",
      "iteration 2900 loss 296.3975\n",
      "iteration 3000 loss 296.3975\n",
      "iteration 3100 loss 296.3975\n",
      "iteration 3200 loss 296.3975\n",
      "iteration 3300 loss 296.3975\n",
      "iteration 3400 loss 296.3975\n",
      "iteration 3500 loss 296.3975\n",
      "iteration 3600 loss 296.3975\n",
      "iteration 3700 loss 296.3975\n",
      "iteration 3800 loss 296.3975\n",
      "iteration 3900 loss 296.3975\n",
      "iteration 4000 loss 296.3975\n",
      "iteration 4100 loss 296.3975\n",
      "iteration 4200 loss 296.3975\n",
      "iteration 4300 loss 296.3975\n",
      "iteration 4400 loss 296.3975\n",
      "iteration 4500 loss 296.3975\n",
      "iteration 4600 loss 296.3975\n",
      "iteration 4700 loss 296.3975\n",
      "iteration 4800 loss 296.3975\n",
      "iteration 4900 loss 296.3975\n",
      "iteration 5000 loss 296.3975Train finished\n",
      "Selected features are: [15 40 48]\n",
      "Selected weights are: [0.9956125  0.7222763  0.57566214]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.001542564351660418, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 18.785971128002643\n",
      "28.901919121083555 \n",
      "\n",
      "\n",
      " 1 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 229.07208\n",
      "iteration 200 loss 226.49289\n",
      "iteration 300 loss 225.12448\n",
      "iteration 400 loss 224.58752\n",
      "iteration 500 loss 224.4517\n",
      "iteration 600 loss 224.35818\n",
      "iteration 700 loss 224.30351\n",
      "iteration 800 loss 224.25822\n",
      "iteration 900 loss 224.22311\n",
      "iteration 1000 loss 224.19205\n",
      "iteration 1100 loss 224.16434\n",
      "iteration 1200 loss 224.13956\n",
      "iteration 1300 loss 224.11739\n",
      "iteration 1400 loss 224.09763\n",
      "iteration 1500 loss 224.07997\n",
      "iteration 1600 loss 224.07285\n",
      "iteration 1700 loss 224.07217\n",
      "iteration 1800 loss 224.07224\n",
      "iteration 1900 loss 224.07224\n",
      "iteration 2000 loss 224.07225\n",
      "iteration 2100 loss 224.07227\n",
      "iteration 2200 loss 224.07225\n",
      "iteration 2300 loss 224.07225\n",
      "iteration 2400 loss 224.07224\n",
      "iteration 2500 loss 224.07224\n",
      "iteration 2600 loss 224.07224\n",
      "iteration 2700 loss 224.07224\n",
      "iteration 2800 loss 224.07224\n",
      "iteration 2900 loss 224.07224\n",
      "iteration 3000 loss 224.07224\n",
      "iteration 3100 loss 224.07224\n",
      "iteration 3200 loss 224.07224\n",
      "iteration 3300 loss 224.07224\n",
      "iteration 3400 loss 224.07224\n",
      "iteration 3500 loss 224.07224\n",
      "iteration 3600 loss 224.07224\n",
      "iteration 3700 loss 224.07224\n",
      "iteration 3800 loss 224.07224\n",
      "iteration 3900 loss 224.07224\n",
      "iteration 4000 loss 224.07224\n",
      "iteration 4100 loss 224.07224\n",
      "iteration 4200 loss 224.07224\n",
      "iteration 4300 loss 224.07224\n",
      "iteration 4400 loss 224.07224\n",
      "iteration 4500 loss 224.07224\n",
      "iteration 4600 loss 224.07224\n",
      "iteration 4700 loss 224.07224\n",
      "iteration 4800 loss 224.07224\n",
      "iteration 4900 loss 224.07224\n",
      "iteration 5000 loss 224.07224Train finished\n",
      "Selected features are: [14 15 48 51 63]\n",
      "Selected weights are: [0.42287025 0.99923086 0.99923086 0.99923086 0.99923086]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0009444935030183346, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 41.2321831419599\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 323.13428\n",
      "iteration 200 loss 314.01282\n",
      "iteration 300 loss 312.8941\n",
      "iteration 400 loss 312.6872\n",
      "iteration 500 loss 312.5644\n",
      "iteration 600 loss 312.51587\n",
      "iteration 700 loss 312.50134\n",
      "iteration 800 loss 312.4889\n",
      "iteration 900 loss 312.4773\n",
      "iteration 1000 loss 312.46652\n",
      "iteration 1100 loss 312.4564\n",
      "iteration 1200 loss 312.4469\n",
      "iteration 1300 loss 312.4381\n",
      "iteration 1400 loss 312.42987\n",
      "iteration 1500 loss 312.42212\n",
      "iteration 1600 loss 312.4149\n",
      "iteration 1700 loss 312.41342\n",
      "iteration 1800 loss 312.41315\n",
      "iteration 1900 loss 312.41284\n",
      "iteration 2000 loss 312.4126\n",
      "iteration 2100 loss 312.41238\n",
      "iteration 2200 loss 312.41223\n",
      "iteration 2300 loss 312.4121\n",
      "iteration 2400 loss 312.41193\n",
      "iteration 2500 loss 312.41187\n",
      "iteration 2600 loss 312.41174\n",
      "iteration 2700 loss 312.41162\n",
      "iteration 2800 loss 312.41156\n",
      "iteration 2900 loss 312.4115\n",
      "iteration 3000 loss 312.41144\n",
      "iteration 3100 loss 312.4114\n",
      "iteration 3200 loss 312.41138\n",
      "iteration 3300 loss 312.41138\n",
      "iteration 3400 loss 312.41132\n",
      "iteration 3500 loss 312.41125\n",
      "iteration 3600 loss 312.41125\n",
      "iteration 3700 loss 312.41125\n",
      "iteration 3800 loss 312.4112\n",
      "iteration 3900 loss 312.41122\n",
      "iteration 4000 loss 312.41116\n",
      "iteration 4100 loss 312.4112\n",
      "iteration 4200 loss 312.41116\n",
      "iteration 4300 loss 312.41116\n",
      "iteration 4400 loss 312.41113\n",
      "iteration 4500 loss 312.41116\n",
      "iteration 4600 loss 312.41113\n",
      "iteration 4700 loss 312.41113\n",
      "iteration 4800 loss 312.41113\n",
      "iteration 4900 loss 312.41116\n",
      "iteration 5000 loss 312.41116Train finished\n",
      "Selected features are: [14 15 40 48 69]\n",
      "Selected weights are: [0.5155694  0.9984664  0.9984664  0.9984664  0.69660974]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0006566616998682878, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 9.67379024508724\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 254.90868\n",
      "iteration 200 loss 251.25568\n",
      "iteration 300 loss 250.52873\n",
      "iteration 400 loss 250.49107\n",
      "iteration 500 loss 250.46153\n",
      "iteration 600 loss 250.43513\n",
      "iteration 700 loss 250.41464\n",
      "iteration 800 loss 250.39694\n",
      "iteration 900 loss 250.38356\n",
      "iteration 1000 loss 250.37143\n",
      "iteration 1100 loss 250.3603\n",
      "iteration 1200 loss 250.34999\n",
      "iteration 1300 loss 250.34042\n",
      "iteration 1400 loss 250.33147\n",
      "iteration 1500 loss 250.32312\n",
      "iteration 1600 loss 250.31525\n",
      "iteration 1700 loss 250.30789\n",
      "iteration 1800 loss 250.30093\n",
      "iteration 1900 loss 250.2944\n",
      "iteration 2000 loss 250.28827\n",
      "iteration 2100 loss 250.28244\n",
      "iteration 2200 loss 250.27701\n",
      "iteration 2300 loss 250.27184\n",
      "iteration 2400 loss 250.26697\n",
      "iteration 2500 loss 250.26239\n",
      "iteration 2600 loss 250.25797\n",
      "iteration 2700 loss 250.25385\n",
      "iteration 2800 loss 250.24994\n",
      "iteration 2900 loss 250.24773\n",
      "iteration 3000 loss 250.24747\n",
      "iteration 3100 loss 250.24725\n",
      "iteration 3200 loss 250.24716\n",
      "iteration 3300 loss 250.24704\n",
      "iteration 3400 loss 250.24695\n",
      "iteration 3500 loss 250.24692\n",
      "iteration 3600 loss 250.24686\n",
      "iteration 3700 loss 250.24686\n",
      "iteration 3800 loss 250.2468\n",
      "iteration 3900 loss 250.24677\n",
      "iteration 4000 loss 250.2468\n",
      "iteration 4100 loss 250.24673\n",
      "iteration 4200 loss 250.24677\n",
      "iteration 4300 loss 250.24677\n",
      "iteration 4400 loss 250.2467\n",
      "iteration 4500 loss 250.24673\n",
      "iteration 4600 loss 250.24672\n",
      "iteration 4700 loss 250.2467\n",
      "iteration 4800 loss 250.24667\n",
      "iteration 4900 loss 250.2467\n",
      "iteration 5000 loss 250.2467Train finished\n",
      "Selected features are: [ 8 10 14 15 48]\n",
      "Selected weights are: [0.99912375 0.99912375 0.658643   0.99912375 0.99912375]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0004565363820823901, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 32.49369029215284\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 240.2272\n",
      "iteration 200 loss 236.37259\n",
      "iteration 300 loss 231.72177\n",
      "iteration 400 loss 229.58395\n",
      "iteration 500 loss 229.10254\n",
      "iteration 600 loss 228.8548\n",
      "iteration 700 loss 228.69952\n",
      "iteration 800 loss 228.65608\n",
      "iteration 900 loss 228.63762\n",
      "iteration 1000 loss 228.62505\n",
      "iteration 1100 loss 228.6243\n",
      "iteration 1200 loss 228.62427\n",
      "iteration 1300 loss 228.62422\n",
      "iteration 1400 loss 228.6242\n",
      "iteration 1500 loss 228.62418\n",
      "iteration 1600 loss 228.62418\n",
      "iteration 1700 loss 228.62418\n",
      "iteration 1800 loss 228.62413\n",
      "iteration 1900 loss 228.62416\n",
      "iteration 2000 loss 228.62413\n",
      "iteration 2100 loss 228.62411\n",
      "iteration 2200 loss 228.6241\n",
      "iteration 2300 loss 228.62411\n",
      "iteration 2400 loss 228.62413\n",
      "iteration 2500 loss 228.62411\n",
      "iteration 2600 loss 228.62411\n",
      "iteration 2700 loss 228.6241\n",
      "iteration 2800 loss 228.62408\n",
      "iteration 2900 loss 228.62408\n",
      "iteration 3000 loss 228.62408\n",
      "iteration 3100 loss 228.62408\n",
      "iteration 3200 loss 228.6241\n",
      "iteration 3300 loss 228.62411\n",
      "iteration 3400 loss 228.62413\n",
      "iteration 3500 loss 228.62411\n",
      "iteration 3600 loss 228.62413\n",
      "iteration 3700 loss 228.62411\n",
      "iteration 3800 loss 228.62408\n",
      "iteration 3900 loss 228.62408\n",
      "iteration 4000 loss 228.62413\n",
      "iteration 4100 loss 228.62411\n",
      "iteration 4200 loss 228.62411\n",
      "iteration 4300 loss 228.62411\n",
      "iteration 4400 loss 228.62413\n",
      "iteration 4500 loss 228.62415\n",
      "iteration 4600 loss 228.62413\n",
      "iteration 4700 loss 228.62411\n",
      "iteration 4800 loss 228.62411\n",
      "iteration 4900 loss 228.62413\n",
      "iteration 5000 loss 228.62415Train finished\n",
      "Selected features are: [15 40 48 54 58]\n",
      "Selected weights are: [0.9364859  0.99811774 0.99811774 0.51057166 0.99811774]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0011905835442828831, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 48.22404999516556\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 316.47717\n",
      "iteration 200 loss 313.4629\n",
      "iteration 300 loss 312.15198\n",
      "iteration 400 loss 311.25836\n",
      "iteration 500 loss 311.09036\n",
      "iteration 600 loss 310.96384\n",
      "iteration 700 loss 310.8654\n",
      "iteration 800 loss 310.78815\n",
      "iteration 900 loss 310.72375\n",
      "iteration 1000 loss 310.66928\n",
      "iteration 1100 loss 310.625\n",
      "iteration 1200 loss 310.59625\n",
      "iteration 1300 loss 310.573\n",
      "iteration 1400 loss 310.55304\n",
      "iteration 1500 loss 310.53555\n",
      "iteration 1600 loss 310.51974\n",
      "iteration 1700 loss 310.50525\n",
      "iteration 1800 loss 310.49207\n",
      "iteration 1900 loss 310.48013\n",
      "iteration 2000 loss 310.46902\n",
      "iteration 2100 loss 310.45856\n",
      "iteration 2200 loss 310.44852\n",
      "iteration 2300 loss 310.43887\n",
      "iteration 2400 loss 310.42947\n",
      "iteration 2500 loss 310.42026\n",
      "iteration 2600 loss 310.41125\n",
      "iteration 2700 loss 310.40228\n",
      "iteration 2800 loss 310.39337\n",
      "iteration 2900 loss 310.38446\n",
      "iteration 3000 loss 310.37555\n",
      "iteration 3100 loss 310.36786\n",
      "iteration 3200 loss 310.3658\n",
      "iteration 3300 loss 310.36426\n",
      "iteration 3400 loss 310.3633\n",
      "iteration 3500 loss 310.36273\n",
      "iteration 3600 loss 310.36227\n",
      "iteration 3700 loss 310.36194\n",
      "iteration 3800 loss 310.36166\n",
      "iteration 3900 loss 310.36142\n",
      "iteration 4000 loss 310.36127\n",
      "iteration 4100 loss 310.3611\n",
      "iteration 4200 loss 310.36096\n",
      "iteration 4300 loss 310.36087\n",
      "iteration 4400 loss 310.36078\n",
      "iteration 4500 loss 310.36072\n",
      "iteration 4600 loss 310.36066\n",
      "iteration 4700 loss 310.3606\n",
      "iteration 4800 loss 310.36053\n",
      "iteration 4900 loss 310.36047\n",
      "iteration 5000 loss 310.3604Train finished\n",
      "Selected features are: [49 52 56 62 70]\n",
      "Selected weights are: [0.9981809  0.52751136 0.9981809  0.9981809  0.8351105 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.02688813048083029, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 12.222686360154844\n",
      "\n",
      " 2 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 258.20822\n",
      "iteration 200 loss 257.79843\n",
      "iteration 300 loss 252.83221\n",
      "iteration 400 loss 250.73648\n",
      "iteration 500 loss 249.82327\n",
      "iteration 600 loss 249.0774\n",
      "iteration 700 loss 248.80138\n",
      "iteration 800 loss 248.69989\n",
      "iteration 900 loss 248.6792\n",
      "iteration 1000 loss 248.66188\n",
      "iteration 1100 loss 248.64664\n",
      "iteration 1200 loss 248.633\n",
      "iteration 1300 loss 248.62062\n",
      "iteration 1400 loss 248.60925\n",
      "iteration 1500 loss 248.59879\n",
      "iteration 1600 loss 248.58896\n",
      "iteration 1700 loss 248.57977\n",
      "iteration 1800 loss 248.57132\n",
      "iteration 1900 loss 248.56346\n",
      "iteration 2000 loss 248.55588\n",
      "iteration 2100 loss 248.5485\n",
      "iteration 2200 loss 248.54138\n",
      "iteration 2300 loss 248.53441\n",
      "iteration 2400 loss 248.52765\n",
      "iteration 2500 loss 248.52103\n",
      "iteration 2600 loss 248.51459\n",
      "iteration 2700 loss 248.50928\n",
      "iteration 2800 loss 248.50618\n",
      "iteration 2900 loss 248.50323\n",
      "iteration 3000 loss 248.50037\n",
      "iteration 3100 loss 248.49762\n",
      "iteration 3200 loss 248.495\n",
      "iteration 3300 loss 248.49248\n",
      "iteration 3400 loss 248.49004\n",
      "iteration 3500 loss 248.48773\n",
      "iteration 3600 loss 248.48619\n",
      "iteration 3700 loss 248.48517\n",
      "iteration 3800 loss 248.48427\n",
      "iteration 3900 loss 248.48337\n",
      "iteration 4000 loss 248.48251\n",
      "iteration 4100 loss 248.48172\n",
      "iteration 4200 loss 248.48094\n",
      "iteration 4300 loss 248.48018\n",
      "iteration 4400 loss 248.47943\n",
      "iteration 4500 loss 248.47874\n",
      "iteration 4600 loss 248.47841\n",
      "iteration 4700 loss 248.47833\n",
      "iteration 4800 loss 248.47826\n",
      "iteration 4900 loss 248.4782\n",
      "iteration 5000 loss 248.47818Train finished\n",
      "Selected features are: [14 15 40 48 58]\n",
      "Selected weights are: [0.44704965 0.9988968  0.89289045 0.99361867 0.96708095]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0006282170875466703, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 40.533267696046195\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 293.32898\n",
      "iteration 200 loss 287.9901\n",
      "iteration 300 loss 285.776\n",
      "iteration 400 loss 285.52005\n",
      "iteration 500 loss 285.42725\n",
      "iteration 600 loss 285.41083\n",
      "iteration 700 loss 285.40576\n",
      "iteration 800 loss 285.40192\n",
      "iteration 900 loss 285.39886\n",
      "iteration 1000 loss 285.3964\n",
      "iteration 1100 loss 285.39447\n",
      "iteration 1200 loss 285.39288\n",
      "iteration 1300 loss 285.39157\n",
      "iteration 1400 loss 285.3905\n",
      "iteration 1500 loss 285.38965\n",
      "iteration 1600 loss 285.38898\n",
      "iteration 1700 loss 285.38837\n",
      "iteration 1800 loss 285.38788\n",
      "iteration 1900 loss 285.38785\n",
      "iteration 2000 loss 285.38782\n",
      "iteration 2100 loss 285.38782\n",
      "iteration 2200 loss 285.38782\n",
      "iteration 2300 loss 285.38782\n",
      "iteration 2400 loss 285.38785\n",
      "iteration 2500 loss 285.38782\n",
      "iteration 2600 loss 285.38782\n",
      "iteration 2700 loss 285.38785\n",
      "iteration 2800 loss 285.38785\n",
      "iteration 2900 loss 285.38788\n",
      "iteration 3000 loss 285.38782\n",
      "iteration 3100 loss 285.38785\n",
      "iteration 3200 loss 285.38788\n",
      "iteration 3300 loss 285.38785\n",
      "iteration 3400 loss 285.38785\n",
      "iteration 3500 loss 285.38788\n",
      "iteration 3600 loss 285.38785\n",
      "iteration 3700 loss 285.38788\n",
      "iteration 3800 loss 285.38788\n",
      "iteration 3900 loss 285.38788\n",
      "iteration 4000 loss 285.38788\n",
      "iteration 4100 loss 285.38788\n",
      "iteration 4200 loss 285.38788\n",
      "iteration 4300 loss 285.38788\n",
      "iteration 4400 loss 285.38788\n",
      "iteration 4500 loss 285.38788\n",
      "iteration 4600 loss 285.38788\n",
      "iteration 4700 loss 285.38788\n",
      "iteration 4800 loss 285.38788\n",
      "iteration 4900 loss 285.38788\n",
      "iteration 5000 loss 285.38788Train finished\n",
      "Selected features are: [15 40 48 51 58]\n",
      "Selected weights are: [0.99850845 0.71368635 0.99850845 0.99850845 0.42025754]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0012122727395009363, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 17.599559642284998\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 270.84442\n",
      "iteration 200 loss 262.5072\n",
      "iteration 300 loss 261.17188\n",
      "iteration 400 loss 260.69687\n",
      "iteration 500 loss 260.4864\n",
      "iteration 600 loss 260.44788\n",
      "iteration 700 loss 260.43777\n",
      "iteration 800 loss 260.43512\n",
      "iteration 900 loss 260.4345\n",
      "iteration 1000 loss 260.4341\n",
      "iteration 1100 loss 260.43372\n",
      "iteration 1200 loss 260.43353\n",
      "iteration 1300 loss 260.4333\n",
      "iteration 1400 loss 260.43323\n",
      "iteration 1500 loss 260.4331\n",
      "iteration 1600 loss 260.433\n",
      "iteration 1700 loss 260.43298\n",
      "iteration 1800 loss 260.43292\n",
      "iteration 1900 loss 260.43292\n",
      "iteration 2000 loss 260.4329\n",
      "iteration 2100 loss 260.43286\n",
      "iteration 2200 loss 260.43286\n",
      "iteration 2300 loss 260.43283\n",
      "iteration 2400 loss 260.43286\n",
      "iteration 2500 loss 260.43286\n",
      "iteration 2600 loss 260.43286\n",
      "iteration 2700 loss 260.43286\n",
      "iteration 2800 loss 260.43283\n",
      "iteration 2900 loss 260.4328\n",
      "iteration 3000 loss 260.4328\n",
      "iteration 3100 loss 260.43283\n",
      "iteration 3200 loss 260.4328\n",
      "iteration 3300 loss 260.43283\n",
      "iteration 3400 loss 260.4328\n",
      "iteration 3500 loss 260.4328\n",
      "iteration 3600 loss 260.4328\n",
      "iteration 3700 loss 260.4328\n",
      "iteration 3800 loss 260.4328\n",
      "iteration 3900 loss 260.4328\n",
      "iteration 4000 loss 260.4328\n",
      "iteration 4100 loss 260.4328\n",
      "iteration 4200 loss 260.4328\n",
      "iteration 4300 loss 260.43277\n",
      "iteration 4400 loss 260.4328\n",
      "iteration 4500 loss 260.4328\n",
      "iteration 4600 loss 260.4328\n",
      "iteration 4700 loss 260.4328\n",
      "iteration 4800 loss 260.4328\n",
      "iteration 4900 loss 260.4328\n",
      "iteration 5000 loss 260.4328Train finished\n",
      "Selected features are: [14 15 40 48 58]\n",
      "Selected weights are: [0.529905  0.9985986 0.9985986 0.9985986 0.9985986]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0006885179249863563, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 30.01714320757633\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 274.3535\n",
      "iteration 200 loss 269.73322\n",
      "iteration 300 loss 269.35837\n",
      "iteration 400 loss 269.23044\n",
      "iteration 500 loss 269.16064\n",
      "iteration 600 loss 269.10278\n",
      "iteration 700 loss 269.06894\n",
      "iteration 800 loss 269.0406\n",
      "iteration 900 loss 269.01648\n",
      "iteration 1000 loss 268.99612\n",
      "iteration 1100 loss 268.98492\n",
      "iteration 1200 loss 268.97577\n",
      "iteration 1300 loss 268.968\n",
      "iteration 1400 loss 268.96133\n",
      "iteration 1500 loss 268.95563\n",
      "iteration 1600 loss 268.95074\n",
      "iteration 1700 loss 268.9465\n",
      "iteration 1800 loss 268.94287\n",
      "iteration 1900 loss 268.93973\n",
      "iteration 2000 loss 268.937\n",
      "iteration 2100 loss 268.9347\n",
      "iteration 2200 loss 268.93265\n",
      "iteration 2300 loss 268.9309\n",
      "iteration 2400 loss 268.92935\n",
      "iteration 2500 loss 268.928\n",
      "iteration 2600 loss 268.92706\n",
      "iteration 2700 loss 268.92682\n",
      "iteration 2800 loss 268.9266\n",
      "iteration 2900 loss 268.92648\n",
      "iteration 3000 loss 268.92636\n",
      "iteration 3100 loss 268.92627\n",
      "iteration 3200 loss 268.92618\n",
      "iteration 3300 loss 268.92612\n",
      "iteration 3400 loss 268.9261\n",
      "iteration 3500 loss 268.92606\n",
      "iteration 3600 loss 268.926\n",
      "iteration 3700 loss 268.92596\n",
      "iteration 3800 loss 268.92593\n",
      "iteration 3900 loss 268.9259\n",
      "iteration 4000 loss 268.9259\n",
      "iteration 4100 loss 268.92584\n",
      "iteration 4200 loss 268.92584\n",
      "iteration 4300 loss 268.92584\n",
      "iteration 4400 loss 268.92584\n",
      "iteration 4500 loss 268.92584\n",
      "iteration 4600 loss 268.92584\n",
      "iteration 4700 loss 268.9258\n",
      "iteration 4800 loss 268.92578\n",
      "iteration 4900 loss 268.92578\n",
      "iteration 5000 loss 268.92575Train finished\n",
      "Selected features are: [14 15 40 48 58]\n",
      "Selected weights are: [0.51261806 0.99875003 0.99875003 0.99875003 0.99875003]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0005497357998928753, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 23.433120636528116\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 266.22653\n",
      "iteration 200 loss 263.83197\n",
      "iteration 300 loss 263.19012\n",
      "iteration 400 loss 262.77432\n",
      "iteration 500 loss 262.60675\n",
      "iteration 600 loss 262.4722\n",
      "iteration 700 loss 262.35736\n",
      "iteration 800 loss 262.22476\n",
      "iteration 900 loss 262.11124\n",
      "iteration 1000 loss 262.1018\n",
      "iteration 1100 loss 262.09485\n",
      "iteration 1200 loss 262.08762\n",
      "iteration 1300 loss 262.07993\n",
      "iteration 1400 loss 262.0716\n",
      "iteration 1500 loss 262.06244\n",
      "iteration 1600 loss 262.05237\n",
      "iteration 1700 loss 262.04114\n",
      "iteration 1800 loss 262.0285\n",
      "iteration 1900 loss 262.01425\n",
      "iteration 2000 loss 261.99805\n",
      "iteration 2100 loss 261.9794\n",
      "iteration 2200 loss 261.96738\n",
      "iteration 2300 loss 261.95795\n",
      "iteration 2400 loss 261.94858\n",
      "iteration 2500 loss 261.93896\n",
      "iteration 2600 loss 261.92908\n",
      "iteration 2700 loss 261.91885\n",
      "iteration 2800 loss 261.90823\n",
      "iteration 2900 loss 261.90002\n",
      "iteration 3000 loss 261.89493\n",
      "iteration 3100 loss 261.8897\n",
      "iteration 3200 loss 261.88428\n",
      "iteration 3300 loss 261.87857\n",
      "iteration 3400 loss 261.87256\n",
      "iteration 3500 loss 261.86627\n",
      "iteration 3600 loss 261.85962\n",
      "iteration 3700 loss 261.8526\n",
      "iteration 3800 loss 261.84512\n",
      "iteration 3900 loss 261.83725\n",
      "iteration 4000 loss 261.82886\n",
      "iteration 4100 loss 261.8207\n",
      "iteration 4200 loss 261.812\n",
      "iteration 4300 loss 261.80487\n",
      "iteration 4400 loss 261.8032\n",
      "iteration 4500 loss 261.80206\n",
      "iteration 4600 loss 261.80103\n",
      "iteration 4700 loss 261.79993\n",
      "iteration 4800 loss 261.79883\n",
      "iteration 4900 loss 261.7977\n",
      "iteration 5000 loss 261.7965Train finished\n",
      "Selected features are: [26 33 49 56 62]\n",
      "Selected weights are: [0.6188384 0.971617  0.9991923 0.9991923 0.9853284]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.02529223090325892, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 32.297910463967995\n",
      "\n",
      " 3 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 292.19525\n",
      "iteration 200 loss 284.99634\n",
      "iteration 300 loss 284.64795\n",
      "iteration 400 loss 284.5341\n",
      "iteration 500 loss 284.4909\n",
      "iteration 600 loss 284.45636\n",
      "iteration 700 loss 284.42468\n",
      "iteration 800 loss 284.39557\n",
      "iteration 900 loss 284.3689\n",
      "iteration 1000 loss 284.35217\n",
      "iteration 1100 loss 284.33826\n",
      "iteration 1200 loss 284.33008\n",
      "iteration 1300 loss 284.3265\n",
      "iteration 1400 loss 284.32355\n",
      "iteration 1500 loss 284.32117\n",
      "iteration 1600 loss 284.31912\n",
      "iteration 1700 loss 284.31744\n",
      "iteration 1800 loss 284.31595\n",
      "iteration 1900 loss 284.3147\n",
      "iteration 2000 loss 284.3136\n",
      "iteration 2100 loss 284.31262\n",
      "iteration 2200 loss 284.31183\n",
      "iteration 2300 loss 284.31116\n",
      "iteration 2400 loss 284.31052\n",
      "iteration 2500 loss 284.30997\n",
      "iteration 2600 loss 284.30948\n",
      "iteration 2700 loss 284.30908\n",
      "iteration 2800 loss 284.30872\n",
      "iteration 2900 loss 284.3084\n",
      "iteration 3000 loss 284.30814\n",
      "iteration 3100 loss 284.3079\n",
      "iteration 3200 loss 284.30768\n",
      "iteration 3300 loss 284.3075\n",
      "iteration 3400 loss 284.30734\n",
      "iteration 3500 loss 284.30722\n",
      "iteration 3600 loss 284.30707\n",
      "iteration 3700 loss 284.307\n",
      "iteration 3800 loss 284.30692\n",
      "iteration 3900 loss 284.30682\n",
      "iteration 4000 loss 284.30676\n",
      "iteration 4100 loss 284.3067\n",
      "iteration 4200 loss 284.3067\n",
      "iteration 4300 loss 284.3066\n",
      "iteration 4400 loss 284.30658\n",
      "iteration 4500 loss 284.30655\n",
      "iteration 4600 loss 284.30652\n",
      "iteration 4700 loss 284.30646\n",
      "iteration 4800 loss 284.30643\n",
      "iteration 4900 loss 284.30646\n",
      "iteration 5000 loss 284.3064Train finished\n",
      "Selected features are: [14 15 40 48 69]\n",
      "Selected weights are: [0.56325096 0.998561   0.998561   0.998561   0.8442231 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.000569999055129447, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 19.46685174882486\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 246.67622\n",
      "iteration 200 loss 235.19925\n",
      "iteration 300 loss 233.33713\n",
      "iteration 400 loss 232.76553\n",
      "iteration 500 loss 232.60657\n",
      "iteration 600 loss 232.58163\n",
      "iteration 700 loss 232.56223\n",
      "iteration 800 loss 232.54672\n",
      "iteration 900 loss 232.53445\n",
      "iteration 1000 loss 232.52484\n",
      "iteration 1100 loss 232.51746\n",
      "iteration 1200 loss 232.51172\n",
      "iteration 1300 loss 232.50742\n",
      "iteration 1400 loss 232.50418\n",
      "iteration 1500 loss 232.50175\n",
      "iteration 1600 loss 232.49994\n",
      "iteration 1700 loss 232.49866\n",
      "iteration 1800 loss 232.49768\n",
      "iteration 1900 loss 232.49695\n",
      "iteration 2000 loss 232.49644\n",
      "iteration 2100 loss 232.49606\n",
      "iteration 2200 loss 232.49577\n",
      "iteration 2300 loss 232.49562\n",
      "iteration 2400 loss 232.49545\n",
      "iteration 2500 loss 232.49539\n",
      "iteration 2600 loss 232.4953\n",
      "iteration 2700 loss 232.4953\n",
      "iteration 2800 loss 232.49524\n",
      "iteration 2900 loss 232.49524\n",
      "iteration 3000 loss 232.49522\n",
      "iteration 3100 loss 232.49524\n",
      "iteration 3200 loss 232.49524\n",
      "iteration 3300 loss 232.49522\n",
      "iteration 3400 loss 232.49521\n",
      "iteration 3500 loss 232.49521\n",
      "iteration 3600 loss 232.49521\n",
      "iteration 3700 loss 232.49518\n",
      "iteration 3800 loss 232.49518\n",
      "iteration 3900 loss 232.49521\n",
      "iteration 4000 loss 232.4952\n",
      "iteration 4100 loss 232.49521\n",
      "iteration 4200 loss 232.49521\n",
      "iteration 4300 loss 232.49521\n",
      "iteration 4400 loss 232.49521\n",
      "iteration 4500 loss 232.49522\n",
      "iteration 4600 loss 232.49521\n",
      "iteration 4700 loss 232.49522\n",
      "iteration 4800 loss 232.49521\n",
      "iteration 4900 loss 232.49524\n",
      "iteration 5000 loss 232.49521Train finished\n",
      "Selected features are: [14 15 40 48 58]\n",
      "Selected weights are: [0.5302909  0.99793303 0.99793303 0.99793303 0.565678  ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0005847648279365856, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 41.205821458770345\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 304.66458\n",
      "iteration 200 loss 301.78943\n",
      "iteration 300 loss 300.5912\n",
      "iteration 400 loss 300.3216\n",
      "iteration 500 loss 300.30704\n",
      "iteration 600 loss 300.30258\n",
      "iteration 700 loss 300.30035\n",
      "iteration 800 loss 300.29895\n",
      "iteration 900 loss 300.298\n",
      "iteration 1000 loss 300.29736\n",
      "iteration 1100 loss 300.29684\n",
      "iteration 1200 loss 300.2965\n",
      "iteration 1300 loss 300.2962\n",
      "iteration 1400 loss 300.29593\n",
      "iteration 1500 loss 300.29565\n",
      "iteration 1600 loss 300.29544\n",
      "iteration 1700 loss 300.29526\n",
      "iteration 1800 loss 300.29504\n",
      "iteration 1900 loss 300.2949\n",
      "iteration 2000 loss 300.29474\n",
      "iteration 2100 loss 300.29453\n",
      "iteration 2200 loss 300.29443\n",
      "iteration 2300 loss 300.29425\n",
      "iteration 2400 loss 300.2941\n",
      "iteration 2500 loss 300.29395\n",
      "iteration 2600 loss 300.2938\n",
      "iteration 2700 loss 300.2937\n",
      "iteration 2800 loss 300.2936\n",
      "iteration 2900 loss 300.29346\n",
      "iteration 3000 loss 300.29337\n",
      "iteration 3100 loss 300.29327\n",
      "iteration 3200 loss 300.29318\n",
      "iteration 3300 loss 300.29303\n",
      "iteration 3400 loss 300.29294\n",
      "iteration 3500 loss 300.29288\n",
      "iteration 3600 loss 300.29272\n",
      "iteration 3700 loss 300.29266\n",
      "iteration 3800 loss 300.2926\n",
      "iteration 3900 loss 300.29248\n",
      "iteration 4000 loss 300.29248\n",
      "iteration 4100 loss 300.29236\n",
      "iteration 4200 loss 300.2923\n",
      "iteration 4300 loss 300.2922\n",
      "iteration 4400 loss 300.29218\n",
      "iteration 4500 loss 300.2921\n",
      "iteration 4600 loss 300.29205\n",
      "iteration 4700 loss 300.292\n",
      "iteration 4800 loss 300.29193\n",
      "iteration 4900 loss 300.29187\n",
      "iteration 5000 loss 300.2918Train finished\n",
      "Selected features are: [14 15 40 48 63]\n",
      "Selected weights are: [0.48804566 0.99848574 0.74371344 0.99848574 0.5861476 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0006816010986399904, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 13.183448766929333\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 299.54758\n",
      "iteration 200 loss 296.1569\n",
      "iteration 300 loss 294.00583\n",
      "iteration 400 loss 290.56238\n",
      "iteration 500 loss 289.84\n",
      "iteration 600 loss 289.638\n",
      "iteration 700 loss 289.53757\n",
      "iteration 800 loss 289.48688\n",
      "iteration 900 loss 289.45374\n",
      "iteration 1000 loss 289.4254\n",
      "iteration 1100 loss 289.39954\n",
      "iteration 1200 loss 289.37427\n",
      "iteration 1300 loss 289.3483\n",
      "iteration 1400 loss 289.32544\n",
      "iteration 1500 loss 289.31287\n",
      "iteration 1600 loss 289.3025\n",
      "iteration 1700 loss 289.294\n",
      "iteration 1800 loss 289.287\n",
      "iteration 1900 loss 289.28122\n",
      "iteration 2000 loss 289.27722\n",
      "iteration 2100 loss 289.27582\n",
      "iteration 2200 loss 289.2748\n",
      "iteration 2300 loss 289.27408\n",
      "iteration 2400 loss 289.27362\n",
      "iteration 2500 loss 289.27325\n",
      "iteration 2600 loss 289.27295\n",
      "iteration 2700 loss 289.27274\n",
      "iteration 2800 loss 289.2726\n",
      "iteration 2900 loss 289.2725\n",
      "iteration 3000 loss 289.2724\n",
      "iteration 3100 loss 289.27234\n",
      "iteration 3200 loss 289.27228\n",
      "iteration 3300 loss 289.27222\n",
      "iteration 3400 loss 289.27222\n",
      "iteration 3500 loss 289.2722\n",
      "iteration 3600 loss 289.2722\n",
      "iteration 3700 loss 289.27216\n",
      "iteration 3800 loss 289.27216\n",
      "iteration 3900 loss 289.27216\n",
      "iteration 4000 loss 289.27216\n",
      "iteration 4100 loss 289.27213\n",
      "iteration 4200 loss 289.2721\n",
      "iteration 4300 loss 289.27213\n",
      "iteration 4400 loss 289.27213\n",
      "iteration 4500 loss 289.27213\n",
      "iteration 4600 loss 289.27216\n",
      "iteration 4700 loss 289.27213\n",
      "iteration 4800 loss 289.27213\n",
      "iteration 4900 loss 289.27213\n",
      "iteration 5000 loss 289.27213Train finished\n",
      "Selected features are: [15 33 51 54 56]\n",
      "Selected weights are: [0.6257568 0.9987903 0.9987903 0.9987903 0.9987903]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0019453217647426991, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 19.048565521301132\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 217.94278\n",
      "iteration 200 loss 215.48805\n",
      "iteration 300 loss 213.78604\n",
      "iteration 400 loss 212.61847\n",
      "iteration 500 loss 212.14706\n",
      "iteration 600 loss 211.88565\n",
      "iteration 700 loss 211.72026\n",
      "iteration 800 loss 211.68843\n",
      "iteration 900 loss 211.66673\n",
      "iteration 1000 loss 211.65327\n",
      "iteration 1100 loss 211.64792\n",
      "iteration 1200 loss 211.64365\n",
      "iteration 1300 loss 211.64012\n",
      "iteration 1400 loss 211.6372\n",
      "iteration 1500 loss 211.63481\n",
      "iteration 1600 loss 211.6328\n",
      "iteration 1700 loss 211.63109\n",
      "iteration 1800 loss 211.62967\n",
      "iteration 1900 loss 211.62843\n",
      "iteration 2000 loss 211.62738\n",
      "iteration 2100 loss 211.62643\n",
      "iteration 2200 loss 211.62561\n",
      "iteration 2300 loss 211.62485\n",
      "iteration 2400 loss 211.62425\n",
      "iteration 2500 loss 211.62367\n",
      "iteration 2600 loss 211.62314\n",
      "iteration 2700 loss 211.62267\n",
      "iteration 2800 loss 211.62225\n",
      "iteration 2900 loss 211.62186\n",
      "iteration 3000 loss 211.62149\n",
      "iteration 3100 loss 211.62115\n",
      "iteration 3200 loss 211.62086\n",
      "iteration 3300 loss 211.62059\n",
      "iteration 3400 loss 211.62032\n",
      "iteration 3500 loss 211.62006\n",
      "iteration 3600 loss 211.61986\n",
      "iteration 3700 loss 211.61963\n",
      "iteration 3800 loss 211.61945\n",
      "iteration 3900 loss 211.61926\n",
      "iteration 4000 loss 211.61911\n",
      "iteration 4100 loss 211.61894\n",
      "iteration 4200 loss 211.6188\n",
      "iteration 4300 loss 211.61868\n",
      "iteration 4400 loss 211.61856\n",
      "iteration 4500 loss 211.61841\n",
      "iteration 4600 loss 211.6183\n",
      "iteration 4700 loss 211.61826\n",
      "iteration 4800 loss 211.6181\n",
      "iteration 4900 loss 211.61801\n",
      "iteration 5000 loss 211.61795Train finished\n",
      "Selected features are: [15 33 48 51 63]\n",
      "Selected weights are: [0.9987174  0.9987174  0.8835692  0.9987174  0.47306088]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0017099404584987254, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 52.08897375384638\n",
      "\n",
      " 4 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 232.0463\n",
      "iteration 200 loss 230.46161\n",
      "iteration 300 loss 228.27484\n",
      "iteration 400 loss 227.49533\n",
      "iteration 500 loss 227.14845\n",
      "iteration 600 loss 226.71721\n",
      "iteration 700 loss 226.13484\n",
      "iteration 800 loss 225.77097\n",
      "iteration 900 loss 225.66245\n",
      "iteration 1000 loss 225.56854\n",
      "iteration 1100 loss 225.4899\n",
      "iteration 1200 loss 225.41602\n",
      "iteration 1300 loss 225.37979\n",
      "iteration 1400 loss 225.37042\n",
      "iteration 1500 loss 225.36707\n",
      "iteration 1600 loss 225.36441\n",
      "iteration 1700 loss 225.36241\n",
      "iteration 1800 loss 225.3608\n",
      "iteration 1900 loss 225.35953\n",
      "iteration 2000 loss 225.35846\n",
      "iteration 2100 loss 225.35754\n",
      "iteration 2200 loss 225.35678\n",
      "iteration 2300 loss 225.35614\n",
      "iteration 2400 loss 225.35556\n",
      "iteration 2500 loss 225.35498\n",
      "iteration 2600 loss 225.35449\n",
      "iteration 2700 loss 225.354\n",
      "iteration 2800 loss 225.35353\n",
      "iteration 2900 loss 225.3531\n",
      "iteration 3000 loss 225.3527\n",
      "iteration 3100 loss 225.35226\n",
      "iteration 3200 loss 225.35187\n",
      "iteration 3300 loss 225.35144\n",
      "iteration 3400 loss 225.35107\n",
      "iteration 3500 loss 225.35068\n",
      "iteration 3600 loss 225.3503\n",
      "iteration 3700 loss 225.34995\n",
      "iteration 3800 loss 225.34958\n",
      "iteration 3900 loss 225.34917\n",
      "iteration 4000 loss 225.34882\n",
      "iteration 4100 loss 225.34845\n",
      "iteration 4200 loss 225.34807\n",
      "iteration 4300 loss 225.34772\n",
      "iteration 4400 loss 225.34737\n",
      "iteration 4500 loss 225.34698\n",
      "iteration 4600 loss 225.34666\n",
      "iteration 4700 loss 225.34631\n",
      "iteration 4800 loss 225.34598\n",
      "iteration 4900 loss 225.34561\n",
      "iteration 5000 loss 225.34526Train finished\n",
      "Selected features are: [ 7 11 26 46 49]\n",
      "Selected weights are: [0.9994558 0.9994558 0.9994558 0.7647848 0.9994558]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 9.568128408493987e-05, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 48.21520332946277\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 312.79202\n",
      "iteration 200 loss 308.70193\n",
      "iteration 300 loss 306.91107\n",
      "iteration 400 loss 305.87546\n",
      "iteration 500 loss 305.32895\n",
      "iteration 600 loss 305.10663\n",
      "iteration 700 loss 304.87012\n",
      "iteration 800 loss 304.6366\n",
      "iteration 900 loss 304.49774\n",
      "iteration 1000 loss 304.44556\n",
      "iteration 1100 loss 304.44553\n",
      "iteration 1200 loss 304.44553\n",
      "iteration 1300 loss 304.44553\n",
      "iteration 1400 loss 304.4455\n",
      "iteration 1500 loss 304.4455\n",
      "iteration 1600 loss 304.4455\n",
      "iteration 1700 loss 304.4455\n",
      "iteration 1800 loss 304.4455\n",
      "iteration 1900 loss 304.4455\n",
      "iteration 2000 loss 304.4455\n",
      "iteration 2100 loss 304.4455\n",
      "iteration 2200 loss 304.4455\n",
      "iteration 2300 loss 304.4455\n",
      "iteration 2400 loss 304.4455\n",
      "iteration 2500 loss 304.4455\n",
      "iteration 2600 loss 304.4455\n",
      "iteration 2700 loss 304.4455\n",
      "iteration 2800 loss 304.4455\n",
      "iteration 2900 loss 304.4455\n",
      "iteration 3000 loss 304.4455\n",
      "iteration 3100 loss 304.4455\n",
      "iteration 3200 loss 304.44547\n",
      "iteration 3300 loss 304.4455\n",
      "iteration 3400 loss 304.4455\n",
      "iteration 3500 loss 304.4455\n",
      "iteration 3600 loss 304.4455\n",
      "iteration 3700 loss 304.4455\n",
      "iteration 3800 loss 304.4455\n",
      "iteration 3900 loss 304.4455\n",
      "iteration 4000 loss 304.4455\n",
      "iteration 4100 loss 304.4455\n",
      "iteration 4200 loss 304.4455\n",
      "iteration 4300 loss 304.4455\n",
      "iteration 4400 loss 304.4455\n",
      "iteration 4500 loss 304.4455\n",
      "iteration 4600 loss 304.4455\n",
      "iteration 4700 loss 304.4455\n",
      "iteration 4800 loss 304.4455\n",
      "iteration 4900 loss 304.4455\n",
      "iteration 5000 loss 304.4455Train finished\n",
      "Selected features are: [33 46 51 54 56]\n",
      "Selected weights are: [0.99831647 0.99831647 0.99831647 0.60684484 0.99831647]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.004410365555528746, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 13.267425291432597\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 279.00037\n",
      "iteration 200 loss 273.59534\n",
      "iteration 300 loss 272.07275\n",
      "iteration 400 loss 271.63928\n",
      "iteration 500 loss 271.4506\n",
      "iteration 600 loss 271.35638\n",
      "iteration 700 loss 271.28217\n",
      "iteration 800 loss 271.24005\n",
      "iteration 900 loss 271.22504\n",
      "iteration 1000 loss 271.21606\n",
      "iteration 1100 loss 271.2105\n",
      "iteration 1200 loss 271.20703\n",
      "iteration 1300 loss 271.20483\n",
      "iteration 1400 loss 271.2029\n",
      "iteration 1500 loss 271.2013\n",
      "iteration 1600 loss 271.1999\n",
      "iteration 1700 loss 271.19867\n",
      "iteration 1800 loss 271.1976\n",
      "iteration 1900 loss 271.19653\n",
      "iteration 2000 loss 271.1955\n",
      "iteration 2100 loss 271.19446\n",
      "iteration 2200 loss 271.19342\n",
      "iteration 2300 loss 271.19232\n",
      "iteration 2400 loss 271.19122\n",
      "iteration 2500 loss 271.1901\n",
      "iteration 2600 loss 271.1889\n",
      "iteration 2700 loss 271.18768\n",
      "iteration 2800 loss 271.18646\n",
      "iteration 2900 loss 271.18524\n",
      "iteration 3000 loss 271.1839\n",
      "iteration 3100 loss 271.18256\n",
      "iteration 3200 loss 271.18115\n",
      "iteration 3300 loss 271.1797\n",
      "iteration 3400 loss 271.17816\n",
      "iteration 3500 loss 271.1767\n",
      "iteration 3600 loss 271.1751\n",
      "iteration 3700 loss 271.1735\n",
      "iteration 3800 loss 271.17218\n",
      "iteration 3900 loss 271.1721\n",
      "iteration 4000 loss 271.17206\n",
      "iteration 4100 loss 271.172\n",
      "iteration 4200 loss 271.172\n",
      "iteration 4300 loss 271.172\n",
      "iteration 4400 loss 271.17203\n",
      "iteration 4500 loss 271.172\n",
      "iteration 4600 loss 271.172\n",
      "iteration 4700 loss 271.172\n",
      "iteration 4800 loss 271.172\n",
      "iteration 4900 loss 271.17206\n",
      "iteration 5000 loss 271.172Train finished\n",
      "Selected features are: [11 33 46 51 56]\n",
      "Selected weights are: [0.7679635  0.9991304  0.9991304  0.96068937 0.9991304 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.001981620432656419, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 27.384449369648344\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 287.5947\n",
      "iteration 200 loss 285.0675\n",
      "iteration 300 loss 283.18396\n",
      "iteration 400 loss 282.92383\n",
      "iteration 500 loss 282.89465\n",
      "iteration 600 loss 282.89175\n",
      "iteration 700 loss 282.8896\n",
      "iteration 800 loss 282.88794\n",
      "iteration 900 loss 282.88654\n",
      "iteration 1000 loss 282.88535\n",
      "iteration 1100 loss 282.88428\n",
      "iteration 1200 loss 282.88327\n",
      "iteration 1300 loss 282.88232\n",
      "iteration 1400 loss 282.88144\n",
      "iteration 1500 loss 282.8806\n",
      "iteration 1600 loss 282.8798\n",
      "iteration 1700 loss 282.87906\n",
      "iteration 1800 loss 282.87833\n",
      "iteration 1900 loss 282.87756\n",
      "iteration 2000 loss 282.87686\n",
      "iteration 2100 loss 282.87622\n",
      "iteration 2200 loss 282.87555\n",
      "iteration 2300 loss 282.875\n",
      "iteration 2400 loss 282.87436\n",
      "iteration 2500 loss 282.87378\n",
      "iteration 2600 loss 282.87323\n",
      "iteration 2700 loss 282.8727\n",
      "iteration 2800 loss 282.8722\n",
      "iteration 2900 loss 282.8717\n",
      "iteration 3000 loss 282.87122\n",
      "iteration 3100 loss 282.87073\n",
      "iteration 3200 loss 282.8703\n",
      "iteration 3300 loss 282.8698\n",
      "iteration 3400 loss 282.86942\n",
      "iteration 3500 loss 282.869\n",
      "iteration 3600 loss 282.8686\n",
      "iteration 3700 loss 282.86816\n",
      "iteration 3800 loss 282.86786\n",
      "iteration 3900 loss 282.8675\n",
      "iteration 4000 loss 282.86713\n",
      "iteration 4100 loss 282.86676\n",
      "iteration 4200 loss 282.86636\n",
      "iteration 4300 loss 282.8661\n",
      "iteration 4400 loss 282.86572\n",
      "iteration 4500 loss 282.86545\n",
      "iteration 4600 loss 282.86505\n",
      "iteration 4700 loss 282.86487\n",
      "iteration 4800 loss 282.86456\n",
      "iteration 4900 loss 282.86426\n",
      "iteration 5000 loss 282.86395Train finished\n",
      "Selected features are: [ 8 14 15 48 63]\n",
      "Selected weights are: [0.41018566 0.9988652  0.9988652  0.9988652  0.56199276]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0006979636862641346, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 20.075254955803445\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 249.6506\n",
      "iteration 200 loss 244.27362\n",
      "iteration 300 loss 244.03648\n",
      "iteration 400 loss 243.97716\n",
      "iteration 500 loss 243.93996\n",
      "iteration 600 loss 243.91373\n",
      "iteration 700 loss 243.90028\n",
      "iteration 800 loss 243.8887\n",
      "iteration 900 loss 243.87866\n",
      "iteration 1000 loss 243.87195\n",
      "iteration 1100 loss 243.86844\n",
      "iteration 1200 loss 243.86578\n",
      "iteration 1300 loss 243.86378\n",
      "iteration 1400 loss 243.86238\n",
      "iteration 1500 loss 243.8613\n",
      "iteration 1600 loss 243.86053\n",
      "iteration 1700 loss 243.85999\n",
      "iteration 1800 loss 243.85956\n",
      "iteration 1900 loss 243.85922\n",
      "iteration 2000 loss 243.85901\n",
      "iteration 2100 loss 243.85886\n",
      "iteration 2200 loss 243.85876\n",
      "iteration 2300 loss 243.85866\n",
      "iteration 2400 loss 243.85861\n",
      "iteration 2500 loss 243.85855\n",
      "iteration 2600 loss 243.85855\n",
      "iteration 2700 loss 243.85854\n",
      "iteration 2800 loss 243.85852\n",
      "iteration 2900 loss 243.85849\n",
      "iteration 3000 loss 243.85847\n",
      "iteration 3100 loss 243.85847\n",
      "iteration 3200 loss 243.85849\n",
      "iteration 3300 loss 243.85849\n",
      "iteration 3400 loss 243.85849\n",
      "iteration 3500 loss 243.85849\n",
      "iteration 3600 loss 243.85849\n",
      "iteration 3700 loss 243.85849\n",
      "iteration 3800 loss 243.85849\n",
      "iteration 3900 loss 243.85849\n",
      "iteration 4000 loss 243.85849\n",
      "iteration 4100 loss 243.85846\n",
      "iteration 4200 loss 243.85847\n",
      "iteration 4300 loss 243.85849\n",
      "iteration 4400 loss 243.85849\n",
      "iteration 4500 loss 243.85849\n",
      "iteration 4600 loss 243.85849\n",
      "iteration 4700 loss 243.8585\n",
      "iteration 4800 loss 243.85849\n",
      "iteration 4900 loss 243.8585\n",
      "iteration 5000 loss 243.85854Train finished\n",
      "Selected features are: [14 15 40 48 63]\n",
      "Selected weights are: [0.46882215 0.99858314 0.99858314 0.99858314 0.66516495]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0006474220821523945, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 34.53707134582354\n",
      "\n",
      " 5 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 278.90033\n",
      "iteration 200 loss 274.9367\n",
      "iteration 300 loss 274.15427\n",
      "iteration 400 loss 273.62378\n",
      "iteration 500 loss 273.42075\n",
      "iteration 600 loss 273.2866\n",
      "iteration 700 loss 273.18628\n",
      "iteration 800 loss 273.12634\n",
      "iteration 900 loss 273.06696\n",
      "iteration 1000 loss 273.00684\n",
      "iteration 1100 loss 272.9594\n",
      "iteration 1200 loss 272.92874\n",
      "iteration 1300 loss 272.9024\n",
      "iteration 1400 loss 272.87952\n",
      "iteration 1500 loss 272.85944\n",
      "iteration 1600 loss 272.84454\n",
      "iteration 1700 loss 272.84412\n",
      "iteration 1800 loss 272.84378\n",
      "iteration 1900 loss 272.84354\n",
      "iteration 2000 loss 272.84332\n",
      "iteration 2100 loss 272.8431\n",
      "iteration 2200 loss 272.84293\n",
      "iteration 2300 loss 272.84274\n",
      "iteration 2400 loss 272.84262\n",
      "iteration 2500 loss 272.8425\n",
      "iteration 2600 loss 272.8424\n",
      "iteration 2700 loss 272.8423\n",
      "iteration 2800 loss 272.84222\n",
      "iteration 2900 loss 272.8421\n",
      "iteration 3000 loss 272.84204\n",
      "iteration 3100 loss 272.84198\n",
      "iteration 3200 loss 272.8419\n",
      "iteration 3300 loss 272.8419\n",
      "iteration 3400 loss 272.84183\n",
      "iteration 3500 loss 272.8418\n",
      "iteration 3600 loss 272.84174\n",
      "iteration 3700 loss 272.84174\n",
      "iteration 3800 loss 272.8417\n",
      "iteration 3900 loss 272.84167\n",
      "iteration 4000 loss 272.84167\n",
      "iteration 4100 loss 272.84164\n",
      "iteration 4200 loss 272.84164\n",
      "iteration 4300 loss 272.8416\n",
      "iteration 4400 loss 272.8416\n",
      "iteration 4500 loss 272.84158\n",
      "iteration 4600 loss 272.84158\n",
      "iteration 4700 loss 272.8416\n",
      "iteration 4800 loss 272.8416\n",
      "iteration 4900 loss 272.84155\n",
      "iteration 5000 loss 272.84155Train finished\n",
      "Selected features are: [ 8 11 26 46 56]\n",
      "Selected weights are: [0.53272724 0.40922403 0.99877566 0.99877566 0.99877566]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0013785586213382521, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 27.930344815574603\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 315.59467\n",
      "iteration 200 loss 310.672\n",
      "iteration 300 loss 309.39072\n",
      "iteration 400 loss 309.15024\n",
      "iteration 500 loss 309.01807\n",
      "iteration 600 loss 308.94217\n",
      "iteration 700 loss 308.90076\n",
      "iteration 800 loss 308.8688\n",
      "iteration 900 loss 308.847\n",
      "iteration 1000 loss 308.82812\n",
      "iteration 1100 loss 308.81058\n",
      "iteration 1200 loss 308.79434\n",
      "iteration 1300 loss 308.78094\n",
      "iteration 1400 loss 308.77777\n",
      "iteration 1500 loss 308.77505\n",
      "iteration 1600 loss 308.7727\n",
      "iteration 1700 loss 308.77063\n",
      "iteration 1800 loss 308.7688\n",
      "iteration 1900 loss 308.76715\n",
      "iteration 2000 loss 308.76562\n",
      "iteration 2100 loss 308.76425\n",
      "iteration 2200 loss 308.76312\n",
      "iteration 2300 loss 308.76196\n",
      "iteration 2400 loss 308.7609\n",
      "iteration 2500 loss 308.75998\n",
      "iteration 2600 loss 308.7591\n",
      "iteration 2700 loss 308.7583\n",
      "iteration 2800 loss 308.75757\n",
      "iteration 2900 loss 308.7569\n",
      "iteration 3000 loss 308.7563\n",
      "iteration 3100 loss 308.7557\n",
      "iteration 3200 loss 308.7552\n",
      "iteration 3300 loss 308.7547\n",
      "iteration 3400 loss 308.7542\n",
      "iteration 3500 loss 308.75385\n",
      "iteration 3600 loss 308.75348\n",
      "iteration 3700 loss 308.7531\n",
      "iteration 3800 loss 308.75278\n",
      "iteration 3900 loss 308.75247\n",
      "iteration 4000 loss 308.75223\n",
      "iteration 4100 loss 308.75198\n",
      "iteration 4200 loss 308.75174\n",
      "iteration 4300 loss 308.75153\n",
      "iteration 4400 loss 308.75128\n",
      "iteration 4500 loss 308.75116\n",
      "iteration 4600 loss 308.75098\n",
      "iteration 4700 loss 308.7508\n",
      "iteration 4800 loss 308.75064\n",
      "iteration 4900 loss 308.75052\n",
      "iteration 5000 loss 308.7504Train finished\n",
      "Selected features are: [14 15 40 48 58]\n",
      "Selected weights are: [0.48040926 0.9985665  0.9985665  0.9985665  0.88999885]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0006182341263166732, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 11.547321473943796\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 226.1932\n",
      "iteration 200 loss 214.33882\n",
      "iteration 300 loss 212.0781\n",
      "iteration 400 loss 211.33817\n",
      "iteration 500 loss 211.11392\n",
      "iteration 600 loss 210.9931\n",
      "iteration 700 loss 210.92154\n",
      "iteration 800 loss 210.87099\n",
      "iteration 900 loss 210.83102\n",
      "iteration 1000 loss 210.80817\n",
      "iteration 1100 loss 210.79576\n",
      "iteration 1200 loss 210.78635\n",
      "iteration 1300 loss 210.77916\n",
      "iteration 1400 loss 210.77539\n",
      "iteration 1500 loss 210.77402\n",
      "iteration 1600 loss 210.77306\n",
      "iteration 1700 loss 210.77245\n",
      "iteration 1800 loss 210.77196\n",
      "iteration 1900 loss 210.77164\n",
      "iteration 2000 loss 210.7713\n",
      "iteration 2100 loss 210.77109\n",
      "iteration 2200 loss 210.77086\n",
      "iteration 2300 loss 210.77065\n",
      "iteration 2400 loss 210.77046\n",
      "iteration 2500 loss 210.77026\n",
      "iteration 2600 loss 210.77011\n",
      "iteration 2700 loss 210.76999\n",
      "iteration 2800 loss 210.76985\n",
      "iteration 2900 loss 210.76971\n",
      "iteration 3000 loss 210.76959\n",
      "iteration 3100 loss 210.76944\n",
      "iteration 3200 loss 210.76938\n",
      "iteration 3300 loss 210.76923\n",
      "iteration 3400 loss 210.76915\n",
      "iteration 3500 loss 210.76906\n",
      "iteration 3600 loss 210.76898\n",
      "iteration 3700 loss 210.7689\n",
      "iteration 3800 loss 210.7688\n",
      "iteration 3900 loss 210.76875\n",
      "iteration 4000 loss 210.76868\n",
      "iteration 4100 loss 210.76862\n",
      "iteration 4200 loss 210.76855\n",
      "iteration 4300 loss 210.76848\n",
      "iteration 4400 loss 210.76845\n",
      "iteration 4500 loss 210.7684\n",
      "iteration 4600 loss 210.76836\n",
      "iteration 4700 loss 210.76834\n",
      "iteration 4800 loss 210.7683\n",
      "iteration 4900 loss 210.76825\n",
      "iteration 5000 loss 210.76822Train finished\n",
      "Selected features are: [14 15 40 48 58]\n",
      "Selected weights are: [0.5126524 0.9988012 0.7248029 0.9988012 0.9954956]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0005811784310778675, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 51.68202175827279\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 273.1031\n",
      "iteration 200 loss 268.68915\n",
      "iteration 300 loss 267.48178\n",
      "iteration 400 loss 267.14343\n",
      "iteration 500 loss 266.92453\n",
      "iteration 600 loss 266.86536\n",
      "iteration 700 loss 266.8246\n",
      "iteration 800 loss 266.79672\n",
      "iteration 900 loss 266.78528\n",
      "iteration 1000 loss 266.77756\n",
      "iteration 1100 loss 266.76807\n",
      "iteration 1200 loss 266.7566\n",
      "iteration 1300 loss 266.74274\n",
      "iteration 1400 loss 266.72614\n",
      "iteration 1500 loss 266.70657\n",
      "iteration 1600 loss 266.68378\n",
      "iteration 1700 loss 266.6562\n",
      "iteration 1800 loss 266.64233\n",
      "iteration 1900 loss 266.64188\n",
      "iteration 2000 loss 266.64172\n",
      "iteration 2100 loss 266.64166\n",
      "iteration 2200 loss 266.6416\n",
      "iteration 2300 loss 266.64154\n",
      "iteration 2400 loss 266.6415\n",
      "iteration 2500 loss 266.64148\n",
      "iteration 2600 loss 266.64148\n",
      "iteration 2700 loss 266.64142\n",
      "iteration 2800 loss 266.64136\n",
      "iteration 2900 loss 266.64136\n",
      "iteration 3000 loss 266.6413\n",
      "iteration 3100 loss 266.6413\n",
      "iteration 3200 loss 266.64124\n",
      "iteration 3300 loss 266.64124\n",
      "iteration 3400 loss 266.64117\n",
      "iteration 3500 loss 266.6411\n",
      "iteration 3600 loss 266.6411\n",
      "iteration 3700 loss 266.64105\n",
      "iteration 3800 loss 266.641\n",
      "iteration 3900 loss 266.641\n",
      "iteration 4000 loss 266.64096\n",
      "iteration 4100 loss 266.64093\n",
      "iteration 4200 loss 266.64087\n",
      "iteration 4300 loss 266.64084\n",
      "iteration 4400 loss 266.6408\n",
      "iteration 4500 loss 266.64075\n",
      "iteration 4600 loss 266.64072\n",
      "iteration 4700 loss 266.6407\n",
      "iteration 4800 loss 266.64062\n",
      "iteration 4900 loss 266.64056\n",
      "iteration 5000 loss 266.64053Train finished\n",
      "Selected features are: [33 46 49 51 56]\n",
      "Selected weights are: [0.99912316 0.94809216 0.99912316 0.5503135  0.99912316]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.004074418396370639, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 29.888603005572605\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 280.6916\n",
      "iteration 200 loss 277.34772\n",
      "iteration 300 loss 276.41608\n",
      "iteration 400 loss 276.22418\n",
      "iteration 500 loss 276.1318\n",
      "iteration 600 loss 276.0747\n",
      "iteration 700 loss 276.0443\n",
      "iteration 800 loss 276.03424\n",
      "iteration 900 loss 276.028\n",
      "iteration 1000 loss 276.02368\n",
      "iteration 1100 loss 276.02054\n",
      "iteration 1200 loss 276.018\n",
      "iteration 1300 loss 276.016\n",
      "iteration 1400 loss 276.01544\n",
      "iteration 1500 loss 276.0151\n",
      "iteration 1600 loss 276.01486\n",
      "iteration 1700 loss 276.01477\n",
      "iteration 1800 loss 276.01465\n",
      "iteration 1900 loss 276.01465\n",
      "iteration 2000 loss 276.0146\n",
      "iteration 2100 loss 276.01453\n",
      "iteration 2200 loss 276.01453\n",
      "iteration 2300 loss 276.01456\n",
      "iteration 2400 loss 276.01456\n",
      "iteration 2500 loss 276.0145\n",
      "iteration 2600 loss 276.01453\n",
      "iteration 2700 loss 276.01453\n",
      "iteration 2800 loss 276.01453\n",
      "iteration 2900 loss 276.01453\n",
      "iteration 3000 loss 276.01453\n",
      "iteration 3100 loss 276.01453\n",
      "iteration 3200 loss 276.01453\n",
      "iteration 3300 loss 276.01453\n",
      "iteration 3400 loss 276.01453\n",
      "iteration 3500 loss 276.01456\n",
      "iteration 3600 loss 276.01453\n",
      "iteration 3700 loss 276.01453\n",
      "iteration 3800 loss 276.01453\n",
      "iteration 3900 loss 276.01453\n",
      "iteration 4000 loss 276.01453\n",
      "iteration 4100 loss 276.01453\n",
      "iteration 4200 loss 276.01453\n",
      "iteration 4300 loss 276.01453\n",
      "iteration 4400 loss 276.01456\n",
      "iteration 4500 loss 276.01456\n",
      "iteration 4600 loss 276.01453\n",
      "iteration 4700 loss 276.01453\n",
      "iteration 4800 loss 276.01453\n",
      "iteration 4900 loss 276.01453\n",
      "iteration 5000 loss 276.01456Train finished\n",
      "Selected features are: [14 15 40 48 58]\n",
      "Selected weights are: [0.49089208 0.9988495  0.9834709  0.9988495  0.4662755 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0006296118184585838, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 23.496671835833762\n",
      "\n",
      " 6 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 232.30005\n",
      "iteration 200 loss 226.906\n",
      "iteration 300 loss 226.20663\n",
      "iteration 400 loss 226.08148\n",
      "iteration 500 loss 225.99063\n",
      "iteration 600 loss 225.92506\n",
      "iteration 700 loss 225.8829\n",
      "iteration 800 loss 225.85048\n",
      "iteration 900 loss 225.83081\n",
      "iteration 1000 loss 225.81505\n",
      "iteration 1100 loss 225.8019\n",
      "iteration 1200 loss 225.79059\n",
      "iteration 1300 loss 225.78085\n",
      "iteration 1400 loss 225.77238\n",
      "iteration 1500 loss 225.76503\n",
      "iteration 1600 loss 225.75862\n",
      "iteration 1700 loss 225.75299\n",
      "iteration 1800 loss 225.74808\n",
      "iteration 1900 loss 225.74384\n",
      "iteration 2000 loss 225.74265\n",
      "iteration 2100 loss 225.74239\n",
      "iteration 2200 loss 225.74231\n",
      "iteration 2300 loss 225.74228\n",
      "iteration 2400 loss 225.74223\n",
      "iteration 2500 loss 225.74222\n",
      "iteration 2600 loss 225.74222\n",
      "iteration 2700 loss 225.74219\n",
      "iteration 2800 loss 225.74219\n",
      "iteration 2900 loss 225.74219\n",
      "iteration 3000 loss 225.74217\n",
      "iteration 3100 loss 225.74217\n",
      "iteration 3200 loss 225.74222\n",
      "iteration 3300 loss 225.74219\n",
      "iteration 3400 loss 225.74217\n",
      "iteration 3500 loss 225.74216\n",
      "iteration 3600 loss 225.74217\n",
      "iteration 3700 loss 225.74217\n",
      "iteration 3800 loss 225.74214\n",
      "iteration 3900 loss 225.74214\n",
      "iteration 4000 loss 225.74219\n",
      "iteration 4100 loss 225.74217\n",
      "iteration 4200 loss 225.74216\n",
      "iteration 4300 loss 225.74216\n",
      "iteration 4400 loss 225.74217\n",
      "iteration 4500 loss 225.74216\n",
      "iteration 4600 loss 225.74216\n",
      "iteration 4700 loss 225.74217\n",
      "iteration 4800 loss 225.74216\n",
      "iteration 4900 loss 225.74216\n",
      "iteration 5000 loss 225.74219Train finished\n",
      "Selected features are: [14 15 40 48 58]\n",
      "Selected weights are: [0.51530087 0.94085467 0.9990644  0.9990644  0.9990644 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00071857676686256, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 43.888546052214\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 272.71185\n",
      "iteration 200 loss 266.4159\n",
      "iteration 300 loss 265.56018\n",
      "iteration 400 loss 265.27368\n",
      "iteration 500 loss 265.07812\n",
      "iteration 600 loss 264.96545\n",
      "iteration 700 loss 264.90167\n",
      "iteration 800 loss 264.838\n",
      "iteration 900 loss 264.80234\n",
      "iteration 1000 loss 264.76892\n",
      "iteration 1100 loss 264.73352\n",
      "iteration 1200 loss 264.70526\n",
      "iteration 1300 loss 264.69788\n",
      "iteration 1400 loss 264.69037\n",
      "iteration 1500 loss 264.68256\n",
      "iteration 1600 loss 264.67834\n",
      "iteration 1700 loss 264.67838\n",
      "iteration 1800 loss 264.67834\n",
      "iteration 1900 loss 264.67834\n",
      "iteration 2000 loss 264.67834\n",
      "iteration 2100 loss 264.67834\n",
      "iteration 2200 loss 264.67834\n",
      "iteration 2300 loss 264.67834\n",
      "iteration 2400 loss 264.67834\n",
      "iteration 2500 loss 264.67834\n",
      "iteration 2600 loss 264.67834\n",
      "iteration 2700 loss 264.67834\n",
      "iteration 2800 loss 264.67834\n",
      "iteration 2900 loss 264.67834\n",
      "iteration 3000 loss 264.67834\n",
      "iteration 3100 loss 264.67834\n",
      "iteration 3200 loss 264.67834\n",
      "iteration 3300 loss 264.67834\n",
      "iteration 3400 loss 264.67834\n",
      "iteration 3500 loss 264.67834\n",
      "iteration 3600 loss 264.67834\n",
      "iteration 3700 loss 264.67834\n",
      "iteration 3800 loss 264.67834\n",
      "iteration 3900 loss 264.67834\n",
      "iteration 4000 loss 264.67834\n",
      "iteration 4100 loss 264.67834\n",
      "iteration 4200 loss 264.67834\n",
      "iteration 4300 loss 264.67834\n",
      "iteration 4400 loss 264.67834\n",
      "iteration 4500 loss 264.67834\n",
      "iteration 4600 loss 264.67834\n",
      "iteration 4700 loss 264.67834\n",
      "iteration 4800 loss 264.67834\n",
      "iteration 4900 loss 264.67834\n",
      "iteration 5000 loss 264.67834Train finished\n",
      "Selected features are: [33 46 49 51 56]\n",
      "Selected weights are: [0.78303266 0.8668792  0.998243   0.6086155  0.998243  ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.004342377338208164, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 29.725907564939735\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 257.98346\n",
      "iteration 200 loss 253.64633\n",
      "iteration 300 loss 252.24524\n",
      "iteration 400 loss 251.28058\n",
      "iteration 500 loss 251.02167\n",
      "iteration 600 loss 250.90796\n",
      "iteration 700 loss 250.86154\n",
      "iteration 800 loss 250.82608\n",
      "iteration 900 loss 250.79532\n",
      "iteration 1000 loss 250.7694\n",
      "iteration 1100 loss 250.75702\n",
      "iteration 1200 loss 250.74622\n",
      "iteration 1300 loss 250.73648\n",
      "iteration 1400 loss 250.73294\n",
      "iteration 1500 loss 250.73245\n",
      "iteration 1600 loss 250.73218\n",
      "iteration 1700 loss 250.73201\n",
      "iteration 1800 loss 250.73186\n",
      "iteration 1900 loss 250.73175\n",
      "iteration 2000 loss 250.73167\n",
      "iteration 2100 loss 250.7316\n",
      "iteration 2200 loss 250.73155\n",
      "iteration 2300 loss 250.73148\n",
      "iteration 2400 loss 250.73145\n",
      "iteration 2500 loss 250.73137\n",
      "iteration 2600 loss 250.73135\n",
      "iteration 2700 loss 250.73131\n",
      "iteration 2800 loss 250.7313\n",
      "iteration 2900 loss 250.73122\n",
      "iteration 3000 loss 250.73117\n",
      "iteration 3100 loss 250.73117\n",
      "iteration 3200 loss 250.73114\n",
      "iteration 3300 loss 250.73111\n",
      "iteration 3400 loss 250.73105\n",
      "iteration 3500 loss 250.73103\n",
      "iteration 3600 loss 250.73099\n",
      "iteration 3700 loss 250.73099\n",
      "iteration 3800 loss 250.73096\n",
      "iteration 3900 loss 250.73096\n",
      "iteration 4000 loss 250.73091\n",
      "iteration 4100 loss 250.7309\n",
      "iteration 4200 loss 250.73085\n",
      "iteration 4300 loss 250.73083\n",
      "iteration 4400 loss 250.73082\n",
      "iteration 4500 loss 250.73079\n",
      "iteration 4600 loss 250.73077\n",
      "iteration 4700 loss 250.73074\n",
      "iteration 4800 loss 250.73071\n",
      "iteration 4900 loss 250.73067\n",
      "iteration 5000 loss 250.73068Train finished\n",
      "Selected features are: [15 48 54 58 63]\n",
      "Selected weights are: [0.9988223  0.9988223  0.8462774  0.5575852  0.60042566]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0016259160312559375, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 37.28462180663082\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 316.72156\n",
      "iteration 200 loss 310.76605\n",
      "iteration 300 loss 309.93555\n",
      "iteration 400 loss 309.6997\n",
      "iteration 500 loss 309.5149\n",
      "iteration 600 loss 309.4193\n",
      "iteration 700 loss 309.3612\n",
      "iteration 800 loss 309.34943\n",
      "iteration 900 loss 309.34952\n",
      "iteration 1000 loss 309.34955\n",
      "iteration 1100 loss 309.34952\n",
      "iteration 1200 loss 309.34952\n",
      "iteration 1300 loss 309.34952\n",
      "iteration 1400 loss 309.34955\n",
      "iteration 1500 loss 309.34955\n",
      "iteration 1600 loss 309.34955\n",
      "iteration 1700 loss 309.34955\n",
      "iteration 1800 loss 309.34955\n",
      "iteration 1900 loss 309.34955\n",
      "iteration 2000 loss 309.34955\n",
      "iteration 2100 loss 309.34955\n",
      "iteration 2200 loss 309.34955\n",
      "iteration 2300 loss 309.34955\n",
      "iteration 2400 loss 309.34955\n",
      "iteration 2500 loss 309.34955\n",
      "iteration 2600 loss 309.34955\n",
      "iteration 2700 loss 309.34955\n",
      "iteration 2800 loss 309.34955\n",
      "iteration 2900 loss 309.34955\n",
      "iteration 3000 loss 309.34955\n",
      "iteration 3100 loss 309.34955\n",
      "iteration 3200 loss 309.34955\n",
      "iteration 3300 loss 309.34955\n",
      "iteration 3400 loss 309.34955\n",
      "iteration 3500 loss 309.34955\n",
      "iteration 3600 loss 309.34955\n",
      "iteration 3700 loss 309.34955\n",
      "iteration 3800 loss 309.34955\n",
      "iteration 3900 loss 309.34955\n",
      "iteration 4000 loss 309.34955\n",
      "iteration 4100 loss 309.34955\n",
      "iteration 4200 loss 309.34955\n",
      "iteration 4300 loss 309.34955\n",
      "iteration 4400 loss 309.34955\n",
      "iteration 4500 loss 309.34955\n",
      "iteration 4600 loss 309.34955\n",
      "iteration 4700 loss 309.34955\n",
      "iteration 4800 loss 309.34955\n",
      "iteration 4900 loss 309.34955\n",
      "iteration 5000 loss 309.34955Train finished\n",
      "Selected features are: [14 15 40 48 58]\n",
      "Selected weights are: [0.46044043 0.99811566 0.99811566 0.99811566 0.99811566]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0005152405061788691, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 8.774939024204173\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 291.232\n",
      "iteration 200 loss 286.6029\n",
      "iteration 300 loss 285.27917\n",
      "iteration 400 loss 284.61035\n",
      "iteration 500 loss 284.208\n",
      "iteration 600 loss 283.8552\n",
      "iteration 700 loss 283.60715\n",
      "iteration 800 loss 283.51953\n",
      "iteration 900 loss 283.44156\n",
      "iteration 1000 loss 283.38312\n",
      "iteration 1100 loss 283.36218\n",
      "iteration 1200 loss 283.3518\n",
      "iteration 1300 loss 283.3458\n",
      "iteration 1400 loss 283.34177\n",
      "iteration 1500 loss 283.33853\n",
      "iteration 1600 loss 283.33588\n",
      "iteration 1700 loss 283.3335\n",
      "iteration 1800 loss 283.33136\n",
      "iteration 1900 loss 283.33044\n",
      "iteration 2000 loss 283.33008\n",
      "iteration 2100 loss 283.32993\n",
      "iteration 2200 loss 283.32983\n",
      "iteration 2300 loss 283.32974\n",
      "iteration 2400 loss 283.3297\n",
      "iteration 2500 loss 283.32968\n",
      "iteration 2600 loss 283.32968\n",
      "iteration 2700 loss 283.32965\n",
      "iteration 2800 loss 283.32965\n",
      "iteration 2900 loss 283.32962\n",
      "iteration 3000 loss 283.32965\n",
      "iteration 3100 loss 283.32968\n",
      "iteration 3200 loss 283.32968\n",
      "iteration 3300 loss 283.32965\n",
      "iteration 3400 loss 283.32965\n",
      "iteration 3500 loss 283.32965\n",
      "iteration 3600 loss 283.32965\n",
      "iteration 3700 loss 283.32962\n",
      "iteration 3800 loss 283.32962\n",
      "iteration 3900 loss 283.32965\n",
      "iteration 4000 loss 283.32965\n",
      "iteration 4100 loss 283.32965\n",
      "iteration 4200 loss 283.32962\n",
      "iteration 4300 loss 283.32965\n",
      "iteration 4400 loss 283.32962\n",
      "iteration 4500 loss 283.32965\n",
      "iteration 4600 loss 283.32965\n",
      "iteration 4700 loss 283.32965\n",
      "iteration 4800 loss 283.32965\n",
      "iteration 4900 loss 283.32965\n",
      "iteration 5000 loss 283.32962Train finished\n",
      "Selected features are: [46 49 52 56 70]\n",
      "Selected weights are: [0.8641393  0.51314455 0.9985249  0.9985249  0.9985249 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.007072512099631622, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 24.032051030394438\n",
      "\n",
      " 7 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 316.78366\n",
      "iteration 200 loss 313.7879\n",
      "iteration 300 loss 312.8449\n",
      "iteration 400 loss 312.18817\n",
      "iteration 500 loss 311.6485\n",
      "iteration 600 loss 310.28442\n",
      "iteration 700 loss 308.50058\n",
      "iteration 800 loss 307.7104\n",
      "iteration 900 loss 307.6564\n",
      "iteration 1000 loss 307.62213\n",
      "iteration 1100 loss 307.59436\n",
      "iteration 1200 loss 307.56924\n",
      "iteration 1300 loss 307.54565\n",
      "iteration 1400 loss 307.52692\n",
      "iteration 1500 loss 307.51477\n",
      "iteration 1600 loss 307.51154\n",
      "iteration 1700 loss 307.5094\n",
      "iteration 1800 loss 307.50787\n",
      "iteration 1900 loss 307.5068\n",
      "iteration 2000 loss 307.50598\n",
      "iteration 2100 loss 307.50537\n",
      "iteration 2200 loss 307.50476\n",
      "iteration 2300 loss 307.50433\n",
      "iteration 2400 loss 307.50397\n",
      "iteration 2500 loss 307.5036\n",
      "iteration 2600 loss 307.50336\n",
      "iteration 2700 loss 307.50308\n",
      "iteration 2800 loss 307.50287\n",
      "iteration 2900 loss 307.5027\n",
      "iteration 3000 loss 307.5025\n",
      "iteration 3100 loss 307.50232\n",
      "iteration 3200 loss 307.5022\n",
      "iteration 3300 loss 307.502\n",
      "iteration 3400 loss 307.5019\n",
      "iteration 3500 loss 307.50177\n",
      "iteration 3600 loss 307.5016\n",
      "iteration 3700 loss 307.50153\n",
      "iteration 3800 loss 307.5014\n",
      "iteration 3900 loss 307.50128\n",
      "iteration 4000 loss 307.50113\n",
      "iteration 4100 loss 307.50104\n",
      "iteration 4200 loss 307.50092\n",
      "iteration 4300 loss 307.5008\n",
      "iteration 4400 loss 307.5007\n",
      "iteration 4500 loss 307.50055\n",
      "iteration 4600 loss 307.50043\n",
      "iteration 4700 loss 307.50037\n",
      "iteration 4800 loss 307.50027\n",
      "iteration 4900 loss 307.5001\n",
      "iteration 5000 loss 307.5Train finished\n",
      "Selected features are: [15 33 51 54 56]\n",
      "Selected weights are: [0.9192257 0.9991112 0.9991112 0.4120285 0.9991112]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.002380863812750397, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 13.122057560889342\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 272.08173\n",
      "iteration 200 loss 266.00757\n",
      "iteration 300 loss 265.55637\n",
      "iteration 400 loss 265.44727\n",
      "iteration 500 loss 265.3777\n",
      "iteration 600 loss 265.34564\n",
      "iteration 700 loss 265.31943\n",
      "iteration 800 loss 265.29706\n",
      "iteration 900 loss 265.27765\n",
      "iteration 1000 loss 265.26044\n",
      "iteration 1100 loss 265.25458\n",
      "iteration 1200 loss 265.25278\n",
      "iteration 1300 loss 265.2518\n",
      "iteration 1400 loss 265.25116\n",
      "iteration 1500 loss 265.25085\n",
      "iteration 1600 loss 265.25067\n",
      "iteration 1700 loss 265.25055\n",
      "iteration 1800 loss 265.25043\n",
      "iteration 1900 loss 265.25037\n",
      "iteration 2000 loss 265.25037\n",
      "iteration 2100 loss 265.25037\n",
      "iteration 2200 loss 265.25037\n",
      "iteration 2300 loss 265.2503\n",
      "iteration 2400 loss 265.2503\n",
      "iteration 2500 loss 265.2503\n",
      "iteration 2600 loss 265.25034\n",
      "iteration 2700 loss 265.2503\n",
      "iteration 2800 loss 265.2503\n",
      "iteration 2900 loss 265.25034\n",
      "iteration 3000 loss 265.25034\n",
      "iteration 3100 loss 265.2503\n",
      "iteration 3200 loss 265.25034\n",
      "iteration 3300 loss 265.2503\n",
      "iteration 3400 loss 265.25034\n",
      "iteration 3500 loss 265.25034\n",
      "iteration 3600 loss 265.2503\n",
      "iteration 3700 loss 265.2503\n",
      "iteration 3800 loss 265.2503\n",
      "iteration 3900 loss 265.25034\n",
      "iteration 4000 loss 265.25034\n",
      "iteration 4100 loss 265.2503\n",
      "iteration 4200 loss 265.2503\n",
      "iteration 4300 loss 265.25037\n",
      "iteration 4400 loss 265.2503\n",
      "iteration 4500 loss 265.25034\n",
      "iteration 4600 loss 265.2503\n",
      "iteration 4700 loss 265.2503\n",
      "iteration 4800 loss 265.25037\n",
      "iteration 4900 loss 265.25037\n",
      "iteration 5000 loss 265.25034Train finished\n",
      "Selected features are: [14 15 40 48 58]\n",
      "Selected weights are: [0.45346823 0.9964383  0.99814075 0.99814075 0.7958167 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.000614525637914072, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 27.90086004345076\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 262.7686\n",
      "iteration 200 loss 253.88039\n",
      "iteration 300 loss 250.63647\n",
      "iteration 400 loss 250.43211\n",
      "iteration 500 loss 250.3804\n",
      "iteration 600 loss 250.33807\n",
      "iteration 700 loss 250.30788\n",
      "iteration 800 loss 250.2815\n",
      "iteration 900 loss 250.25842\n",
      "iteration 1000 loss 250.23816\n",
      "iteration 1100 loss 250.22034\n",
      "iteration 1200 loss 250.20456\n",
      "iteration 1300 loss 250.19052\n",
      "iteration 1400 loss 250.18143\n",
      "iteration 1500 loss 250.17549\n",
      "iteration 1600 loss 250.17053\n",
      "iteration 1700 loss 250.16635\n",
      "iteration 1800 loss 250.16286\n",
      "iteration 1900 loss 250.15987\n",
      "iteration 2000 loss 250.1574\n",
      "iteration 2100 loss 250.15627\n",
      "iteration 2200 loss 250.15549\n",
      "iteration 2300 loss 250.1549\n",
      "iteration 2400 loss 250.1544\n",
      "iteration 2500 loss 250.15404\n",
      "iteration 2600 loss 250.15375\n",
      "iteration 2700 loss 250.15349\n",
      "iteration 2800 loss 250.15329\n",
      "iteration 2900 loss 250.15315\n",
      "iteration 3000 loss 250.15302\n",
      "iteration 3100 loss 250.15294\n",
      "iteration 3200 loss 250.15286\n",
      "iteration 3300 loss 250.1528\n",
      "iteration 3400 loss 250.15276\n",
      "iteration 3500 loss 250.15274\n",
      "iteration 3600 loss 250.15271\n",
      "iteration 3700 loss 250.1527\n",
      "iteration 3800 loss 250.1527\n",
      "iteration 3900 loss 250.15268\n",
      "iteration 4000 loss 250.15268\n",
      "iteration 4100 loss 250.15271\n",
      "iteration 4200 loss 250.15271\n",
      "iteration 4300 loss 250.15268\n",
      "iteration 4400 loss 250.1527\n",
      "iteration 4500 loss 250.15271\n",
      "iteration 4600 loss 250.15271\n",
      "iteration 4700 loss 250.1527\n",
      "iteration 4800 loss 250.15268\n",
      "iteration 4900 loss 250.15268\n",
      "iteration 5000 loss 250.15268Train finished\n",
      "Selected features are: [15 40 48 51 58]\n",
      "Selected weights are: [0.9984869 0.9984869 0.6162011 0.9984869 0.9984869]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0010589145094509758, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 33.60063441402019\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 262.51068\n",
      "iteration 200 loss 257.4431\n",
      "iteration 300 loss 256.5608\n",
      "iteration 400 loss 256.25653\n",
      "iteration 500 loss 256.10135\n",
      "iteration 600 loss 256.03088\n",
      "iteration 700 loss 255.99284\n",
      "iteration 800 loss 255.96378\n",
      "iteration 900 loss 255.9408\n",
      "iteration 1000 loss 255.92204\n",
      "iteration 1100 loss 255.91208\n",
      "iteration 1200 loss 255.9087\n",
      "iteration 1300 loss 255.90648\n",
      "iteration 1400 loss 255.9049\n",
      "iteration 1500 loss 255.90378\n",
      "iteration 1600 loss 255.90303\n",
      "iteration 1700 loss 255.90245\n",
      "iteration 1800 loss 255.90211\n",
      "iteration 1900 loss 255.90228\n",
      "iteration 2000 loss 255.90239\n",
      "iteration 2100 loss 255.90242\n",
      "iteration 2200 loss 255.9024\n",
      "iteration 2300 loss 255.90244\n",
      "iteration 2400 loss 255.9024\n",
      "iteration 2500 loss 255.9024\n",
      "iteration 2600 loss 255.9024\n",
      "iteration 2700 loss 255.9024\n",
      "iteration 2800 loss 255.9024\n",
      "iteration 2900 loss 255.9024\n",
      "iteration 3000 loss 255.9024\n",
      "iteration 3100 loss 255.9024\n",
      "iteration 3200 loss 255.9024\n",
      "iteration 3300 loss 255.9024\n",
      "iteration 3400 loss 255.9024\n",
      "iteration 3500 loss 255.9024\n",
      "iteration 3600 loss 255.9024\n",
      "iteration 3700 loss 255.9024\n",
      "iteration 3800 loss 255.9024\n",
      "iteration 3900 loss 255.9024\n",
      "iteration 4000 loss 255.9024\n",
      "iteration 4100 loss 255.9024\n",
      "iteration 4200 loss 255.9024\n",
      "iteration 4300 loss 255.9024\n",
      "iteration 4400 loss 255.9024\n",
      "iteration 4500 loss 255.9024\n",
      "iteration 4600 loss 255.9024\n",
      "iteration 4700 loss 255.9024\n",
      "iteration 4800 loss 255.9024\n",
      "iteration 4900 loss 255.9024\n",
      "iteration 5000 loss 255.9024Train finished\n",
      "Selected features are: [14 15 40 48 58]\n",
      "Selected weights are: [0.6037867 0.9989642 0.9989642 0.9989642 0.9989642]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0005480327174455713, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 28.93318269886745\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 254.40222\n",
      "iteration 200 loss 249.7739\n",
      "iteration 300 loss 245.49127\n",
      "iteration 400 loss 243.43448\n",
      "iteration 500 loss 241.47032\n",
      "iteration 600 loss 241.06879\n",
      "iteration 700 loss 240.98563\n",
      "iteration 800 loss 240.94162\n",
      "iteration 900 loss 240.89899\n",
      "iteration 1000 loss 240.86748\n",
      "iteration 1100 loss 240.84421\n",
      "iteration 1200 loss 240.82632\n",
      "iteration 1300 loss 240.8162\n",
      "iteration 1400 loss 240.80928\n",
      "iteration 1500 loss 240.80278\n",
      "iteration 1600 loss 240.79669\n",
      "iteration 1700 loss 240.79105\n",
      "iteration 1800 loss 240.78577\n",
      "iteration 1900 loss 240.78087\n",
      "iteration 2000 loss 240.77625\n",
      "iteration 2100 loss 240.77196\n",
      "iteration 2200 loss 240.76793\n",
      "iteration 2300 loss 240.76416\n",
      "iteration 2400 loss 240.76065\n",
      "iteration 2500 loss 240.75739\n",
      "iteration 2600 loss 240.75836\n",
      "iteration 2700 loss 240.75836\n",
      "iteration 2800 loss 240.75839\n",
      "iteration 2900 loss 240.75839\n",
      "iteration 3000 loss 240.75839\n",
      "iteration 3100 loss 240.75839\n",
      "iteration 3200 loss 240.75839\n",
      "iteration 3300 loss 240.75839\n",
      "iteration 3400 loss 240.75839\n",
      "iteration 3500 loss 240.75839\n",
      "iteration 3600 loss 240.75839\n",
      "iteration 3700 loss 240.75839\n",
      "iteration 3800 loss 240.75839\n",
      "iteration 3900 loss 240.75839\n",
      "iteration 4000 loss 240.75839\n",
      "iteration 4100 loss 240.75839\n",
      "iteration 4200 loss 240.75839\n",
      "iteration 4300 loss 240.75839\n",
      "iteration 4400 loss 240.75839\n",
      "iteration 4500 loss 240.75839\n",
      "iteration 4600 loss 240.75839\n",
      "iteration 4700 loss 240.75839\n",
      "iteration 4800 loss 240.75839\n",
      "iteration 4900 loss 240.75839\n",
      "iteration 5000 loss 240.75839Train finished\n",
      "Selected features are: [ 8 14 15 33 48]\n",
      "Selected weights are: [0.99884063 0.99884063 0.99884063 0.74932164 0.99884063]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0007540376039438232, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 41.76872919273265\n",
      "\n",
      " 8 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 274.16785\n",
      "iteration 200 loss 267.31702\n",
      "iteration 300 loss 266.51892\n",
      "iteration 400 loss 266.20178\n",
      "iteration 500 loss 265.998\n",
      "iteration 600 loss 265.8978\n",
      "iteration 700 loss 265.89594\n",
      "iteration 800 loss 265.8944\n",
      "iteration 900 loss 265.893\n",
      "iteration 1000 loss 265.89178\n",
      "iteration 1100 loss 265.89062\n",
      "iteration 1200 loss 265.88962\n",
      "iteration 1300 loss 265.88867\n",
      "iteration 1400 loss 265.88776\n",
      "iteration 1500 loss 265.88696\n",
      "iteration 1600 loss 265.88623\n",
      "iteration 1700 loss 265.8856\n",
      "iteration 1800 loss 265.88577\n",
      "iteration 1900 loss 265.8858\n",
      "iteration 2000 loss 265.88583\n",
      "iteration 2100 loss 265.88586\n",
      "iteration 2200 loss 265.88583\n",
      "iteration 2300 loss 265.88583\n",
      "iteration 2400 loss 265.8858\n",
      "iteration 2500 loss 265.8858\n",
      "iteration 2600 loss 265.8858\n",
      "iteration 2700 loss 265.8858\n",
      "iteration 2800 loss 265.88583\n",
      "iteration 2900 loss 265.88583\n",
      "iteration 3000 loss 265.8858\n",
      "iteration 3100 loss 265.8858\n",
      "iteration 3200 loss 265.8858\n",
      "iteration 3300 loss 265.88583\n",
      "iteration 3400 loss 265.88583\n",
      "iteration 3500 loss 265.8858\n",
      "iteration 3600 loss 265.8858\n",
      "iteration 3700 loss 265.8858\n",
      "iteration 3800 loss 265.8858\n",
      "iteration 3900 loss 265.88583\n",
      "iteration 4000 loss 265.88583\n",
      "iteration 4100 loss 265.8858\n",
      "iteration 4200 loss 265.8858\n",
      "iteration 4300 loss 265.8858\n",
      "iteration 4400 loss 265.88583\n",
      "iteration 4500 loss 265.88583\n",
      "iteration 4600 loss 265.8858\n",
      "iteration 4700 loss 265.8858\n",
      "iteration 4800 loss 265.8858\n",
      "iteration 4900 loss 265.8858\n",
      "iteration 5000 loss 265.88583Train finished\n",
      "Selected features are: [14 15 40 48 58]\n",
      "Selected weights are: [0.5242404 0.9985314 0.9985314 0.9985314 0.9985314]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0006280458296195092, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 26.51498192638987\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 285.45673\n",
      "iteration 200 loss 278.9423\n",
      "iteration 300 loss 278.05823\n",
      "iteration 400 loss 277.88907\n",
      "iteration 500 loss 277.76236\n",
      "iteration 600 loss 277.68518\n",
      "iteration 700 loss 277.6537\n",
      "iteration 800 loss 277.63266\n",
      "iteration 900 loss 277.61456\n",
      "iteration 1000 loss 277.59888\n",
      "iteration 1100 loss 277.58508\n",
      "iteration 1200 loss 277.5794\n",
      "iteration 1300 loss 277.5743\n",
      "iteration 1400 loss 277.56985\n",
      "iteration 1500 loss 277.56586\n",
      "iteration 1600 loss 277.56226\n",
      "iteration 1700 loss 277.55893\n",
      "iteration 1800 loss 277.55597\n",
      "iteration 1900 loss 277.55316\n",
      "iteration 2000 loss 277.5506\n",
      "iteration 2100 loss 277.54825\n",
      "iteration 2200 loss 277.54602\n",
      "iteration 2300 loss 277.544\n",
      "iteration 2400 loss 277.5421\n",
      "iteration 2500 loss 277.54037\n",
      "iteration 2600 loss 277.53876\n",
      "iteration 2700 loss 277.53726\n",
      "iteration 2800 loss 277.53595\n",
      "iteration 2900 loss 277.5346\n",
      "iteration 3000 loss 277.53345\n",
      "iteration 3100 loss 277.53232\n",
      "iteration 3200 loss 277.53128\n",
      "iteration 3300 loss 277.53033\n",
      "iteration 3400 loss 277.52942\n",
      "iteration 3500 loss 277.5286\n",
      "iteration 3600 loss 277.5279\n",
      "iteration 3700 loss 277.52716\n",
      "iteration 3800 loss 277.5265\n",
      "iteration 3900 loss 277.52576\n",
      "iteration 4000 loss 277.52533\n",
      "iteration 4100 loss 277.52515\n",
      "iteration 4200 loss 277.525\n",
      "iteration 4300 loss 277.5249\n",
      "iteration 4400 loss 277.52484\n",
      "iteration 4500 loss 277.52475\n",
      "iteration 4600 loss 277.5247\n",
      "iteration 4700 loss 277.52466\n",
      "iteration 4800 loss 277.5246\n",
      "iteration 4900 loss 277.5246\n",
      "iteration 5000 loss 277.52454Train finished\n",
      "Selected features are: [14 15 40 48 58]\n",
      "Selected weights are: [0.46668276 0.9989004  0.9989004  0.9989004  0.9989004 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0005539176691795943, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 23.4332661836755\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 242.92465\n",
      "iteration 200 loss 231.82541\n",
      "iteration 300 loss 229.74336\n",
      "iteration 400 loss 229.01038\n",
      "iteration 500 loss 228.71979\n",
      "iteration 600 loss 228.50836\n",
      "iteration 700 loss 228.332\n",
      "iteration 800 loss 228.19466\n",
      "iteration 900 loss 228.1598\n",
      "iteration 1000 loss 228.13196\n",
      "iteration 1100 loss 228.10672\n",
      "iteration 1200 loss 228.08377\n",
      "iteration 1300 loss 228.06277\n",
      "iteration 1400 loss 228.04344\n",
      "iteration 1500 loss 228.02559\n",
      "iteration 1600 loss 228.00894\n",
      "iteration 1700 loss 227.99388\n",
      "iteration 1800 loss 227.98949\n",
      "iteration 1900 loss 227.98865\n",
      "iteration 2000 loss 227.9881\n",
      "iteration 2100 loss 227.98767\n",
      "iteration 2200 loss 227.98726\n",
      "iteration 2300 loss 227.9869\n",
      "iteration 2400 loss 227.9866\n",
      "iteration 2500 loss 227.98627\n",
      "iteration 2600 loss 227.98602\n",
      "iteration 2700 loss 227.98575\n",
      "iteration 2800 loss 227.98549\n",
      "iteration 2900 loss 227.98528\n",
      "iteration 3000 loss 227.98505\n",
      "iteration 3100 loss 227.98485\n",
      "iteration 3200 loss 227.98466\n",
      "iteration 3300 loss 227.9845\n",
      "iteration 3400 loss 227.9843\n",
      "iteration 3500 loss 227.98409\n",
      "iteration 3600 loss 227.98395\n",
      "iteration 3700 loss 227.98376\n",
      "iteration 3800 loss 227.98363\n",
      "iteration 3900 loss 227.98343\n",
      "iteration 4000 loss 227.9833\n",
      "iteration 4100 loss 227.98318\n",
      "iteration 4200 loss 227.98303\n",
      "iteration 4300 loss 227.98286\n",
      "iteration 4400 loss 227.9827\n",
      "iteration 4500 loss 227.98256\n",
      "iteration 4600 loss 227.98242\n",
      "iteration 4700 loss 227.98228\n",
      "iteration 4800 loss 227.98213\n",
      "iteration 4900 loss 227.982\n",
      "iteration 5000 loss 227.98184Train finished\n",
      "Selected features are: [ 7 46 49 56 70]\n",
      "Selected weights are: [0.28407288 0.942168   0.99848616 0.99848616 0.99848616]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00010036963121234381, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 45.42018123456496\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 279.3028\n",
      "iteration 200 loss 270.64856\n",
      "iteration 300 loss 268.04358\n",
      "iteration 400 loss 266.7312\n",
      "iteration 500 loss 266.36374\n",
      "iteration 600 loss 266.0932\n",
      "iteration 700 loss 265.91833\n",
      "iteration 800 loss 265.8224\n",
      "iteration 900 loss 265.79608\n",
      "iteration 1000 loss 265.79205\n",
      "iteration 1100 loss 265.79034\n",
      "iteration 1200 loss 265.7895\n",
      "iteration 1300 loss 265.78906\n",
      "iteration 1400 loss 265.78882\n",
      "iteration 1500 loss 265.78876\n",
      "iteration 1600 loss 265.7887\n",
      "iteration 1700 loss 265.7887\n",
      "iteration 1800 loss 265.78867\n",
      "iteration 1900 loss 265.7887\n",
      "iteration 2000 loss 265.7887\n",
      "iteration 2100 loss 265.7887\n",
      "iteration 2200 loss 265.7887\n",
      "iteration 2300 loss 265.78867\n",
      "iteration 2400 loss 265.78867\n",
      "iteration 2500 loss 265.78867\n",
      "iteration 2600 loss 265.78867\n",
      "iteration 2700 loss 265.7887\n",
      "iteration 2800 loss 265.78867\n",
      "iteration 2900 loss 265.7887\n",
      "iteration 3000 loss 265.78867\n",
      "iteration 3100 loss 265.7887\n",
      "iteration 3200 loss 265.7887\n",
      "iteration 3300 loss 265.7887\n",
      "iteration 3400 loss 265.7887\n",
      "iteration 3500 loss 265.78867\n",
      "iteration 3600 loss 265.7887\n",
      "iteration 3700 loss 265.7887\n",
      "iteration 3800 loss 265.7887\n",
      "iteration 3900 loss 265.7887\n",
      "iteration 4000 loss 265.7887\n",
      "iteration 4100 loss 265.7887\n",
      "iteration 4200 loss 265.7887\n",
      "iteration 4300 loss 265.7887\n",
      "iteration 4400 loss 265.7887\n",
      "iteration 4500 loss 265.7887\n",
      "iteration 4600 loss 265.78867\n",
      "iteration 4700 loss 265.7887\n",
      "iteration 4800 loss 265.7887\n",
      "iteration 4900 loss 265.7887\n",
      "iteration 5000 loss 265.78867Train finished\n",
      "Selected features are: [15 40 48 54 58]\n",
      "Selected weights are: [0.99789536 0.7377032  0.99789536 0.99789536 0.9180067 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0011832981741326212, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 31.27854102256292\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 305.4592\n",
      "iteration 200 loss 295.4804\n",
      "iteration 300 loss 292.7671\n",
      "iteration 400 loss 292.4322\n",
      "iteration 500 loss 292.27692\n",
      "iteration 600 loss 292.1516\n",
      "iteration 700 loss 292.04453\n",
      "iteration 800 loss 291.963\n",
      "iteration 900 loss 291.9101\n",
      "iteration 1000 loss 291.90558\n",
      "iteration 1100 loss 291.9042\n",
      "iteration 1200 loss 291.90393\n",
      "iteration 1300 loss 291.9037\n",
      "iteration 1400 loss 291.9035\n",
      "iteration 1500 loss 291.9034\n",
      "iteration 1600 loss 291.90335\n",
      "iteration 1700 loss 291.90335\n",
      "iteration 1800 loss 291.90332\n",
      "iteration 1900 loss 291.90332\n",
      "iteration 2000 loss 291.9033\n",
      "iteration 2100 loss 291.90332\n",
      "iteration 2200 loss 291.9033\n",
      "iteration 2300 loss 291.90332\n",
      "iteration 2400 loss 291.90332\n",
      "iteration 2500 loss 291.90332\n",
      "iteration 2600 loss 291.90332\n",
      "iteration 2700 loss 291.90332\n",
      "iteration 2800 loss 291.90332\n",
      "iteration 2900 loss 291.90332\n",
      "iteration 3000 loss 291.90332\n",
      "iteration 3100 loss 291.90332\n",
      "iteration 3200 loss 291.90332\n",
      "iteration 3300 loss 291.90332\n",
      "iteration 3400 loss 291.90332\n",
      "iteration 3500 loss 291.90332\n",
      "iteration 3600 loss 291.90332\n",
      "iteration 3700 loss 291.90332\n",
      "iteration 3800 loss 291.90335\n",
      "iteration 3900 loss 291.90332\n",
      "iteration 4000 loss 291.90332\n",
      "iteration 4100 loss 291.90332\n",
      "iteration 4200 loss 291.90332\n",
      "iteration 4300 loss 291.90332\n",
      "iteration 4400 loss 291.90332\n",
      "iteration 4500 loss 291.90332\n",
      "iteration 4600 loss 291.90332\n",
      "iteration 4700 loss 291.90332\n",
      "iteration 4800 loss 291.90332\n",
      "iteration 4900 loss 291.90332\n",
      "iteration 5000 loss 291.90332Train finished\n",
      "Selected features are: [14 15 40 48 58]\n",
      "Selected weights are: [0.998574   0.998574   0.97026354 0.998574   0.71605545]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0007298225851989642, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 17.972137941739614\n",
      "\n",
      " 9 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 282.88135\n",
      "iteration 200 loss 279.5443\n",
      "iteration 300 loss 278.89557\n",
      "iteration 400 loss 278.6216\n",
      "iteration 500 loss 278.4412\n",
      "iteration 600 loss 278.31036\n",
      "iteration 700 loss 278.23483\n",
      "iteration 800 loss 278.19846\n",
      "iteration 900 loss 278.17075\n",
      "iteration 1000 loss 278.1482\n",
      "iteration 1100 loss 278.13867\n",
      "iteration 1200 loss 278.1333\n",
      "iteration 1300 loss 278.12936\n",
      "iteration 1400 loss 278.12628\n",
      "iteration 1500 loss 278.1236\n",
      "iteration 1600 loss 278.12134\n",
      "iteration 1700 loss 278.11935\n",
      "iteration 1800 loss 278.1175\n",
      "iteration 1900 loss 278.11597\n",
      "iteration 2000 loss 278.11453\n",
      "iteration 2100 loss 278.1133\n",
      "iteration 2200 loss 278.1122\n",
      "iteration 2300 loss 278.11118\n",
      "iteration 2400 loss 278.11\n",
      "iteration 2500 loss 278.1098\n",
      "iteration 2600 loss 278.10974\n",
      "iteration 2700 loss 278.10974\n",
      "iteration 2800 loss 278.10968\n",
      "iteration 2900 loss 278.10968\n",
      "iteration 3000 loss 278.10968\n",
      "iteration 3100 loss 278.10968\n",
      "iteration 3200 loss 278.10965\n",
      "iteration 3300 loss 278.10965\n",
      "iteration 3400 loss 278.10962\n",
      "iteration 3500 loss 278.10965\n",
      "iteration 3600 loss 278.10962\n",
      "iteration 3700 loss 278.10965\n",
      "iteration 3800 loss 278.10962\n",
      "iteration 3900 loss 278.10968\n",
      "iteration 4000 loss 278.10962\n",
      "iteration 4100 loss 278.10962\n",
      "iteration 4200 loss 278.10962\n",
      "iteration 4300 loss 278.10962\n",
      "iteration 4400 loss 278.10965\n",
      "iteration 4500 loss 278.10962\n",
      "iteration 4600 loss 278.10962\n",
      "iteration 4700 loss 278.10962\n",
      "iteration 4800 loss 278.10962\n",
      "iteration 4900 loss 278.10962\n",
      "iteration 5000 loss 278.10962Train finished\n",
      "Selected features are: [14 15 48 58 63]\n",
      "Selected weights are: [0.54045886 0.9986853  0.9986853  0.72607344 0.9986853 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0007543040985512056, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 20.80201321362135\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 229.2255\n",
      "iteration 200 loss 226.96916\n",
      "iteration 300 loss 224.1139\n",
      "iteration 400 loss 222.50586\n",
      "iteration 500 loss 221.91885\n",
      "iteration 600 loss 221.63573\n",
      "iteration 700 loss 221.51024\n",
      "iteration 800 loss 221.43428\n",
      "iteration 900 loss 221.38525\n",
      "iteration 1000 loss 221.35547\n",
      "iteration 1100 loss 221.32703\n",
      "iteration 1200 loss 221.29913\n",
      "iteration 1300 loss 221.2711\n",
      "iteration 1400 loss 221.2427\n",
      "iteration 1500 loss 221.21375\n",
      "iteration 1600 loss 221.18419\n",
      "iteration 1700 loss 221.16083\n",
      "iteration 1800 loss 221.13777\n",
      "iteration 1900 loss 221.12048\n",
      "iteration 2000 loss 221.11285\n",
      "iteration 2100 loss 221.10535\n",
      "iteration 2200 loss 221.10083\n",
      "iteration 2300 loss 221.09999\n",
      "iteration 2400 loss 221.09924\n",
      "iteration 2500 loss 221.09846\n",
      "iteration 2600 loss 221.09778\n",
      "iteration 2700 loss 221.0971\n",
      "iteration 2800 loss 221.09647\n",
      "iteration 2900 loss 221.09583\n",
      "iteration 3000 loss 221.09521\n",
      "iteration 3100 loss 221.0946\n",
      "iteration 3200 loss 221.09402\n",
      "iteration 3300 loss 221.09348\n",
      "iteration 3400 loss 221.09293\n",
      "iteration 3500 loss 221.09245\n",
      "iteration 3600 loss 221.09193\n",
      "iteration 3700 loss 221.09145\n",
      "iteration 3800 loss 221.09097\n",
      "iteration 3900 loss 221.09053\n",
      "iteration 4000 loss 221.09006\n",
      "iteration 4100 loss 221.08966\n",
      "iteration 4200 loss 221.08923\n",
      "iteration 4300 loss 221.08885\n",
      "iteration 4400 loss 221.08847\n",
      "iteration 4500 loss 221.08812\n",
      "iteration 4600 loss 221.08774\n",
      "iteration 4700 loss 221.08739\n",
      "iteration 4800 loss 221.08704\n",
      "iteration 4900 loss 221.08673\n",
      "iteration 5000 loss 221.08643Train finished\n",
      "Selected features are: [11 46 49 51 52]\n",
      "Selected weights are: [0.67691046 0.99931395 0.99931395 0.99931395 0.57972884]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0018382116419450244, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 50.96206049150938\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 255.08157\n",
      "iteration 200 loss 250.9707\n",
      "iteration 300 loss 250.15108\n",
      "iteration 400 loss 249.9557\n",
      "iteration 500 loss 249.7821\n",
      "iteration 600 loss 249.59535\n",
      "iteration 700 loss 249.39592\n",
      "iteration 800 loss 249.26813\n",
      "iteration 900 loss 249.24815\n",
      "iteration 1000 loss 249.2291\n",
      "iteration 1100 loss 249.21083\n",
      "iteration 1200 loss 249.19412\n",
      "iteration 1300 loss 249.18216\n",
      "iteration 1400 loss 249.17082\n",
      "iteration 1500 loss 249.16006\n",
      "iteration 1600 loss 249.15228\n",
      "iteration 1700 loss 249.14847\n",
      "iteration 1800 loss 249.14543\n",
      "iteration 1900 loss 249.14302\n",
      "iteration 2000 loss 249.14114\n",
      "iteration 2100 loss 249.13968\n",
      "iteration 2200 loss 249.13852\n",
      "iteration 2300 loss 249.13763\n",
      "iteration 2400 loss 249.1369\n",
      "iteration 2500 loss 249.13641\n",
      "iteration 2600 loss 249.13597\n",
      "iteration 2700 loss 249.13564\n",
      "iteration 2800 loss 249.13538\n",
      "iteration 2900 loss 249.13524\n",
      "iteration 3000 loss 249.13506\n",
      "iteration 3100 loss 249.13492\n",
      "iteration 3200 loss 249.13483\n",
      "iteration 3300 loss 249.13477\n",
      "iteration 3400 loss 249.13474\n",
      "iteration 3500 loss 249.1347\n",
      "iteration 3600 loss 249.13464\n",
      "iteration 3700 loss 249.13463\n",
      "iteration 3800 loss 249.1346\n",
      "iteration 3900 loss 249.13464\n",
      "iteration 4000 loss 249.13461\n",
      "iteration 4100 loss 249.13458\n",
      "iteration 4200 loss 249.13463\n",
      "iteration 4300 loss 249.1346\n",
      "iteration 4400 loss 249.1346\n",
      "iteration 4500 loss 249.13461\n",
      "iteration 4600 loss 249.13458\n",
      "iteration 4700 loss 249.1346\n",
      "iteration 4800 loss 249.13455\n",
      "iteration 4900 loss 249.13458\n",
      "iteration 5000 loss 249.13458Train finished\n",
      "Selected features are: [11 26 46 56 70]\n",
      "Selected weights are: [0.41272524 0.9985315  0.9252073  0.9985315  0.86027557]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.002553060906492958, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 36.75989535945464\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 299.5951\n",
      "iteration 200 loss 294.47235\n",
      "iteration 300 loss 293.47733\n",
      "iteration 400 loss 293.00665\n",
      "iteration 500 loss 292.80096\n",
      "iteration 600 loss 292.77734\n",
      "iteration 700 loss 292.7669\n",
      "iteration 800 loss 292.7624\n",
      "iteration 900 loss 292.76263\n",
      "iteration 1000 loss 292.76266\n",
      "iteration 1100 loss 292.7627\n",
      "iteration 1200 loss 292.7627\n",
      "iteration 1300 loss 292.7627\n",
      "iteration 1400 loss 292.7627\n",
      "iteration 1500 loss 292.7627\n",
      "iteration 1600 loss 292.7627\n",
      "iteration 1700 loss 292.7627\n",
      "iteration 1800 loss 292.7627\n",
      "iteration 1900 loss 292.7627\n",
      "iteration 2000 loss 292.7627\n",
      "iteration 2100 loss 292.7627\n",
      "iteration 2200 loss 292.7627\n",
      "iteration 2300 loss 292.7627\n",
      "iteration 2400 loss 292.7627\n",
      "iteration 2500 loss 292.7627\n",
      "iteration 2600 loss 292.7627\n",
      "iteration 2700 loss 292.7627\n",
      "iteration 2800 loss 292.7627\n",
      "iteration 2900 loss 292.7627\n",
      "iteration 3000 loss 292.7627\n",
      "iteration 3100 loss 292.7627\n",
      "iteration 3200 loss 292.7627\n",
      "iteration 3300 loss 292.7627\n",
      "iteration 3400 loss 292.7627\n",
      "iteration 3500 loss 292.7627\n",
      "iteration 3600 loss 292.7627\n",
      "iteration 3700 loss 292.7627\n",
      "iteration 3800 loss 292.7627\n",
      "iteration 3900 loss 292.7627\n",
      "iteration 4000 loss 292.7627\n",
      "iteration 4100 loss 292.7627\n",
      "iteration 4200 loss 292.7627\n",
      "iteration 4300 loss 292.7627\n",
      "iteration 4400 loss 292.7627\n",
      "iteration 4500 loss 292.7627\n",
      "iteration 4600 loss 292.7627\n",
      "iteration 4700 loss 292.7627\n",
      "iteration 4800 loss 292.7627\n",
      "iteration 4900 loss 292.7627\n",
      "iteration 5000 loss 292.7627Train finished\n",
      "Selected features are: [14 15 40 48 58]\n",
      "Selected weights are: [0.44242185 0.9985018  0.9985018  0.9985018  0.9985018 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0006996461518071823, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 16.083097678948594\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 293.49487\n",
      "iteration 200 loss 287.54984\n",
      "iteration 300 loss 285.92752\n",
      "iteration 400 loss 285.66336\n",
      "iteration 500 loss 285.5661\n",
      "iteration 600 loss 285.5393\n",
      "iteration 700 loss 285.52087\n",
      "iteration 800 loss 285.50677\n",
      "iteration 900 loss 285.49557\n",
      "iteration 1000 loss 285.4864\n",
      "iteration 1100 loss 285.47873\n",
      "iteration 1200 loss 285.47223\n",
      "iteration 1300 loss 285.46664\n",
      "iteration 1400 loss 285.46185\n",
      "iteration 1500 loss 285.45767\n",
      "iteration 1600 loss 285.45447\n",
      "iteration 1700 loss 285.4524\n",
      "iteration 1800 loss 285.4505\n",
      "iteration 1900 loss 285.44873\n",
      "iteration 2000 loss 285.44708\n",
      "iteration 2100 loss 285.44556\n",
      "iteration 2200 loss 285.4441\n",
      "iteration 2300 loss 285.4427\n",
      "iteration 2400 loss 285.4414\n",
      "iteration 2500 loss 285.44012\n",
      "iteration 2600 loss 285.43896\n",
      "iteration 2700 loss 285.4378\n",
      "iteration 2800 loss 285.43674\n",
      "iteration 2900 loss 285.43567\n",
      "iteration 3000 loss 285.43475\n",
      "iteration 3100 loss 285.43375\n",
      "iteration 3200 loss 285.43286\n",
      "iteration 3300 loss 285.43195\n",
      "iteration 3400 loss 285.4312\n",
      "iteration 3500 loss 285.43036\n",
      "iteration 3600 loss 285.42963\n",
      "iteration 3700 loss 285.42896\n",
      "iteration 3800 loss 285.42825\n",
      "iteration 3900 loss 285.4276\n",
      "iteration 4000 loss 285.427\n",
      "iteration 4100 loss 285.42636\n",
      "iteration 4200 loss 285.42636\n",
      "iteration 4300 loss 285.42633\n",
      "iteration 4400 loss 285.42633\n",
      "iteration 4500 loss 285.42636\n",
      "iteration 4600 loss 285.42636\n",
      "iteration 4700 loss 285.42633\n",
      "iteration 4800 loss 285.42633\n",
      "iteration 4900 loss 285.42633\n",
      "iteration 5000 loss 285.42633Train finished\n",
      "Selected features are: [14 15 40 48 58]\n",
      "Selected weights are: [0.54439723 0.99866587 0.8957283  0.99866587 0.9561067 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0005997523669914536, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 20.316675932078255\n",
      "\n",
      " 10 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 278.0174\n",
      "iteration 200 loss 272.3974\n",
      "iteration 300 loss 268.09424\n",
      "iteration 400 loss 267.35254\n",
      "iteration 500 loss 267.10822\n",
      "iteration 600 loss 266.9712\n",
      "iteration 700 loss 266.90973\n",
      "iteration 800 loss 266.83572\n",
      "iteration 900 loss 266.7638\n",
      "iteration 1000 loss 266.70776\n",
      "iteration 1100 loss 266.6784\n",
      "iteration 1200 loss 266.6604\n",
      "iteration 1300 loss 266.6469\n",
      "iteration 1400 loss 266.63672\n",
      "iteration 1500 loss 266.62903\n",
      "iteration 1600 loss 266.62323\n",
      "iteration 1700 loss 266.61896\n",
      "iteration 1800 loss 266.6158\n",
      "iteration 1900 loss 266.61346\n",
      "iteration 2000 loss 266.61176\n",
      "iteration 2100 loss 266.61047\n",
      "iteration 2200 loss 266.60956\n",
      "iteration 2300 loss 266.6089\n",
      "iteration 2400 loss 266.60843\n",
      "iteration 2500 loss 266.60806\n",
      "iteration 2600 loss 266.60785\n",
      "iteration 2700 loss 266.60767\n",
      "iteration 2800 loss 266.60757\n",
      "iteration 2900 loss 266.60748\n",
      "iteration 3000 loss 266.6074\n",
      "iteration 3100 loss 266.60736\n",
      "iteration 3200 loss 266.60736\n",
      "iteration 3300 loss 266.6073\n",
      "iteration 3400 loss 266.6073\n",
      "iteration 3500 loss 266.60733\n",
      "iteration 3600 loss 266.6073\n",
      "iteration 3700 loss 266.6073\n",
      "iteration 3800 loss 266.60727\n",
      "iteration 3900 loss 266.6073\n",
      "iteration 4000 loss 266.60727\n",
      "iteration 4100 loss 266.60727\n",
      "iteration 4200 loss 266.60727\n",
      "iteration 4300 loss 266.6073\n",
      "iteration 4400 loss 266.6073\n",
      "iteration 4500 loss 266.60727\n",
      "iteration 4600 loss 266.60733\n",
      "iteration 4700 loss 266.6073\n",
      "iteration 4800 loss 266.6073\n",
      "iteration 4900 loss 266.6073\n",
      "iteration 5000 loss 266.6073Train finished\n",
      "Selected features are: [ 7  8 14 26 46]\n",
      "Selected weights are: [0.75461596 0.8332629  0.9981508  0.9849472  0.9981508 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 9.795610502922289e-05, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 31.51582301260509\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 279.40002\n",
      "iteration 200 loss 275.10974\n",
      "iteration 300 loss 274.56763\n",
      "iteration 400 loss 274.4219\n",
      "iteration 500 loss 274.33673\n",
      "iteration 600 loss 274.274\n",
      "iteration 700 loss 274.22943\n",
      "iteration 800 loss 274.2058\n",
      "iteration 900 loss 274.19507\n",
      "iteration 1000 loss 274.18762\n",
      "iteration 1100 loss 274.1824\n",
      "iteration 1200 loss 274.1787\n",
      "iteration 1300 loss 274.17606\n",
      "iteration 1400 loss 274.17416\n",
      "iteration 1500 loss 274.1728\n",
      "iteration 1600 loss 274.1718\n",
      "iteration 1700 loss 274.17102\n",
      "iteration 1800 loss 274.17044\n",
      "iteration 1900 loss 274.17\n",
      "iteration 2000 loss 274.16965\n",
      "iteration 2100 loss 274.16934\n",
      "iteration 2200 loss 274.16907\n",
      "iteration 2300 loss 274.16888\n",
      "iteration 2400 loss 274.16867\n",
      "iteration 2500 loss 274.16852\n",
      "iteration 2600 loss 274.16833\n",
      "iteration 2700 loss 274.1682\n",
      "iteration 2800 loss 274.1681\n",
      "iteration 2900 loss 274.16794\n",
      "iteration 3000 loss 274.16785\n",
      "iteration 3100 loss 274.16772\n",
      "iteration 3200 loss 274.1676\n",
      "iteration 3300 loss 274.16748\n",
      "iteration 3400 loss 274.16742\n",
      "iteration 3500 loss 274.16736\n",
      "iteration 3600 loss 274.16724\n",
      "iteration 3700 loss 274.1671\n",
      "iteration 3800 loss 274.16708\n",
      "iteration 3900 loss 274.16702\n",
      "iteration 4000 loss 274.16696\n",
      "iteration 4100 loss 274.16687\n",
      "iteration 4200 loss 274.1668\n",
      "iteration 4300 loss 274.16675\n",
      "iteration 4400 loss 274.1667\n",
      "iteration 4500 loss 274.16663\n",
      "iteration 4600 loss 274.16656\n",
      "iteration 4700 loss 274.1665\n",
      "iteration 4800 loss 274.16647\n",
      "iteration 4900 loss 274.16644\n",
      "iteration 5000 loss 274.16635Train finished\n",
      "Selected features are: [14 15 40 48 58]\n",
      "Selected weights are: [0.4922549  0.9987191  0.71758467 0.9987191  0.81687695]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0005809246068437361, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 22.783887318201636\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 194.26266\n",
      "iteration 200 loss 191.58365\n",
      "iteration 300 loss 190.20416\n",
      "iteration 400 loss 189.60419\n",
      "iteration 500 loss 189.34705\n",
      "iteration 600 loss 189.2403\n",
      "iteration 700 loss 189.17987\n",
      "iteration 800 loss 189.13068\n",
      "iteration 900 loss 189.11267\n",
      "iteration 1000 loss 189.09705\n",
      "iteration 1100 loss 189.08273\n",
      "iteration 1200 loss 189.07283\n",
      "iteration 1300 loss 189.06888\n",
      "iteration 1400 loss 189.06555\n",
      "iteration 1500 loss 189.06271\n",
      "iteration 1600 loss 189.0603\n",
      "iteration 1700 loss 189.05818\n",
      "iteration 1800 loss 189.0564\n",
      "iteration 1900 loss 189.05487\n",
      "iteration 2000 loss 189.0542\n",
      "iteration 2100 loss 189.0541\n",
      "iteration 2200 loss 189.05402\n",
      "iteration 2300 loss 189.05397\n",
      "iteration 2400 loss 189.05396\n",
      "iteration 2500 loss 189.05396\n",
      "iteration 2600 loss 189.05392\n",
      "iteration 2700 loss 189.05392\n",
      "iteration 2800 loss 189.05391\n",
      "iteration 2900 loss 189.0539\n",
      "iteration 3000 loss 189.0539\n",
      "iteration 3100 loss 189.05388\n",
      "iteration 3200 loss 189.0539\n",
      "iteration 3300 loss 189.0539\n",
      "iteration 3400 loss 189.0539\n",
      "iteration 3500 loss 189.0539\n",
      "iteration 3600 loss 189.05385\n",
      "iteration 3700 loss 189.05388\n",
      "iteration 3800 loss 189.05386\n",
      "iteration 3900 loss 189.05388\n",
      "iteration 4000 loss 189.05386\n",
      "iteration 4100 loss 189.05386\n",
      "iteration 4200 loss 189.05388\n",
      "iteration 4300 loss 189.05386\n",
      "iteration 4400 loss 189.05385\n",
      "iteration 4500 loss 189.0539\n",
      "iteration 4600 loss 189.05386\n",
      "iteration 4700 loss 189.05386\n",
      "iteration 4800 loss 189.05386\n",
      "iteration 4900 loss 189.05388\n",
      "iteration 5000 loss 189.05386Train finished\n",
      "Selected features are: [ 7 15 40 48 51]\n",
      "Selected weights are: [0.53110844 0.9987958  0.9987958  0.7671545  0.9987958 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00011687478109068487, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 60.387815967033\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 304.9313\n",
      "iteration 200 loss 299.65604\n",
      "iteration 300 loss 298.78168\n",
      "iteration 400 loss 298.63818\n",
      "iteration 500 loss 298.5362\n",
      "iteration 600 loss 298.50958\n",
      "iteration 700 loss 298.4927\n",
      "iteration 800 loss 298.47864\n",
      "iteration 900 loss 298.4662\n",
      "iteration 1000 loss 298.455\n",
      "iteration 1100 loss 298.44565\n",
      "iteration 1200 loss 298.44556\n",
      "iteration 1300 loss 298.44543\n",
      "iteration 1400 loss 298.44543\n",
      "iteration 1500 loss 298.44556\n",
      "iteration 1600 loss 298.44553\n",
      "iteration 1700 loss 298.44556\n",
      "iteration 1800 loss 298.44556\n",
      "iteration 1900 loss 298.4456\n",
      "iteration 2000 loss 298.44562\n",
      "iteration 2100 loss 298.44556\n",
      "iteration 2200 loss 298.44562\n",
      "iteration 2300 loss 298.4456\n",
      "iteration 2400 loss 298.4456\n",
      "iteration 2500 loss 298.44556\n",
      "iteration 2600 loss 298.4456\n",
      "iteration 2700 loss 298.4456\n",
      "iteration 2800 loss 298.4456\n",
      "iteration 2900 loss 298.4456\n",
      "iteration 3000 loss 298.4456\n",
      "iteration 3100 loss 298.4456\n",
      "iteration 3200 loss 298.4456\n",
      "iteration 3300 loss 298.4456\n",
      "iteration 3400 loss 298.4456\n",
      "iteration 3500 loss 298.4456\n",
      "iteration 3600 loss 298.4456\n",
      "iteration 3700 loss 298.4456\n",
      "iteration 3800 loss 298.4456\n",
      "iteration 3900 loss 298.4456\n",
      "iteration 4000 loss 298.4456\n",
      "iteration 4100 loss 298.4456\n",
      "iteration 4200 loss 298.4456\n",
      "iteration 4300 loss 298.4456\n",
      "iteration 4400 loss 298.4456\n",
      "iteration 4500 loss 298.4456\n",
      "iteration 4600 loss 298.4456\n",
      "iteration 4700 loss 298.4456\n",
      "iteration 4800 loss 298.4456\n",
      "iteration 4900 loss 298.4456\n",
      "iteration 5000 loss 298.4456Train finished\n",
      "Selected features are: [15 40 48 58 69]\n",
      "Selected weights are: [0.9977345  0.84408826 0.9977345  0.9977345  0.47811928]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0014706407769428276, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 10.884689183392226\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 301.96765\n",
      "iteration 200 loss 293.37567\n",
      "iteration 300 loss 291.6054\n",
      "iteration 400 loss 291.52676\n",
      "iteration 500 loss 291.47662\n",
      "iteration 600 loss 291.4374\n",
      "iteration 700 loss 291.41302\n",
      "iteration 800 loss 291.39282\n",
      "iteration 900 loss 291.38202\n",
      "iteration 1000 loss 291.37488\n",
      "iteration 1100 loss 291.36908\n",
      "iteration 1200 loss 291.36407\n",
      "iteration 1300 loss 291.3597\n",
      "iteration 1400 loss 291.35583\n",
      "iteration 1500 loss 291.35233\n",
      "iteration 1600 loss 291.34906\n",
      "iteration 1700 loss 291.34607\n",
      "iteration 1800 loss 291.34332\n",
      "iteration 1900 loss 291.34076\n",
      "iteration 2000 loss 291.33844\n",
      "iteration 2100 loss 291.33624\n",
      "iteration 2200 loss 291.3342\n",
      "iteration 2300 loss 291.33234\n",
      "iteration 2400 loss 291.3305\n",
      "iteration 2500 loss 291.32886\n",
      "iteration 2600 loss 291.32834\n",
      "iteration 2700 loss 291.32825\n",
      "iteration 2800 loss 291.3282\n",
      "iteration 2900 loss 291.32812\n",
      "iteration 3000 loss 291.32812\n",
      "iteration 3100 loss 291.3281\n",
      "iteration 3200 loss 291.32806\n",
      "iteration 3300 loss 291.32806\n",
      "iteration 3400 loss 291.32806\n",
      "iteration 3500 loss 291.32806\n",
      "iteration 3600 loss 291.32806\n",
      "iteration 3700 loss 291.3281\n",
      "iteration 3800 loss 291.32806\n",
      "iteration 3900 loss 291.32806\n",
      "iteration 4000 loss 291.32806\n",
      "iteration 4100 loss 291.32806\n",
      "iteration 4200 loss 291.3281\n",
      "iteration 4300 loss 291.32806\n",
      "iteration 4400 loss 291.32806\n",
      "iteration 4500 loss 291.32806\n",
      "iteration 4600 loss 291.32806\n",
      "iteration 4700 loss 291.32806\n",
      "iteration 4800 loss 291.32806\n",
      "iteration 4900 loss 291.32806\n",
      "iteration 5000 loss 291.32806Train finished\n",
      "Selected features are: [14 15 40 48 58]\n",
      "Selected weights are: [0.5274977 0.9988    0.9988    0.9988    0.695404 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0005120861649324371, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 18.78597129190707\n",
      "28.87355994515984 \n",
      "\n",
      "\n",
      " 1 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 228.18518\n",
      "iteration 200 loss 225.93065\n",
      "iteration 300 loss 224.69328\n",
      "iteration 400 loss 224.17519\n",
      "iteration 500 loss 223.89816\n",
      "iteration 600 loss 223.7077\n",
      "iteration 700 loss 223.53114\n",
      "iteration 800 loss 223.37436\n",
      "iteration 900 loss 223.33267\n",
      "iteration 1000 loss 223.29416\n",
      "iteration 1100 loss 223.25723\n",
      "iteration 1200 loss 223.2221\n",
      "iteration 1300 loss 223.18823\n",
      "iteration 1400 loss 223.15543\n",
      "iteration 1500 loss 223.12341\n",
      "iteration 1600 loss 223.09238\n",
      "iteration 1700 loss 223.06203\n",
      "iteration 1800 loss 223.0325\n",
      "iteration 1900 loss 223.0037\n",
      "iteration 2000 loss 222.9755\n",
      "iteration 2100 loss 222.94789\n",
      "iteration 2200 loss 222.92088\n",
      "iteration 2300 loss 222.89445\n",
      "iteration 2400 loss 222.86838\n",
      "iteration 2500 loss 222.84314\n",
      "iteration 2600 loss 222.81825\n",
      "iteration 2700 loss 222.8028\n",
      "iteration 2800 loss 222.79211\n",
      "iteration 2900 loss 222.78381\n",
      "iteration 3000 loss 222.7764\n",
      "iteration 3100 loss 222.7695\n",
      "iteration 3200 loss 222.763\n",
      "iteration 3300 loss 222.75664\n",
      "iteration 3400 loss 222.7508\n",
      "iteration 3500 loss 222.74518\n",
      "iteration 3600 loss 222.74004\n",
      "iteration 3700 loss 222.73488\n",
      "iteration 3800 loss 222.72986\n",
      "iteration 3900 loss 222.72493\n",
      "iteration 4000 loss 222.72\n",
      "iteration 4100 loss 222.7152\n",
      "iteration 4200 loss 222.71042\n",
      "iteration 4300 loss 222.70578\n",
      "iteration 4400 loss 222.70152\n",
      "iteration 4500 loss 222.69728\n",
      "iteration 4600 loss 222.69328\n",
      "iteration 4700 loss 222.68954\n",
      "iteration 4800 loss 222.6857\n",
      "iteration 4900 loss 222.68391\n",
      "iteration 5000 loss 222.68219Train finished\n",
      "Selected features are: [ 2 14 15 24 40 48 51 55 58 63]\n",
      "Selected weights are: [0.99991506 0.47284803 0.99991506 0.99991506 0.4821204  0.99991506\n",
      " 0.99991506 0.9212471  0.76332337 0.99991506]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0006336318020798503, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 41.232183230461416\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 328.08182\n",
      "iteration 200 loss 317.95926\n",
      "iteration 300 loss 316.32895\n",
      "iteration 400 loss 315.54257\n",
      "iteration 500 loss 315.03683\n",
      "iteration 600 loss 314.56046\n",
      "iteration 700 loss 313.9978\n",
      "iteration 800 loss 313.28992\n",
      "iteration 900 loss 312.43753\n",
      "iteration 1000 loss 311.9103\n",
      "iteration 1100 loss 311.6021\n",
      "iteration 1200 loss 311.41327\n",
      "iteration 1300 loss 311.31836\n",
      "iteration 1400 loss 311.23578\n",
      "iteration 1500 loss 311.15775\n",
      "iteration 1600 loss 311.1282\n",
      "iteration 1700 loss 311.10754\n",
      "iteration 1800 loss 311.09085\n",
      "iteration 1900 loss 311.0776\n",
      "iteration 2000 loss 311.07693\n",
      "iteration 2100 loss 311.07657\n",
      "iteration 2200 loss 311.07623\n",
      "iteration 2300 loss 311.07593\n",
      "iteration 2400 loss 311.07562\n",
      "iteration 2500 loss 311.07535\n",
      "iteration 2600 loss 311.07507\n",
      "iteration 2700 loss 311.07477\n",
      "iteration 2800 loss 311.07452\n",
      "iteration 2900 loss 311.07422\n",
      "iteration 3000 loss 311.07404\n",
      "iteration 3100 loss 311.07376\n",
      "iteration 3200 loss 311.0735\n",
      "iteration 3300 loss 311.07324\n",
      "iteration 3400 loss 311.07303\n",
      "iteration 3500 loss 311.07278\n",
      "iteration 3600 loss 311.07257\n",
      "iteration 3700 loss 311.07233\n",
      "iteration 3800 loss 311.07214\n",
      "iteration 3900 loss 311.07196\n",
      "iteration 4000 loss 311.07175\n",
      "iteration 4100 loss 311.0716\n",
      "iteration 4200 loss 311.07138\n",
      "iteration 4300 loss 311.07123\n",
      "iteration 4400 loss 311.07104\n",
      "iteration 4500 loss 311.07086\n",
      "iteration 4600 loss 311.07068\n",
      "iteration 4700 loss 311.0705\n",
      "iteration 4800 loss 311.07034\n",
      "iteration 4900 loss 311.0702\n",
      "iteration 5000 loss 311.07007Train finished\n",
      "Selected features are: [ 8 11 26 37 46 49 52 56 62 70]\n",
      "Selected weights are: [0.33242723 0.4875202  0.99961716 0.99961716 0.82659495 0.99961716\n",
      " 0.99961716 0.99961716 0.99961716 0.99961716]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0011218514041861324, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 9.673790099566345\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 253.60283\n",
      "iteration 200 loss 250.26099\n",
      "iteration 300 loss 249.2402\n",
      "iteration 400 loss 248.78116\n",
      "iteration 500 loss 248.366\n",
      "iteration 600 loss 247.97296\n",
      "iteration 700 loss 247.60674\n",
      "iteration 800 loss 247.27316\n",
      "iteration 900 loss 247.08951\n",
      "iteration 1000 loss 247.01146\n",
      "iteration 1100 loss 246.95398\n",
      "iteration 1200 loss 246.92102\n",
      "iteration 1300 loss 246.89157\n",
      "iteration 1400 loss 246.876\n",
      "iteration 1500 loss 246.86115\n",
      "iteration 1600 loss 246.84686\n",
      "iteration 1700 loss 246.8376\n",
      "iteration 1800 loss 246.8309\n",
      "iteration 1900 loss 246.8255\n",
      "iteration 2000 loss 246.82056\n",
      "iteration 2100 loss 246.81583\n",
      "iteration 2200 loss 246.81125\n",
      "iteration 2300 loss 246.80681\n",
      "iteration 2400 loss 246.80247\n",
      "iteration 2500 loss 246.79826\n",
      "iteration 2600 loss 246.79419\n",
      "iteration 2700 loss 246.79022\n",
      "iteration 2800 loss 246.78633\n",
      "iteration 2900 loss 246.7825\n",
      "iteration 3000 loss 246.77881\n",
      "iteration 3100 loss 246.77524\n",
      "iteration 3200 loss 246.77168\n",
      "iteration 3300 loss 246.76822\n",
      "iteration 3400 loss 246.76518\n",
      "iteration 3500 loss 246.76309\n",
      "iteration 3600 loss 246.76096\n",
      "iteration 3700 loss 246.7589\n",
      "iteration 3800 loss 246.75781\n",
      "iteration 3900 loss 246.75725\n",
      "iteration 4000 loss 246.75668\n",
      "iteration 4100 loss 246.75616\n",
      "iteration 4200 loss 246.75562\n",
      "iteration 4300 loss 246.75513\n",
      "iteration 4400 loss 246.75467\n",
      "iteration 4500 loss 246.75421\n",
      "iteration 4600 loss 246.75375\n",
      "iteration 4700 loss 246.75333\n",
      "iteration 4800 loss 246.75293\n",
      "iteration 4900 loss 246.75253\n",
      "iteration 5000 loss 246.75206Train finished\n",
      "Selected features are: [ 8  9 10 14 15 40 48 58 63 69]\n",
      "Selected weights are: [0.99952483 0.99952483 0.99952483 0.59053636 0.99952483 0.62353104\n",
      " 0.99952483 0.99952483 0.99952483 0.6867681 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00036063794028635103, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 32.493690264699126\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 240.04813\n",
      "iteration 200 loss 235.186\n",
      "iteration 300 loss 230.43637\n",
      "iteration 400 loss 228.34927\n",
      "iteration 500 loss 227.29933\n",
      "iteration 600 loss 226.77652\n",
      "iteration 700 loss 226.41853\n",
      "iteration 800 loss 226.15704\n",
      "iteration 900 loss 225.9816\n",
      "iteration 1000 loss 225.8133\n",
      "iteration 1100 loss 225.65292\n",
      "iteration 1200 loss 225.50009\n",
      "iteration 1300 loss 225.3541\n",
      "iteration 1400 loss 225.23419\n",
      "iteration 1500 loss 225.1576\n",
      "iteration 1600 loss 225.11507\n",
      "iteration 1700 loss 225.07742\n",
      "iteration 1800 loss 225.06418\n",
      "iteration 1900 loss 225.05515\n",
      "iteration 2000 loss 225.04768\n",
      "iteration 2100 loss 225.04109\n",
      "iteration 2200 loss 225.03455\n",
      "iteration 2300 loss 225.02989\n",
      "iteration 2400 loss 225.0253\n",
      "iteration 2500 loss 225.02124\n",
      "iteration 2600 loss 225.01898\n",
      "iteration 2700 loss 225.01732\n",
      "iteration 2800 loss 225.01575\n",
      "iteration 2900 loss 225.01425\n",
      "iteration 3000 loss 225.01279\n",
      "iteration 3100 loss 225.01132\n",
      "iteration 3200 loss 225.00995\n",
      "iteration 3300 loss 225.00859\n",
      "iteration 3400 loss 225.0073\n",
      "iteration 3500 loss 225.00604\n",
      "iteration 3600 loss 225.00482\n",
      "iteration 3700 loss 225.00363\n",
      "iteration 3800 loss 225.00247\n",
      "iteration 3900 loss 225.00134\n",
      "iteration 4000 loss 225.00029\n",
      "iteration 4100 loss 224.9992\n",
      "iteration 4200 loss 224.9982\n",
      "iteration 4300 loss 224.9972\n",
      "iteration 4400 loss 224.99623\n",
      "iteration 4500 loss 224.99532\n",
      "iteration 4600 loss 224.99445\n",
      "iteration 4700 loss 224.99358\n",
      "iteration 4800 loss 224.99272\n",
      "iteration 4900 loss 224.99188\n",
      "iteration 5000 loss 224.99124Train finished\n",
      "Selected features are: [ 9 15 24 40 48 54 55 58 63 70]\n",
      "Selected weights are: [0.99968755 0.99968755 0.7226651  0.99968755 0.99968755 0.99968755\n",
      " 0.6443247  0.99968755 0.99968755 0.99968755]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0011314935145386147, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 48.22404999455271\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 315.05127\n",
      "iteration 200 loss 312.62015\n",
      "iteration 300 loss 310.5341\n",
      "iteration 400 loss 308.71185\n",
      "iteration 500 loss 307.0595\n",
      "iteration 600 loss 305.46576\n",
      "iteration 700 loss 304.17505\n",
      "iteration 800 loss 303.88654\n",
      "iteration 900 loss 303.81137\n",
      "iteration 1000 loss 303.74158\n",
      "iteration 1100 loss 303.67633\n",
      "iteration 1200 loss 303.61627\n",
      "iteration 1300 loss 303.57416\n",
      "iteration 1400 loss 303.5696\n",
      "iteration 1500 loss 303.56577\n",
      "iteration 1600 loss 303.5625\n",
      "iteration 1700 loss 303.56155\n",
      "iteration 1800 loss 303.56116\n",
      "iteration 1900 loss 303.5608\n",
      "iteration 2000 loss 303.5605\n",
      "iteration 2100 loss 303.56027\n",
      "iteration 2200 loss 303.56006\n",
      "iteration 2300 loss 303.55994\n",
      "iteration 2400 loss 303.5598\n",
      "iteration 2500 loss 303.55972\n",
      "iteration 2600 loss 303.55966\n",
      "iteration 2700 loss 303.55957\n",
      "iteration 2800 loss 303.55954\n",
      "iteration 2900 loss 303.5595\n",
      "iteration 3000 loss 303.5595\n",
      "iteration 3100 loss 303.55945\n",
      "iteration 3200 loss 303.55945\n",
      "iteration 3300 loss 303.55942\n",
      "iteration 3400 loss 303.5594\n",
      "iteration 3500 loss 303.55942\n",
      "iteration 3600 loss 303.5594\n",
      "iteration 3700 loss 303.5594\n",
      "iteration 3800 loss 303.5594\n",
      "iteration 3900 loss 303.5594\n",
      "iteration 4000 loss 303.5594\n",
      "iteration 4100 loss 303.5594\n",
      "iteration 4200 loss 303.5594\n",
      "iteration 4300 loss 303.5594\n",
      "iteration 4400 loss 303.5594\n",
      "iteration 4500 loss 303.5594\n",
      "iteration 4600 loss 303.5594\n",
      "iteration 4700 loss 303.5594\n",
      "iteration 4800 loss 303.5594\n",
      "iteration 4900 loss 303.5594\n",
      "iteration 5000 loss 303.5594Train finished\n",
      "Selected features are: [ 8 11 13 26 37 49 52 56 62 70]\n",
      "Selected weights are: [0.75435585 0.6892836  0.43688044 0.9995096  0.9995096  0.9995096\n",
      " 0.9995096  0.9995096  0.9995096  0.9995096 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0018437982929577464, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 12.222686527320048\n",
      "\n",
      " 2 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 257.9069\n",
      "iteration 200 loss 256.42413\n",
      "iteration 300 loss 250.8518\n",
      "iteration 400 loss 249.7724\n",
      "iteration 500 loss 248.78343\n",
      "iteration 600 loss 247.88832\n",
      "iteration 700 loss 247.06396\n",
      "iteration 800 loss 246.43092\n",
      "iteration 900 loss 245.81891\n",
      "iteration 1000 loss 245.30835\n",
      "iteration 1100 loss 244.92702\n",
      "iteration 1200 loss 244.57947\n",
      "iteration 1300 loss 244.42673\n",
      "iteration 1400 loss 244.37943\n",
      "iteration 1500 loss 244.36723\n",
      "iteration 1600 loss 244.36081\n",
      "iteration 1700 loss 244.3543\n",
      "iteration 1800 loss 244.35272\n",
      "iteration 1900 loss 244.35187\n",
      "iteration 2000 loss 244.35124\n",
      "iteration 2100 loss 244.35077\n",
      "iteration 2200 loss 244.35017\n",
      "iteration 2300 loss 244.34961\n",
      "iteration 2400 loss 244.34908\n",
      "iteration 2500 loss 244.34854\n",
      "iteration 2600 loss 244.348\n",
      "iteration 2700 loss 244.34747\n",
      "iteration 2800 loss 244.34691\n",
      "iteration 2900 loss 244.3464\n",
      "iteration 3000 loss 244.34589\n",
      "iteration 3100 loss 244.34535\n",
      "iteration 3200 loss 244.34483\n",
      "iteration 3300 loss 244.3443\n",
      "iteration 3400 loss 244.34377\n",
      "iteration 3500 loss 244.34326\n",
      "iteration 3600 loss 244.34274\n",
      "iteration 3700 loss 244.34221\n",
      "iteration 3800 loss 244.34169\n",
      "iteration 3900 loss 244.34122\n",
      "iteration 4000 loss 244.3407\n",
      "iteration 4100 loss 244.34021\n",
      "iteration 4200 loss 244.33969\n",
      "iteration 4300 loss 244.33917\n",
      "iteration 4400 loss 244.33867\n",
      "iteration 4500 loss 244.33818\n",
      "iteration 4600 loss 244.3377\n",
      "iteration 4700 loss 244.33722\n",
      "iteration 4800 loss 244.33673\n",
      "iteration 4900 loss 244.33621\n",
      "iteration 5000 loss 244.33621Train finished\n",
      "Selected features are: [14 15 24 33 40 48 54 58 63 69]\n",
      "Selected weights are: [0.5102269 0.9996873 0.5243685 0.9996873 0.9996873 0.9996873 0.9996873\n",
      " 0.9996873 0.9996873 0.9996873]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0005418402716410837, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 40.53326771434887\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 290.30066\n",
      "iteration 200 loss 285.5404\n",
      "iteration 300 loss 283.9605\n",
      "iteration 400 loss 283.15198\n",
      "iteration 500 loss 282.51672\n",
      "iteration 600 loss 281.9973\n",
      "iteration 700 loss 281.5766\n",
      "iteration 800 loss 281.31052\n",
      "iteration 900 loss 281.12927\n",
      "iteration 1000 loss 281.02188\n",
      "iteration 1100 loss 280.95825\n",
      "iteration 1200 loss 280.90808\n",
      "iteration 1300 loss 280.8684\n",
      "iteration 1400 loss 280.84802\n",
      "iteration 1500 loss 280.8294\n",
      "iteration 1600 loss 280.81595\n",
      "iteration 1700 loss 280.80872\n",
      "iteration 1800 loss 280.80493\n",
      "iteration 1900 loss 280.80133\n",
      "iteration 2000 loss 280.79788\n",
      "iteration 2100 loss 280.79462\n",
      "iteration 2200 loss 280.7914\n",
      "iteration 2300 loss 280.78827\n",
      "iteration 2400 loss 280.78546\n",
      "iteration 2500 loss 280.7827\n",
      "iteration 2600 loss 280.78003\n",
      "iteration 2700 loss 280.7774\n",
      "iteration 2800 loss 280.7749\n",
      "iteration 2900 loss 280.7724\n",
      "iteration 3000 loss 280.76996\n",
      "iteration 3100 loss 280.7676\n",
      "iteration 3200 loss 280.76532\n",
      "iteration 3300 loss 280.76434\n",
      "iteration 3400 loss 280.76395\n",
      "iteration 3500 loss 280.76367\n",
      "iteration 3600 loss 280.7635\n",
      "iteration 3700 loss 280.76337\n",
      "iteration 3800 loss 280.7632\n",
      "iteration 3900 loss 280.76312\n",
      "iteration 4000 loss 280.76306\n",
      "iteration 4100 loss 280.763\n",
      "iteration 4200 loss 280.763\n",
      "iteration 4300 loss 280.76294\n",
      "iteration 4400 loss 280.76294\n",
      "iteration 4500 loss 280.76288\n",
      "iteration 4600 loss 280.76288\n",
      "iteration 4700 loss 280.76288\n",
      "iteration 4800 loss 280.76285\n",
      "iteration 4900 loss 280.76285\n",
      "iteration 5000 loss 280.76285Train finished\n",
      "Selected features are: [ 9 14 15 40 48 51 54 58 63 69]\n",
      "Selected weights are: [0.9995319  0.40544605 0.9995319  0.9995319  0.9995319  0.9995319\n",
      " 0.87786096 0.9995319  0.9995319  0.9995319 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.000524180049354116, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 17.599559628698437\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 269.86902\n",
      "iteration 200 loss 261.92307\n",
      "iteration 300 loss 259.63934\n",
      "iteration 400 loss 258.71234\n",
      "iteration 500 loss 258.31918\n",
      "iteration 600 loss 257.97318\n",
      "iteration 700 loss 257.66647\n",
      "iteration 800 loss 257.3853\n",
      "iteration 900 loss 257.12903\n",
      "iteration 1000 loss 256.9397\n",
      "iteration 1100 loss 256.79785\n",
      "iteration 1200 loss 256.71387\n",
      "iteration 1300 loss 256.6684\n",
      "iteration 1400 loss 256.63068\n",
      "iteration 1500 loss 256.60355\n",
      "iteration 1600 loss 256.58414\n",
      "iteration 1700 loss 256.57025\n",
      "iteration 1800 loss 256.5622\n",
      "iteration 1900 loss 256.555\n",
      "iteration 2000 loss 256.54834\n",
      "iteration 2100 loss 256.54202\n",
      "iteration 2200 loss 256.536\n",
      "iteration 2300 loss 256.53012\n",
      "iteration 2400 loss 256.5244\n",
      "iteration 2500 loss 256.5188\n",
      "iteration 2600 loss 256.51337\n",
      "iteration 2700 loss 256.50967\n",
      "iteration 2800 loss 256.50946\n",
      "iteration 2900 loss 256.50925\n",
      "iteration 3000 loss 256.50916\n",
      "iteration 3100 loss 256.50903\n",
      "iteration 3200 loss 256.5089\n",
      "iteration 3300 loss 256.50888\n",
      "iteration 3400 loss 256.5088\n",
      "iteration 3500 loss 256.50873\n",
      "iteration 3600 loss 256.50867\n",
      "iteration 3700 loss 256.5086\n",
      "iteration 3800 loss 256.50854\n",
      "iteration 3900 loss 256.5085\n",
      "iteration 4000 loss 256.50845\n",
      "iteration 4100 loss 256.50842\n",
      "iteration 4200 loss 256.5084\n",
      "iteration 4300 loss 256.50833\n",
      "iteration 4400 loss 256.5083\n",
      "iteration 4500 loss 256.50824\n",
      "iteration 4600 loss 256.50818\n",
      "iteration 4700 loss 256.50812\n",
      "iteration 4800 loss 256.50812\n",
      "iteration 4900 loss 256.50806\n",
      "iteration 5000 loss 256.508Train finished\n",
      "Selected features are: [ 9 14 15 40 48 54 58 63 69 70]\n",
      "Selected weights are: [0.85330373 0.55583286 0.9995392  0.9995392  0.9995392  0.5585341\n",
      " 0.9995392  0.9995392  0.9995392  0.9995392 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0005837521389989717, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 30.01714321241702\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 273.06152\n",
      "iteration 200 loss 268.9269\n",
      "iteration 300 loss 267.9574\n",
      "iteration 400 loss 267.26837\n",
      "iteration 500 loss 266.6648\n",
      "iteration 600 loss 266.16943\n",
      "iteration 700 loss 265.89682\n",
      "iteration 800 loss 265.6546\n",
      "iteration 900 loss 265.43353\n",
      "iteration 1000 loss 265.30914\n",
      "iteration 1100 loss 265.22986\n",
      "iteration 1200 loss 265.17242\n",
      "iteration 1300 loss 265.12738\n",
      "iteration 1400 loss 265.09888\n",
      "iteration 1500 loss 265.0734\n",
      "iteration 1600 loss 265.04956\n",
      "iteration 1700 loss 265.0333\n",
      "iteration 1800 loss 265.02173\n",
      "iteration 1900 loss 265.01633\n",
      "iteration 2000 loss 265.0118\n",
      "iteration 2100 loss 265.01114\n",
      "iteration 2200 loss 265.0108\n",
      "iteration 2300 loss 265.01056\n",
      "iteration 2400 loss 265.01038\n",
      "iteration 2500 loss 265.01013\n",
      "iteration 2600 loss 265.00995\n",
      "iteration 2700 loss 265.00977\n",
      "iteration 2800 loss 265.00958\n",
      "iteration 2900 loss 265.0094\n",
      "iteration 3000 loss 265.00922\n",
      "iteration 3100 loss 265.0091\n",
      "iteration 3200 loss 265.00888\n",
      "iteration 3300 loss 265.00876\n",
      "iteration 3400 loss 265.00858\n",
      "iteration 3500 loss 265.00842\n",
      "iteration 3600 loss 265.0083\n",
      "iteration 3700 loss 265.00815\n",
      "iteration 3800 loss 265.008\n",
      "iteration 3900 loss 265.00787\n",
      "iteration 4000 loss 265.0077\n",
      "iteration 4100 loss 265.00763\n",
      "iteration 4200 loss 265.00745\n",
      "iteration 4300 loss 265.00732\n",
      "iteration 4400 loss 265.0072\n",
      "iteration 4500 loss 265.00708\n",
      "iteration 4600 loss 265.00696\n",
      "iteration 4700 loss 265.0068\n",
      "iteration 4800 loss 265.00665\n",
      "iteration 4900 loss 265.0066\n",
      "iteration 5000 loss 265.00647Train finished\n",
      "Selected features are: [ 9 10 14 15 24 40 48 58 63 69]\n",
      "Selected weights are: [0.9995275  0.7664473  0.55017036 0.9995275  0.75342184 0.9995275\n",
      " 0.9995275  0.9995275  0.9995275  0.82049435]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00047901642167604344, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 23.433120629325014\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 264.94348\n",
      "iteration 200 loss 263.4615\n",
      "iteration 300 loss 262.9536\n",
      "iteration 400 loss 262.6084\n",
      "iteration 500 loss 262.4057\n",
      "iteration 600 loss 262.19028\n",
      "iteration 700 loss 261.94836\n",
      "iteration 800 loss 261.7799\n",
      "iteration 900 loss 261.56122\n",
      "iteration 1000 loss 260.87708\n",
      "iteration 1100 loss 260.1585\n",
      "iteration 1200 loss 259.3943\n",
      "iteration 1300 loss 258.6344\n",
      "iteration 1400 loss 258.11597\n",
      "iteration 1500 loss 257.77335\n",
      "iteration 1600 loss 257.4892\n",
      "iteration 1700 loss 257.2328\n",
      "iteration 1800 loss 257.00885\n",
      "iteration 1900 loss 256.8979\n",
      "iteration 2000 loss 256.84073\n",
      "iteration 2100 loss 256.789\n",
      "iteration 2200 loss 256.76898\n",
      "iteration 2300 loss 256.75714\n",
      "iteration 2400 loss 256.7469\n",
      "iteration 2500 loss 256.73782\n",
      "iteration 2600 loss 256.72977\n",
      "iteration 2700 loss 256.72247\n",
      "iteration 2800 loss 256.71576\n",
      "iteration 2900 loss 256.70956\n",
      "iteration 3000 loss 256.70377\n",
      "iteration 3100 loss 256.6983\n",
      "iteration 3200 loss 256.69397\n",
      "iteration 3300 loss 256.6929\n",
      "iteration 3400 loss 256.69287\n",
      "iteration 3500 loss 256.6929\n",
      "iteration 3600 loss 256.69287\n",
      "iteration 3700 loss 256.6929\n",
      "iteration 3800 loss 256.6929\n",
      "iteration 3900 loss 256.69287\n",
      "iteration 4000 loss 256.69287\n",
      "iteration 4100 loss 256.69284\n",
      "iteration 4200 loss 256.69287\n",
      "iteration 4300 loss 256.69287\n",
      "iteration 4400 loss 256.6929\n",
      "iteration 4500 loss 256.69287\n",
      "iteration 4600 loss 256.6929\n",
      "iteration 4700 loss 256.6929\n",
      "iteration 4800 loss 256.69287\n",
      "iteration 4900 loss 256.69287\n",
      "iteration 5000 loss 256.6929Train finished\n",
      "Selected features are: [13 26 37 46 49 51 52 56 62 70]\n",
      "Selected weights are: [0.9996947  0.9996947  0.9996947  0.51463664 0.9996947  0.3708636\n",
      " 0.9996947  0.9996947  0.9996947  0.9996947 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0037191413872884388, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 32.297909164450616\n",
      "\n",
      " 3 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 290.62994\n",
      "iteration 200 loss 284.20078\n",
      "iteration 300 loss 283.39288\n",
      "iteration 400 loss 282.67932\n",
      "iteration 500 loss 282.05383\n",
      "iteration 600 loss 281.49255\n",
      "iteration 700 loss 281.04303\n",
      "iteration 800 loss 280.74146\n",
      "iteration 900 loss 280.49527\n",
      "iteration 1000 loss 280.30676\n",
      "iteration 1100 loss 280.19855\n",
      "iteration 1200 loss 280.14923\n",
      "iteration 1300 loss 280.12247\n",
      "iteration 1400 loss 280.10147\n",
      "iteration 1500 loss 280.08777\n",
      "iteration 1600 loss 280.07703\n",
      "iteration 1700 loss 280.0722\n",
      "iteration 1800 loss 280.06796\n",
      "iteration 1900 loss 280.06415\n",
      "iteration 2000 loss 280.0607\n",
      "iteration 2100 loss 280.0576\n",
      "iteration 2200 loss 280.05475\n",
      "iteration 2300 loss 280.05215\n",
      "iteration 2400 loss 280.0498\n",
      "iteration 2500 loss 280.04764\n",
      "iteration 2600 loss 280.04572\n",
      "iteration 2700 loss 280.0439\n",
      "iteration 2800 loss 280.04224\n",
      "iteration 2900 loss 280.04077\n",
      "iteration 3000 loss 280.03943\n",
      "iteration 3100 loss 280.03815\n",
      "iteration 3200 loss 280.03702\n",
      "iteration 3300 loss 280.03592\n",
      "iteration 3400 loss 280.03497\n",
      "iteration 3500 loss 280.0341\n",
      "iteration 3600 loss 280.03326\n",
      "iteration 3700 loss 280.03253\n",
      "iteration 3800 loss 280.0318\n",
      "iteration 3900 loss 280.0312\n",
      "iteration 4000 loss 280.03058\n",
      "iteration 4100 loss 280.03006\n",
      "iteration 4200 loss 280.0295\n",
      "iteration 4300 loss 280.0291\n",
      "iteration 4400 loss 280.0287\n",
      "iteration 4500 loss 280.02823\n",
      "iteration 4600 loss 280.0279\n",
      "iteration 4700 loss 280.02756\n",
      "iteration 4800 loss 280.02722\n",
      "iteration 4900 loss 280.02692\n",
      "iteration 5000 loss 280.02664Train finished\n",
      "Selected features are: [ 9 14 15 24 40 48 55 58 63 69]\n",
      "Selected weights are: [0.99954283 0.5764279  0.99954283 0.52454597 0.99954283 0.99954283\n",
      " 0.9494175  0.99954283 0.99954283 0.99954283]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0005120861533973017, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 19.466851731264764\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 243.14693\n",
      "iteration 200 loss 235.00189\n",
      "iteration 300 loss 233.70407\n",
      "iteration 400 loss 232.62985\n",
      "iteration 500 loss 231.6264\n",
      "iteration 600 loss 231.04497\n",
      "iteration 700 loss 230.57253\n",
      "iteration 800 loss 230.11612\n",
      "iteration 900 loss 229.94972\n",
      "iteration 1000 loss 229.90475\n",
      "iteration 1100 loss 229.88983\n",
      "iteration 1200 loss 229.87653\n",
      "iteration 1300 loss 229.86832\n",
      "iteration 1400 loss 229.86188\n",
      "iteration 1500 loss 229.85582\n",
      "iteration 1600 loss 229.84996\n",
      "iteration 1700 loss 229.84572\n",
      "iteration 1800 loss 229.84248\n",
      "iteration 1900 loss 229.83945\n",
      "iteration 2000 loss 229.83656\n",
      "iteration 2100 loss 229.83382\n",
      "iteration 2200 loss 229.83112\n",
      "iteration 2300 loss 229.82852\n",
      "iteration 2400 loss 229.82605\n",
      "iteration 2500 loss 229.82417\n",
      "iteration 2600 loss 229.82257\n",
      "iteration 2700 loss 229.82101\n",
      "iteration 2800 loss 229.81955\n",
      "iteration 2900 loss 229.81808\n",
      "iteration 3000 loss 229.81671\n",
      "iteration 3100 loss 229.81567\n",
      "iteration 3200 loss 229.81474\n",
      "iteration 3300 loss 229.81386\n",
      "iteration 3400 loss 229.81299\n",
      "iteration 3500 loss 229.81216\n",
      "iteration 3600 loss 229.81137\n",
      "iteration 3700 loss 229.81055\n",
      "iteration 3800 loss 229.80978\n",
      "iteration 3900 loss 229.80907\n",
      "iteration 4000 loss 229.8085\n",
      "iteration 4100 loss 229.80809\n",
      "iteration 4200 loss 229.80771\n",
      "iteration 4300 loss 229.80728\n",
      "iteration 4400 loss 229.80692\n",
      "iteration 4500 loss 229.80655\n",
      "iteration 4600 loss 229.80621\n",
      "iteration 4700 loss 229.80586\n",
      "iteration 4800 loss 229.80557\n",
      "iteration 4900 loss 229.80528\n",
      "iteration 5000 loss 229.80496Train finished\n",
      "Selected features are: [11 13 14 26 33 46 49 52 56 62]\n",
      "Selected weights are: [0.4016772  0.68968517 0.9998587  0.9998587  0.43462428 0.97185016\n",
      " 0.9998587  0.9998587  0.9998587  0.9998587 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0006144111225437349, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 41.20582160496675\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 303.7978\n",
      "iteration 200 loss 301.00946\n",
      "iteration 300 loss 299.3192\n",
      "iteration 400 loss 298.2782\n",
      "iteration 500 loss 297.37567\n",
      "iteration 600 loss 296.5924\n",
      "iteration 700 loss 296.04767\n",
      "iteration 800 loss 295.85663\n",
      "iteration 900 loss 295.74103\n",
      "iteration 1000 loss 295.64725\n",
      "iteration 1100 loss 295.5825\n",
      "iteration 1200 loss 295.54117\n",
      "iteration 1300 loss 295.50842\n",
      "iteration 1400 loss 295.48563\n",
      "iteration 1500 loss 295.4674\n",
      "iteration 1600 loss 295.4534\n",
      "iteration 1700 loss 295.44345\n",
      "iteration 1800 loss 295.434\n",
      "iteration 1900 loss 295.425\n",
      "iteration 2000 loss 295.41638\n",
      "iteration 2100 loss 295.40805\n",
      "iteration 2200 loss 295.40005\n",
      "iteration 2300 loss 295.3924\n",
      "iteration 2400 loss 295.38492\n",
      "iteration 2500 loss 295.37772\n",
      "iteration 2600 loss 295.3707\n",
      "iteration 2700 loss 295.36383\n",
      "iteration 2800 loss 295.35724\n",
      "iteration 2900 loss 295.35077\n",
      "iteration 3000 loss 295.34448\n",
      "iteration 3100 loss 295.33844\n",
      "iteration 3200 loss 295.33246\n",
      "iteration 3300 loss 295.3288\n",
      "iteration 3400 loss 295.32513\n",
      "iteration 3500 loss 295.3216\n",
      "iteration 3600 loss 295.31815\n",
      "iteration 3700 loss 295.3147\n",
      "iteration 3800 loss 295.31128\n",
      "iteration 3900 loss 295.30792\n",
      "iteration 4000 loss 295.30652\n",
      "iteration 4100 loss 295.3059\n",
      "iteration 4200 loss 295.30542\n",
      "iteration 4300 loss 295.3049\n",
      "iteration 4400 loss 295.30457\n",
      "iteration 4500 loss 295.30423\n",
      "iteration 4600 loss 295.30392\n",
      "iteration 4700 loss 295.30365\n",
      "iteration 4800 loss 295.3034\n",
      "iteration 4900 loss 295.30316\n",
      "iteration 5000 loss 295.30298Train finished\n",
      "Selected features are: [ 2 14 15 40 48 54 55 58 63 69]\n",
      "Selected weights are: [0.9996202  0.49297807 0.9996202  0.9996202  0.9996202  0.7298265\n",
      " 0.9996202  0.9996202  0.9996202  0.9996202 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.000595098036762491, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 13.183448751170598\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 298.0517\n",
      "iteration 200 loss 295.54993\n",
      "iteration 300 loss 293.07388\n",
      "iteration 400 loss 289.95224\n",
      "iteration 500 loss 288.9156\n",
      "iteration 600 loss 288.3667\n",
      "iteration 700 loss 287.88043\n",
      "iteration 800 loss 287.5946\n",
      "iteration 900 loss 287.328\n",
      "iteration 1000 loss 287.08716\n",
      "iteration 1100 loss 286.8871\n",
      "iteration 1200 loss 286.71362\n",
      "iteration 1300 loss 286.561\n",
      "iteration 1400 loss 286.4603\n",
      "iteration 1500 loss 286.40515\n",
      "iteration 1600 loss 286.37012\n",
      "iteration 1700 loss 286.34247\n",
      "iteration 1800 loss 286.32285\n",
      "iteration 1900 loss 286.30548\n",
      "iteration 2000 loss 286.2899\n",
      "iteration 2100 loss 286.2764\n",
      "iteration 2200 loss 286.26712\n",
      "iteration 2300 loss 286.25833\n",
      "iteration 2400 loss 286.24973\n",
      "iteration 2500 loss 286.24124\n",
      "iteration 2600 loss 286.23288\n",
      "iteration 2700 loss 286.22534\n",
      "iteration 2800 loss 286.21854\n",
      "iteration 2900 loss 286.21252\n",
      "iteration 3000 loss 286.20728\n",
      "iteration 3100 loss 286.20227\n",
      "iteration 3200 loss 286.19736\n",
      "iteration 3300 loss 286.1926\n",
      "iteration 3400 loss 286.188\n",
      "iteration 3500 loss 286.18344\n",
      "iteration 3600 loss 286.17905\n",
      "iteration 3700 loss 286.17477\n",
      "iteration 3800 loss 286.17072\n",
      "iteration 3900 loss 286.1669\n",
      "iteration 4000 loss 286.16315\n",
      "iteration 4100 loss 286.1595\n",
      "iteration 4200 loss 286.15588\n",
      "iteration 4300 loss 286.15436\n",
      "iteration 4400 loss 286.1527\n",
      "iteration 4500 loss 286.15112\n",
      "iteration 4600 loss 286.1496\n",
      "iteration 4700 loss 286.1481\n",
      "iteration 4800 loss 286.14664\n",
      "iteration 4900 loss 286.1452\n",
      "iteration 5000 loss 286.1438Train finished\n",
      "Selected features are: [ 2 33 36 48 51 54 55 56 58 63]\n",
      "Selected weights are: [0.4562936  0.99971586 0.99971586 0.50027794 0.99971586 0.99971586\n",
      " 0.6972993  0.99971586 0.99971586 0.8397416 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0034080066408065553, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 19.048565687934076\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 216.5166\n",
      "iteration 200 loss 214.43553\n",
      "iteration 300 loss 212.54233\n",
      "iteration 400 loss 210.7861\n",
      "iteration 500 loss 209.49448\n",
      "iteration 600 loss 209.02272\n",
      "iteration 700 loss 208.66974\n",
      "iteration 800 loss 208.35721\n",
      "iteration 900 loss 208.10747\n",
      "iteration 1000 loss 207.96753\n",
      "iteration 1100 loss 207.88095\n",
      "iteration 1200 loss 207.8276\n",
      "iteration 1300 loss 207.79999\n",
      "iteration 1400 loss 207.78297\n",
      "iteration 1500 loss 207.77158\n",
      "iteration 1600 loss 207.76305\n",
      "iteration 1700 loss 207.76096\n",
      "iteration 1800 loss 207.75931\n",
      "iteration 1900 loss 207.75778\n",
      "iteration 2000 loss 207.75629\n",
      "iteration 2100 loss 207.7546\n",
      "iteration 2200 loss 207.75275\n",
      "iteration 2300 loss 207.7504\n",
      "iteration 2400 loss 207.74731\n",
      "iteration 2500 loss 207.74295\n",
      "iteration 2600 loss 207.73651\n",
      "iteration 2700 loss 207.72725\n",
      "iteration 2800 loss 207.71555\n",
      "iteration 2900 loss 207.70529\n",
      "iteration 3000 loss 207.70102\n",
      "iteration 3100 loss 207.69841\n",
      "iteration 3200 loss 207.6965\n",
      "iteration 3300 loss 207.69485\n",
      "iteration 3400 loss 207.69344\n",
      "iteration 3500 loss 207.69217\n",
      "iteration 3600 loss 207.69098\n",
      "iteration 3700 loss 207.68994\n",
      "iteration 3800 loss 207.68898\n",
      "iteration 3900 loss 207.68811\n",
      "iteration 4000 loss 207.68732\n",
      "iteration 4100 loss 207.68658\n",
      "iteration 4200 loss 207.68593\n",
      "iteration 4300 loss 207.68535\n",
      "iteration 4400 loss 207.68478\n",
      "iteration 4500 loss 207.68428\n",
      "iteration 4600 loss 207.68382\n",
      "iteration 4700 loss 207.68343\n",
      "iteration 4800 loss 207.68301\n",
      "iteration 4900 loss 207.68268\n",
      "iteration 5000 loss 207.68234Train finished\n",
      "Selected features are: [14 15 33 40 48 51 54 58 63 69]\n",
      "Selected weights are: [0.49443442 0.99957204 0.99957204 0.649458   0.99957204 0.99957204\n",
      " 0.99957204 0.99957204 0.99957204 0.8758533 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.000543571171805011, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 52.088973971909525\n",
      "\n",
      " 4 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 231.91791\n",
      "iteration 200 loss 229.6431\n",
      "iteration 300 loss 227.69997\n",
      "iteration 400 loss 226.89085\n",
      "iteration 500 loss 224.69762\n",
      "iteration 600 loss 223.39273\n",
      "iteration 700 loss 222.90579\n",
      "iteration 800 loss 222.56876\n",
      "iteration 900 loss 222.33403\n",
      "iteration 1000 loss 222.21222\n",
      "iteration 1100 loss 222.094\n",
      "iteration 1200 loss 221.9761\n",
      "iteration 1300 loss 221.85553\n",
      "iteration 1400 loss 221.73038\n",
      "iteration 1500 loss 221.59882\n",
      "iteration 1600 loss 221.46724\n",
      "iteration 1700 loss 221.34953\n",
      "iteration 1800 loss 221.26183\n",
      "iteration 1900 loss 221.18893\n",
      "iteration 2000 loss 221.12723\n",
      "iteration 2100 loss 220.99295\n",
      "iteration 2200 loss 220.19724\n",
      "iteration 2300 loss 219.8405\n",
      "iteration 2400 loss 219.81854\n",
      "iteration 2500 loss 219.80486\n",
      "iteration 2600 loss 219.79446\n",
      "iteration 2700 loss 219.7859\n",
      "iteration 2800 loss 219.77849\n",
      "iteration 2900 loss 219.77187\n",
      "iteration 3000 loss 219.76572\n",
      "iteration 3100 loss 219.76004\n",
      "iteration 3200 loss 219.75464\n",
      "iteration 3300 loss 219.74953\n",
      "iteration 3400 loss 219.7446\n",
      "iteration 3500 loss 219.73985\n",
      "iteration 3600 loss 219.73524\n",
      "iteration 3700 loss 219.7308\n",
      "iteration 3800 loss 219.72647\n",
      "iteration 3900 loss 219.72221\n",
      "iteration 4000 loss 219.71812\n",
      "iteration 4100 loss 219.71408\n",
      "iteration 4200 loss 219.71014\n",
      "iteration 4300 loss 219.70624\n",
      "iteration 4400 loss 219.70247\n",
      "iteration 4500 loss 219.69872\n",
      "iteration 4600 loss 219.69504\n",
      "iteration 4700 loss 219.69147\n",
      "iteration 4800 loss 219.68793\n",
      "iteration 4900 loss 219.68474\n",
      "iteration 5000 loss 219.68192Train finished\n",
      "Selected features are: [11 13 14 26 37 44 46 49 52 70]\n",
      "Selected weights are: [0.9996636  0.81945586 0.9996636  0.9996636  0.77718216 0.52041936\n",
      " 0.8048308  0.9996636  0.9996636  0.9996636 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0008311739175824145, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 48.21520304038323\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 311.1148\n",
      "iteration 200 loss 308.03925\n",
      "iteration 300 loss 306.53302\n",
      "iteration 400 loss 305.33163\n",
      "iteration 500 loss 304.19507\n",
      "iteration 600 loss 302.8805\n",
      "iteration 700 loss 300.6168\n",
      "iteration 800 loss 299.48532\n",
      "iteration 900 loss 299.34818\n",
      "iteration 1000 loss 299.22937\n",
      "iteration 1100 loss 299.1413\n",
      "iteration 1200 loss 299.09467\n",
      "iteration 1300 loss 299.06375\n",
      "iteration 1400 loss 299.04062\n",
      "iteration 1500 loss 299.0246\n",
      "iteration 1600 loss 299.00952\n",
      "iteration 1700 loss 298.99734\n",
      "iteration 1800 loss 298.98648\n",
      "iteration 1900 loss 298.9761\n",
      "iteration 2000 loss 298.96606\n",
      "iteration 2100 loss 298.9596\n",
      "iteration 2200 loss 298.9538\n",
      "iteration 2300 loss 298.9483\n",
      "iteration 2400 loss 298.94305\n",
      "iteration 2500 loss 298.9381\n",
      "iteration 2600 loss 298.9333\n",
      "iteration 2700 loss 298.92865\n",
      "iteration 2800 loss 298.92438\n",
      "iteration 2900 loss 298.92017\n",
      "iteration 3000 loss 298.91608\n",
      "iteration 3100 loss 298.9121\n",
      "iteration 3200 loss 298.90826\n",
      "iteration 3300 loss 298.90448\n",
      "iteration 3400 loss 298.90082\n",
      "iteration 3500 loss 298.89716\n",
      "iteration 3600 loss 298.89365\n",
      "iteration 3700 loss 298.89014\n",
      "iteration 3800 loss 298.88678\n",
      "iteration 3900 loss 298.88342\n",
      "iteration 4000 loss 298.88037\n",
      "iteration 4100 loss 298.87732\n",
      "iteration 4200 loss 298.87433\n",
      "iteration 4300 loss 298.87134\n",
      "iteration 4400 loss 298.86844\n",
      "iteration 4500 loss 298.86563\n",
      "iteration 4600 loss 298.86288\n",
      "iteration 4700 loss 298.86017\n",
      "iteration 4800 loss 298.85748\n",
      "iteration 4900 loss 298.85474\n",
      "iteration 5000 loss 298.85205Train finished\n",
      "Selected features are: [15 33 37 46 51 54 56 58 62 63]\n",
      "Selected weights are: [0.9997126  0.9997126  0.86201614 0.9997126  0.9997126  0.9997126\n",
      " 0.9997126  0.58395034 0.8324484  0.7705304 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0011720880531278272, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 13.267426488793106\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 276.91107\n",
      "iteration 200 loss 272.56146\n",
      "iteration 300 loss 270.86536\n",
      "iteration 400 loss 270.17578\n",
      "iteration 500 loss 269.76074\n",
      "iteration 600 loss 269.40765\n",
      "iteration 700 loss 269.0149\n",
      "iteration 800 loss 268.5912\n",
      "iteration 900 loss 268.16284\n",
      "iteration 1000 loss 267.83353\n",
      "iteration 1100 loss 267.6406\n",
      "iteration 1200 loss 267.50174\n",
      "iteration 1300 loss 267.40173\n",
      "iteration 1400 loss 267.3384\n",
      "iteration 1500 loss 267.30164\n",
      "iteration 1600 loss 267.26892\n",
      "iteration 1700 loss 267.23914\n",
      "iteration 1800 loss 267.2118\n",
      "iteration 1900 loss 267.18634\n",
      "iteration 2000 loss 267.1626\n",
      "iteration 2100 loss 267.1402\n",
      "iteration 2200 loss 267.12067\n",
      "iteration 2300 loss 267.10962\n",
      "iteration 2400 loss 267.09888\n",
      "iteration 2500 loss 267.08844\n",
      "iteration 2600 loss 267.07834\n",
      "iteration 2700 loss 267.06848\n",
      "iteration 2800 loss 267.05896\n",
      "iteration 2900 loss 267.04968\n",
      "iteration 3000 loss 267.04428\n",
      "iteration 3100 loss 267.0398\n",
      "iteration 3200 loss 267.0354\n",
      "iteration 3300 loss 267.03113\n",
      "iteration 3400 loss 267.02692\n",
      "iteration 3500 loss 267.02274\n",
      "iteration 3600 loss 267.0201\n",
      "iteration 3700 loss 267.01935\n",
      "iteration 3800 loss 267.01862\n",
      "iteration 3900 loss 267.01794\n",
      "iteration 4000 loss 267.01733\n",
      "iteration 4100 loss 267.01672\n",
      "iteration 4200 loss 267.01617\n",
      "iteration 4300 loss 267.01572\n",
      "iteration 4400 loss 267.0152\n",
      "iteration 4500 loss 267.0147\n",
      "iteration 4600 loss 267.01428\n",
      "iteration 4700 loss 267.01385\n",
      "iteration 4800 loss 267.0135\n",
      "iteration 4900 loss 267.0131\n",
      "iteration 5000 loss 267.01276Train finished\n",
      "Selected features are: [ 8 11 26 33 37 46 49 52 56 62]\n",
      "Selected weights are: [0.84322065 0.9065892  0.9992427  0.79076135 0.96495295 0.9992427\n",
      " 0.9992427  0.72692025 0.9992427  0.9992427 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.001225607021040319, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 27.384449321129704\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 286.71024\n",
      "iteration 200 loss 284.29565\n",
      "iteration 300 loss 282.43164\n",
      "iteration 400 loss 281.98715\n",
      "iteration 500 loss 281.719\n",
      "iteration 600 loss 281.49094\n",
      "iteration 700 loss 281.28934\n",
      "iteration 800 loss 281.105\n",
      "iteration 900 loss 280.93195\n",
      "iteration 1000 loss 280.81412\n",
      "iteration 1100 loss 280.77835\n",
      "iteration 1200 loss 280.74384\n",
      "iteration 1300 loss 280.71002\n",
      "iteration 1400 loss 280.67657\n",
      "iteration 1500 loss 280.64386\n",
      "iteration 1600 loss 280.61203\n",
      "iteration 1700 loss 280.58075\n",
      "iteration 1800 loss 280.55057\n",
      "iteration 1900 loss 280.52448\n",
      "iteration 2000 loss 280.5031\n",
      "iteration 2100 loss 280.48398\n",
      "iteration 2200 loss 280.46594\n",
      "iteration 2300 loss 280.44907\n",
      "iteration 2400 loss 280.43347\n",
      "iteration 2500 loss 280.4182\n",
      "iteration 2600 loss 280.4043\n",
      "iteration 2700 loss 280.39307\n",
      "iteration 2800 loss 280.38214\n",
      "iteration 2900 loss 280.37445\n",
      "iteration 3000 loss 280.36758\n",
      "iteration 3100 loss 280.3611\n",
      "iteration 3200 loss 280.3548\n",
      "iteration 3300 loss 280.34863\n",
      "iteration 3400 loss 280.34262\n",
      "iteration 3500 loss 280.3375\n",
      "iteration 3600 loss 280.33356\n",
      "iteration 3700 loss 280.32983\n",
      "iteration 3800 loss 280.32608\n",
      "iteration 3900 loss 280.3236\n",
      "iteration 4000 loss 280.32263\n",
      "iteration 4100 loss 280.32184\n",
      "iteration 4200 loss 280.32117\n",
      "iteration 4300 loss 280.32056\n",
      "iteration 4400 loss 280.32007\n",
      "iteration 4500 loss 280.31958\n",
      "iteration 4600 loss 280.31906\n",
      "iteration 4700 loss 280.31863\n",
      "iteration 4800 loss 280.31824\n",
      "iteration 4900 loss 280.31784\n",
      "iteration 5000 loss 280.31747Train finished\n",
      "Selected features are: [ 2  9 14 15 24 40 48 55 58 63]\n",
      "Selected weights are: [0.9023937  0.92362565 0.99971974 0.99971974 0.7523249  0.7507767\n",
      " 0.99971974 0.49978656 0.99971974 0.99971974]\n",
      "Best params obtained by cv: {'alpha': 0.001, 'coef0': 1, 'degree': 3, 'gamma': 0.0006982650287167409, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 19.893930370086306\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 249.09338\n",
      "iteration 200 loss 243.56096\n",
      "iteration 300 loss 242.93237\n",
      "iteration 400 loss 242.4487\n",
      "iteration 500 loss 242.0566\n",
      "iteration 600 loss 241.7048\n",
      "iteration 700 loss 241.38524\n",
      "iteration 800 loss 241.24387\n",
      "iteration 900 loss 241.11182\n",
      "iteration 1000 loss 240.98619\n",
      "iteration 1100 loss 240.87218\n",
      "iteration 1200 loss 240.79167\n",
      "iteration 1300 loss 240.73418\n",
      "iteration 1400 loss 240.68465\n",
      "iteration 1500 loss 240.64005\n",
      "iteration 1600 loss 240.60406\n",
      "iteration 1700 loss 240.57341\n",
      "iteration 1800 loss 240.54811\n",
      "iteration 1900 loss 240.53091\n",
      "iteration 2000 loss 240.52196\n",
      "iteration 2100 loss 240.51392\n",
      "iteration 2200 loss 240.50671\n",
      "iteration 2300 loss 240.50029\n",
      "iteration 2400 loss 240.49448\n",
      "iteration 2500 loss 240.48926\n",
      "iteration 2600 loss 240.48454\n",
      "iteration 2700 loss 240.48033\n",
      "iteration 2800 loss 240.47647\n",
      "iteration 2900 loss 240.47296\n",
      "iteration 3000 loss 240.4698\n",
      "iteration 3100 loss 240.46693\n",
      "iteration 3200 loss 240.46431\n",
      "iteration 3300 loss 240.46188\n",
      "iteration 3400 loss 240.45966\n",
      "iteration 3500 loss 240.45767\n",
      "iteration 3600 loss 240.45587\n",
      "iteration 3700 loss 240.45435\n",
      "iteration 3800 loss 240.45294\n",
      "iteration 3900 loss 240.45163\n",
      "iteration 4000 loss 240.45093\n",
      "iteration 4100 loss 240.45038\n",
      "iteration 4200 loss 240.44989\n",
      "iteration 4300 loss 240.44948\n",
      "iteration 4400 loss 240.44907\n",
      "iteration 4500 loss 240.44867\n",
      "iteration 4600 loss 240.44832\n",
      "iteration 4700 loss 240.44794\n",
      "iteration 4800 loss 240.44785\n",
      "iteration 4900 loss 240.44777\n",
      "iteration 5000 loss 240.44772Train finished\n",
      "Selected features are: [ 2  9 14 15 24 40 48 55 63 69]\n",
      "Selected weights are: [0.99951744 0.99951744 0.49474463 0.99951744 0.7086711  0.99951744\n",
      " 0.99951744 0.83385694 0.99951744 0.62708676]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0006157482767856141, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 34.53707133955385\n",
      "\n",
      " 5 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 277.351\n",
      "iteration 200 loss 274.22656\n",
      "iteration 300 loss 273.35867\n",
      "iteration 400 loss 272.60178\n",
      "iteration 500 loss 271.92468\n",
      "iteration 600 loss 271.26163\n",
      "iteration 700 loss 270.706\n",
      "iteration 800 loss 270.4464\n",
      "iteration 900 loss 270.21097\n",
      "iteration 1000 loss 269.99536\n",
      "iteration 1100 loss 269.852\n",
      "iteration 1200 loss 269.7583\n",
      "iteration 1300 loss 269.68793\n",
      "iteration 1400 loss 269.63892\n",
      "iteration 1500 loss 269.59546\n",
      "iteration 1600 loss 269.55856\n",
      "iteration 1700 loss 269.54416\n",
      "iteration 1800 loss 269.5341\n",
      "iteration 1900 loss 269.5241\n",
      "iteration 2000 loss 269.51437\n",
      "iteration 2100 loss 269.50833\n",
      "iteration 2200 loss 269.50247\n",
      "iteration 2300 loss 269.4968\n",
      "iteration 2400 loss 269.49124\n",
      "iteration 2500 loss 269.48575\n",
      "iteration 2600 loss 269.48035\n",
      "iteration 2700 loss 269.47498\n",
      "iteration 2800 loss 269.46973\n",
      "iteration 2900 loss 269.46445\n",
      "iteration 3000 loss 269.45923\n",
      "iteration 3100 loss 269.454\n",
      "iteration 3200 loss 269.44888\n",
      "iteration 3300 loss 269.44373\n",
      "iteration 3400 loss 269.43863\n",
      "iteration 3500 loss 269.43356\n",
      "iteration 3600 loss 269.42844\n",
      "iteration 3700 loss 269.4234\n",
      "iteration 3800 loss 269.42004\n",
      "iteration 3900 loss 269.41895\n",
      "iteration 4000 loss 269.41797\n",
      "iteration 4100 loss 269.41705\n",
      "iteration 4200 loss 269.4162\n",
      "iteration 4300 loss 269.4154\n",
      "iteration 4400 loss 269.4147\n",
      "iteration 4500 loss 269.41397\n",
      "iteration 4600 loss 269.41333\n",
      "iteration 4700 loss 269.4127\n",
      "iteration 4800 loss 269.41208\n",
      "iteration 4900 loss 269.41147\n",
      "iteration 5000 loss 269.41104Train finished\n",
      "Selected features are: [ 8 11 13 26 33 44 46 49 52 56]\n",
      "Selected weights are: [0.91294473 0.6168647  0.9322424  0.99967    0.5449463  0.99967\n",
      " 0.99967    0.99967    0.99967    0.99967   ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0011026722770279117, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 27.930344855753276\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 323.55298\n",
      "iteration 200 loss 315.047\n",
      "iteration 300 loss 310.65628\n",
      "iteration 400 loss 309.322\n",
      "iteration 500 loss 309.16125\n",
      "iteration 600 loss 309.03214\n",
      "iteration 700 loss 308.91675\n",
      "iteration 800 loss 308.80615\n",
      "iteration 900 loss 308.69586\n",
      "iteration 1000 loss 308.58582\n",
      "iteration 1100 loss 308.4779\n",
      "iteration 1200 loss 308.37134\n",
      "iteration 1300 loss 308.26483\n",
      "iteration 1400 loss 308.159\n",
      "iteration 1500 loss 308.05594\n",
      "iteration 1600 loss 307.973\n",
      "iteration 1700 loss 307.90714\n",
      "iteration 1800 loss 307.8659\n",
      "iteration 1900 loss 307.84448\n",
      "iteration 2000 loss 307.82617\n",
      "iteration 2100 loss 307.8123\n",
      "iteration 2200 loss 307.79865\n",
      "iteration 2300 loss 307.78787\n",
      "iteration 2400 loss 307.77814\n",
      "iteration 2500 loss 307.7715\n",
      "iteration 2600 loss 307.7652\n",
      "iteration 2700 loss 307.75903\n",
      "iteration 2800 loss 307.753\n",
      "iteration 2900 loss 307.747\n",
      "iteration 3000 loss 307.7432\n",
      "iteration 3100 loss 307.73935\n",
      "iteration 3200 loss 307.7355\n",
      "iteration 3300 loss 307.73175\n",
      "iteration 3400 loss 307.7279\n",
      "iteration 3500 loss 307.72406\n",
      "iteration 3600 loss 307.72015\n",
      "iteration 3700 loss 307.71616\n",
      "iteration 3800 loss 307.71216\n",
      "iteration 3900 loss 307.708\n",
      "iteration 4000 loss 307.7038\n",
      "iteration 4100 loss 307.70068\n",
      "iteration 4200 loss 307.698\n",
      "iteration 4300 loss 307.6954\n",
      "iteration 4400 loss 307.69275\n",
      "iteration 4500 loss 307.69012\n",
      "iteration 4600 loss 307.6874\n",
      "iteration 4700 loss 307.68472\n",
      "iteration 4800 loss 307.68195\n",
      "iteration 4900 loss 307.6791\n",
      "iteration 5000 loss 307.67627Train finished\n",
      "Selected features are: [14 15 33 48 51 54 56 58 62 63]\n",
      "Selected weights are: [0.99972045 0.99972045 0.99972045 0.7743376  0.99972045 0.6674282\n",
      " 0.99972045 0.61517054 0.7128212  0.99972045]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.000536797029168733, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 11.547321514313778\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 215.89272\n",
      "iteration 200 loss 212.49777\n",
      "iteration 300 loss 210.7977\n",
      "iteration 400 loss 210.06926\n",
      "iteration 500 loss 209.47884\n",
      "iteration 600 loss 208.9776\n",
      "iteration 700 loss 208.6417\n",
      "iteration 800 loss 208.40742\n",
      "iteration 900 loss 208.19702\n",
      "iteration 1000 loss 208.04886\n",
      "iteration 1100 loss 207.94002\n",
      "iteration 1200 loss 207.86415\n",
      "iteration 1300 loss 207.81516\n",
      "iteration 1400 loss 207.77997\n",
      "iteration 1500 loss 207.74956\n",
      "iteration 1600 loss 207.72054\n",
      "iteration 1700 loss 207.69493\n",
      "iteration 1800 loss 207.67474\n",
      "iteration 1900 loss 207.66005\n",
      "iteration 2000 loss 207.6503\n",
      "iteration 2100 loss 207.64555\n",
      "iteration 2200 loss 207.64111\n",
      "iteration 2300 loss 207.63687\n",
      "iteration 2400 loss 207.63284\n",
      "iteration 2500 loss 207.62904\n",
      "iteration 2600 loss 207.62654\n",
      "iteration 2700 loss 207.62427\n",
      "iteration 2800 loss 207.62204\n",
      "iteration 2900 loss 207.6199\n",
      "iteration 3000 loss 207.6178\n",
      "iteration 3100 loss 207.61568\n",
      "iteration 3200 loss 207.61357\n",
      "iteration 3300 loss 207.61145\n",
      "iteration 3400 loss 207.60968\n",
      "iteration 3500 loss 207.60938\n",
      "iteration 3600 loss 207.60907\n",
      "iteration 3700 loss 207.60881\n",
      "iteration 3800 loss 207.60861\n",
      "iteration 3900 loss 207.60837\n",
      "iteration 4000 loss 207.6082\n",
      "iteration 4100 loss 207.60799\n",
      "iteration 4200 loss 207.6078\n",
      "iteration 4300 loss 207.60767\n",
      "iteration 4400 loss 207.60748\n",
      "iteration 4500 loss 207.60733\n",
      "iteration 4600 loss 207.60718\n",
      "iteration 4700 loss 207.607\n",
      "iteration 4800 loss 207.60684\n",
      "iteration 4900 loss 207.60674\n",
      "iteration 5000 loss 207.60657Train finished\n",
      "Selected features are: [ 2  9 14 15 33 40 48 54 58 63]\n",
      "Selected weights are: [0.5439296  0.99965024 0.5794642  0.99965024 0.6861454  0.8671767\n",
      " 0.99965024 0.99965024 0.99965024 0.99965024]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.000509480246887305, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 51.68202177015944\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 271.2061\n",
      "iteration 200 loss 267.9474\n",
      "iteration 300 loss 266.73846\n",
      "iteration 400 loss 265.82635\n",
      "iteration 500 loss 264.96515\n",
      "iteration 600 loss 264.39612\n",
      "iteration 700 loss 263.71066\n",
      "iteration 800 loss 262.94928\n",
      "iteration 900 loss 262.24808\n",
      "iteration 1000 loss 261.88644\n",
      "iteration 1100 loss 261.81024\n",
      "iteration 1200 loss 261.74915\n",
      "iteration 1300 loss 261.6955\n",
      "iteration 1400 loss 261.64868\n",
      "iteration 1500 loss 261.6071\n",
      "iteration 1600 loss 261.56903\n",
      "iteration 1700 loss 261.5334\n",
      "iteration 1800 loss 261.51703\n",
      "iteration 1900 loss 261.51285\n",
      "iteration 2000 loss 261.50925\n",
      "iteration 2100 loss 261.50592\n",
      "iteration 2200 loss 261.50275\n",
      "iteration 2300 loss 261.49973\n",
      "iteration 2400 loss 261.4968\n",
      "iteration 2500 loss 261.49442\n",
      "iteration 2600 loss 261.49207\n",
      "iteration 2700 loss 261.48993\n",
      "iteration 2800 loss 261.4878\n",
      "iteration 2900 loss 261.4862\n",
      "iteration 3000 loss 261.48492\n",
      "iteration 3100 loss 261.48376\n",
      "iteration 3200 loss 261.48267\n",
      "iteration 3300 loss 261.48163\n",
      "iteration 3400 loss 261.4806\n",
      "iteration 3500 loss 261.47968\n",
      "iteration 3600 loss 261.4787\n",
      "iteration 3700 loss 261.47778\n",
      "iteration 3800 loss 261.47687\n",
      "iteration 3900 loss 261.47598\n",
      "iteration 4000 loss 261.47513\n",
      "iteration 4100 loss 261.47424\n",
      "iteration 4200 loss 261.47345\n",
      "iteration 4300 loss 261.47263\n",
      "iteration 4400 loss 261.4718\n",
      "iteration 4500 loss 261.471\n",
      "iteration 4600 loss 261.4702\n",
      "iteration 4700 loss 261.46942\n",
      "iteration 4800 loss 261.4687\n",
      "iteration 4900 loss 261.46793\n",
      "iteration 5000 loss 261.4672Train finished\n",
      "Selected features are: [20 26 33 37 46 49 52 56 62 70]\n",
      "Selected weights are: [0.7114747  0.9997778  0.44789588 0.9997778  0.75318164 0.9997778\n",
      " 0.9997778  0.9997778  0.9997778  0.9997778 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.004322745301392678, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 29.8886029921788\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 279.8039\n",
      "iteration 200 loss 276.67075\n",
      "iteration 300 loss 275.5751\n",
      "iteration 400 loss 274.90216\n",
      "iteration 500 loss 274.4366\n",
      "iteration 600 loss 274.0045\n",
      "iteration 700 loss 273.60648\n",
      "iteration 800 loss 273.24185\n",
      "iteration 900 loss 272.98334\n",
      "iteration 1000 loss 272.8397\n",
      "iteration 1100 loss 272.7185\n",
      "iteration 1200 loss 272.61465\n",
      "iteration 1300 loss 272.5377\n",
      "iteration 1400 loss 272.47693\n",
      "iteration 1500 loss 272.42883\n",
      "iteration 1600 loss 272.39227\n",
      "iteration 1700 loss 272.36545\n",
      "iteration 1800 loss 272.3421\n",
      "iteration 1900 loss 272.32712\n",
      "iteration 2000 loss 272.31372\n",
      "iteration 2100 loss 272.3014\n",
      "iteration 2200 loss 272.28986\n",
      "iteration 2300 loss 272.27884\n",
      "iteration 2400 loss 272.26825\n",
      "iteration 2500 loss 272.25955\n",
      "iteration 2600 loss 272.25586\n",
      "iteration 2700 loss 272.25244\n",
      "iteration 2800 loss 272.24924\n",
      "iteration 2900 loss 272.24835\n",
      "iteration 3000 loss 272.2483\n",
      "iteration 3100 loss 272.24823\n",
      "iteration 3200 loss 272.24814\n",
      "iteration 3300 loss 272.2481\n",
      "iteration 3400 loss 272.2481\n",
      "iteration 3500 loss 272.24805\n",
      "iteration 3600 loss 272.24805\n",
      "iteration 3700 loss 272.24805\n",
      "iteration 3800 loss 272.24805\n",
      "iteration 3900 loss 272.248\n",
      "iteration 4000 loss 272.248\n",
      "iteration 4100 loss 272.24802\n",
      "iteration 4200 loss 272.248\n",
      "iteration 4300 loss 272.24802\n",
      "iteration 4400 loss 272.24802\n",
      "iteration 4500 loss 272.248\n",
      "iteration 4600 loss 272.24805\n",
      "iteration 4700 loss 272.24805\n",
      "iteration 4800 loss 272.24805\n",
      "iteration 4900 loss 272.248\n",
      "iteration 5000 loss 272.24805Train finished\n",
      "Selected features are: [ 2  9 15 40 48 54 55 58 63 69]\n",
      "Selected weights are: [0.5115589  0.9995717  0.9995717  0.9995717  0.9995717  0.50409937\n",
      " 0.9995717  0.9995717  0.9995717  0.90353715]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0011930102270064503, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 23.496671668807068\n",
      "\n",
      " 6 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 240.89143\n",
      "iteration 200 loss 236.7156\n",
      "iteration 300 loss 227.46884\n",
      "iteration 400 loss 224.91403\n",
      "iteration 500 loss 224.3365\n",
      "iteration 600 loss 223.9017\n",
      "iteration 700 loss 223.5078\n",
      "iteration 800 loss 223.13126\n",
      "iteration 900 loss 222.75737\n",
      "iteration 1000 loss 222.3922\n",
      "iteration 1100 loss 222.06013\n",
      "iteration 1200 loss 221.96042\n",
      "iteration 1300 loss 221.89572\n",
      "iteration 1400 loss 221.8537\n",
      "iteration 1500 loss 221.82643\n",
      "iteration 1600 loss 221.80235\n",
      "iteration 1700 loss 221.78954\n",
      "iteration 1800 loss 221.7817\n",
      "iteration 1900 loss 221.7745\n",
      "iteration 2000 loss 221.76778\n",
      "iteration 2100 loss 221.76144\n",
      "iteration 2200 loss 221.75546\n",
      "iteration 2300 loss 221.74974\n",
      "iteration 2400 loss 221.74431\n",
      "iteration 2500 loss 221.73914\n",
      "iteration 2600 loss 221.73413\n",
      "iteration 2700 loss 221.72937\n",
      "iteration 2800 loss 221.72478\n",
      "iteration 2900 loss 221.72038\n",
      "iteration 3000 loss 221.7163\n",
      "iteration 3100 loss 221.71315\n",
      "iteration 3200 loss 221.70999\n",
      "iteration 3300 loss 221.70697\n",
      "iteration 3400 loss 221.70404\n",
      "iteration 3500 loss 221.70116\n",
      "iteration 3600 loss 221.69922\n",
      "iteration 3700 loss 221.69765\n",
      "iteration 3800 loss 221.69653\n",
      "iteration 3900 loss 221.69608\n",
      "iteration 4000 loss 221.69568\n",
      "iteration 4100 loss 221.69534\n",
      "iteration 4200 loss 221.69499\n",
      "iteration 4300 loss 221.6947\n",
      "iteration 4400 loss 221.6944\n",
      "iteration 4500 loss 221.69412\n",
      "iteration 4600 loss 221.69386\n",
      "iteration 4700 loss 221.69363\n",
      "iteration 4800 loss 221.69342\n",
      "iteration 4900 loss 221.69319\n",
      "iteration 5000 loss 221.69298Train finished\n",
      "Selected features are: [33 37 40 48 51 54 56 58 63 69]\n",
      "Selected weights are: [0.99959755 0.99959755 0.89944655 0.5250629  0.99959755 0.99959755\n",
      " 0.99959755 0.99959755 0.7101591  0.75162774]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0023166013640227427, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 43.88854719137529\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 269.69806\n",
      "iteration 200 loss 264.67914\n",
      "iteration 300 loss 262.4565\n",
      "iteration 400 loss 260.88046\n",
      "iteration 500 loss 260.01074\n",
      "iteration 600 loss 259.1859\n",
      "iteration 700 loss 258.64233\n",
      "iteration 800 loss 258.32132\n",
      "iteration 900 loss 258.12274\n",
      "iteration 1000 loss 257.96085\n",
      "iteration 1100 loss 257.9215\n",
      "iteration 1200 loss 257.88422\n",
      "iteration 1300 loss 257.8482\n",
      "iteration 1400 loss 257.82172\n",
      "iteration 1500 loss 257.82074\n",
      "iteration 1600 loss 257.8204\n",
      "iteration 1700 loss 257.82016\n",
      "iteration 1800 loss 257.82\n",
      "iteration 1900 loss 257.81985\n",
      "iteration 2000 loss 257.81976\n",
      "iteration 2100 loss 257.81964\n",
      "iteration 2200 loss 257.81958\n",
      "iteration 2300 loss 257.8195\n",
      "iteration 2400 loss 257.81946\n",
      "iteration 2500 loss 257.8194\n",
      "iteration 2600 loss 257.8194\n",
      "iteration 2700 loss 257.81934\n",
      "iteration 2800 loss 257.8193\n",
      "iteration 2900 loss 257.81927\n",
      "iteration 3000 loss 257.81927\n",
      "iteration 3100 loss 257.81927\n",
      "iteration 3200 loss 257.81924\n",
      "iteration 3300 loss 257.8192\n",
      "iteration 3400 loss 257.8192\n",
      "iteration 3500 loss 257.8192\n",
      "iteration 3600 loss 257.8192\n",
      "iteration 3700 loss 257.81915\n",
      "iteration 3800 loss 257.8192\n",
      "iteration 3900 loss 257.81918\n",
      "iteration 4000 loss 257.81915\n",
      "iteration 4100 loss 257.81915\n",
      "iteration 4200 loss 257.81915\n",
      "iteration 4300 loss 257.81915\n",
      "iteration 4400 loss 257.81918\n",
      "iteration 4500 loss 257.81915\n",
      "iteration 4600 loss 257.81918\n",
      "iteration 4700 loss 257.81912\n",
      "iteration 4800 loss 257.81915\n",
      "iteration 4900 loss 257.81912\n",
      "iteration 5000 loss 257.81918Train finished\n",
      "Selected features are: [11 26 33 37 46 49 52 56 62 70]\n",
      "Selected weights are: [0.6934755  0.9995424  0.60733074 0.9995424  0.84826314 0.9995424\n",
      " 0.9995424  0.9995424  0.9995424  0.9995424 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.001584347112621184, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 29.725907199972532\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 255.25288\n",
      "iteration 200 loss 252.71138\n",
      "iteration 300 loss 251.36652\n",
      "iteration 400 loss 250.44135\n",
      "iteration 500 loss 249.81882\n",
      "iteration 600 loss 249.25436\n",
      "iteration 700 loss 248.73842\n",
      "iteration 800 loss 248.39368\n",
      "iteration 900 loss 248.13165\n",
      "iteration 1000 loss 247.97427\n",
      "iteration 1100 loss 247.87292\n",
      "iteration 1200 loss 247.77927\n",
      "iteration 1300 loss 247.7041\n",
      "iteration 1400 loss 247.65129\n",
      "iteration 1500 loss 247.60666\n",
      "iteration 1600 loss 247.58093\n",
      "iteration 1700 loss 247.56575\n",
      "iteration 1800 loss 247.5511\n",
      "iteration 1900 loss 247.53802\n",
      "iteration 2000 loss 247.52629\n",
      "iteration 2100 loss 247.51495\n",
      "iteration 2200 loss 247.50403\n",
      "iteration 2300 loss 247.49341\n",
      "iteration 2400 loss 247.4859\n",
      "iteration 2500 loss 247.48058\n",
      "iteration 2600 loss 247.47577\n",
      "iteration 2700 loss 247.47125\n",
      "iteration 2800 loss 247.46687\n",
      "iteration 2900 loss 247.4627\n",
      "iteration 3000 loss 247.45868\n",
      "iteration 3100 loss 247.45476\n",
      "iteration 3200 loss 247.45145\n",
      "iteration 3300 loss 247.44812\n",
      "iteration 3400 loss 247.44516\n",
      "iteration 3500 loss 247.44315\n",
      "iteration 3600 loss 247.44118\n",
      "iteration 3700 loss 247.43915\n",
      "iteration 3800 loss 247.43716\n",
      "iteration 3900 loss 247.43513\n",
      "iteration 4000 loss 247.43497\n",
      "iteration 4100 loss 247.43463\n",
      "iteration 4200 loss 247.43436\n",
      "iteration 4300 loss 247.43405\n",
      "iteration 4400 loss 247.4338\n",
      "iteration 4500 loss 247.43356\n",
      "iteration 4600 loss 247.4333\n",
      "iteration 4700 loss 247.43307\n",
      "iteration 4800 loss 247.43285\n",
      "iteration 4900 loss 247.43265\n",
      "iteration 5000 loss 247.43243Train finished\n",
      "Selected features are: [ 2 15 24 33 40 48 54 58 63 69]\n",
      "Selected weights are: [0.8508397  0.9996716  0.5840733  0.9996716  0.5957531  0.9996716\n",
      " 0.9996716  0.9996716  0.9996716  0.96270925]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0009911284832254768, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 37.2846218277293\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 315.25073\n",
      "iteration 200 loss 309.74664\n",
      "iteration 300 loss 308.04117\n",
      "iteration 400 loss 307.1199\n",
      "iteration 500 loss 306.68732\n",
      "iteration 600 loss 306.3024\n",
      "iteration 700 loss 305.94736\n",
      "iteration 800 loss 305.6175\n",
      "iteration 900 loss 305.4104\n",
      "iteration 1000 loss 305.27863\n",
      "iteration 1100 loss 305.23114\n",
      "iteration 1200 loss 305.19196\n",
      "iteration 1300 loss 305.16107\n",
      "iteration 1400 loss 305.13727\n",
      "iteration 1500 loss 305.11926\n",
      "iteration 1600 loss 305.11115\n",
      "iteration 1700 loss 305.10364\n",
      "iteration 1800 loss 305.09656\n",
      "iteration 1900 loss 305.08984\n",
      "iteration 2000 loss 305.08344\n",
      "iteration 2100 loss 305.0772\n",
      "iteration 2200 loss 305.07126\n",
      "iteration 2300 loss 305.06543\n",
      "iteration 2400 loss 305.05975\n",
      "iteration 2500 loss 305.0542\n",
      "iteration 2600 loss 305.05194\n",
      "iteration 2700 loss 305.04996\n",
      "iteration 2800 loss 305.04822\n",
      "iteration 2900 loss 305.0467\n",
      "iteration 3000 loss 305.04535\n",
      "iteration 3100 loss 305.04413\n",
      "iteration 3200 loss 305.04312\n",
      "iteration 3300 loss 305.04208\n",
      "iteration 3400 loss 305.0412\n",
      "iteration 3500 loss 305.0404\n",
      "iteration 3600 loss 305.03973\n",
      "iteration 3700 loss 305.03906\n",
      "iteration 3800 loss 305.0385\n",
      "iteration 3900 loss 305.038\n",
      "iteration 4000 loss 305.03754\n",
      "iteration 4100 loss 305.03714\n",
      "iteration 4200 loss 305.0367\n",
      "iteration 4300 loss 305.03635\n",
      "iteration 4400 loss 305.03604\n",
      "iteration 4500 loss 305.03574\n",
      "iteration 4600 loss 305.03546\n",
      "iteration 4700 loss 305.03525\n",
      "iteration 4800 loss 305.03503\n",
      "iteration 4900 loss 305.03485\n",
      "iteration 5000 loss 305.0346Train finished\n",
      "Selected features are: [ 2  9 15 24 40 48 54 58 63 69]\n",
      "Selected weights are: [0.8237731  0.9995575  0.9995575  0.79475504 0.9995575  0.9995575\n",
      " 0.6741952  0.9995575  0.9995575  0.61924684]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.001076245136447686, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 8.774938856193105\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 289.42273\n",
      "iteration 200 loss 285.85168\n",
      "iteration 300 loss 284.59937\n",
      "iteration 400 loss 283.57538\n",
      "iteration 500 loss 282.55145\n",
      "iteration 600 loss 282.04156\n",
      "iteration 700 loss 281.60815\n",
      "iteration 800 loss 281.411\n",
      "iteration 900 loss 281.33426\n",
      "iteration 1000 loss 281.26257\n",
      "iteration 1100 loss 281.19427\n",
      "iteration 1200 loss 281.12875\n",
      "iteration 1300 loss 281.06555\n",
      "iteration 1400 loss 281.0047\n",
      "iteration 1500 loss 280.94537\n",
      "iteration 1600 loss 280.88754\n",
      "iteration 1700 loss 280.8352\n",
      "iteration 1800 loss 280.81204\n",
      "iteration 1900 loss 280.7912\n",
      "iteration 2000 loss 280.77478\n",
      "iteration 2100 loss 280.76144\n",
      "iteration 2200 loss 280.75015\n",
      "iteration 2300 loss 280.7397\n",
      "iteration 2400 loss 280.73035\n",
      "iteration 2500 loss 280.7214\n",
      "iteration 2600 loss 280.71393\n",
      "iteration 2700 loss 280.70752\n",
      "iteration 2800 loss 280.70135\n",
      "iteration 2900 loss 280.69534\n",
      "iteration 3000 loss 280.68942\n",
      "iteration 3100 loss 280.68362\n",
      "iteration 3200 loss 280.6779\n",
      "iteration 3300 loss 280.67368\n",
      "iteration 3400 loss 280.6703\n",
      "iteration 3500 loss 280.66702\n",
      "iteration 3600 loss 280.66382\n",
      "iteration 3700 loss 280.66068\n",
      "iteration 3800 loss 280.6576\n",
      "iteration 3900 loss 280.65454\n",
      "iteration 4000 loss 280.6515\n",
      "iteration 4100 loss 280.6485\n",
      "iteration 4200 loss 280.64554\n",
      "iteration 4300 loss 280.6432\n",
      "iteration 4400 loss 280.6414\n",
      "iteration 4500 loss 280.63962\n",
      "iteration 4600 loss 280.63785\n",
      "iteration 4700 loss 280.63605\n",
      "iteration 4800 loss 280.63437\n",
      "iteration 4900 loss 280.63385\n",
      "iteration 5000 loss 280.63336Train finished\n",
      "Selected features are: [11 13 26 37 46 49 52 56 62 70]\n",
      "Selected weights are: [0.56070215 0.83440095 0.90633816 0.9997789  0.9997789  0.9997789\n",
      " 0.9997789  0.9997789  0.7903435  0.9997789 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0017953639220273103, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 24.032051188528143\n",
      "\n",
      " 7 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 315.2849\n",
      "iteration 200 loss 312.94104\n",
      "iteration 300 loss 311.22467\n",
      "iteration 400 loss 310.31146\n",
      "iteration 500 loss 309.31543\n",
      "iteration 600 loss 308.24945\n",
      "iteration 700 loss 307.33334\n",
      "iteration 800 loss 306.98785\n",
      "iteration 900 loss 306.81537\n",
      "iteration 1000 loss 306.65448\n",
      "iteration 1100 loss 306.50146\n",
      "iteration 1200 loss 306.35953\n",
      "iteration 1300 loss 306.2649\n",
      "iteration 1400 loss 306.19965\n",
      "iteration 1500 loss 306.16785\n",
      "iteration 1600 loss 306.1549\n",
      "iteration 1700 loss 306.14728\n",
      "iteration 1800 loss 306.14084\n",
      "iteration 1900 loss 306.13495\n",
      "iteration 2000 loss 306.1294\n",
      "iteration 2100 loss 306.12405\n",
      "iteration 2200 loss 306.1194\n",
      "iteration 2300 loss 306.11554\n",
      "iteration 2400 loss 306.11182\n",
      "iteration 2500 loss 306.10815\n",
      "iteration 2600 loss 306.1047\n",
      "iteration 2700 loss 306.1012\n",
      "iteration 2800 loss 306.0978\n",
      "iteration 2900 loss 306.09436\n",
      "iteration 3000 loss 306.09106\n",
      "iteration 3100 loss 306.0882\n",
      "iteration 3200 loss 306.08588\n",
      "iteration 3300 loss 306.0836\n",
      "iteration 3400 loss 306.08136\n",
      "iteration 3500 loss 306.07916\n",
      "iteration 3600 loss 306.07703\n",
      "iteration 3700 loss 306.07486\n",
      "iteration 3800 loss 306.07275\n",
      "iteration 3900 loss 306.07068\n",
      "iteration 4000 loss 306.0686\n",
      "iteration 4100 loss 306.06653\n",
      "iteration 4200 loss 306.06445\n",
      "iteration 4300 loss 306.06238\n",
      "iteration 4400 loss 306.06036\n",
      "iteration 4500 loss 306.05835\n",
      "iteration 4600 loss 306.0563\n",
      "iteration 4700 loss 306.0543\n",
      "iteration 4800 loss 306.0523\n",
      "iteration 4900 loss 306.05035\n",
      "iteration 5000 loss 306.0483Train finished\n",
      "Selected features are: [ 8 20 26 37 44 49 52 56 62 70]\n",
      "Selected weights are: [0.51471806 0.79370856 0.9997541  0.9997541  0.64046407 0.9997541\n",
      " 0.9997541  0.9997541  0.9997541  0.9997541 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.005262483942635404, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 13.122057585381588\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 271.28214\n",
      "iteration 200 loss 264.88702\n",
      "iteration 300 loss 263.6587\n",
      "iteration 400 loss 262.62125\n",
      "iteration 500 loss 261.78427\n",
      "iteration 600 loss 261.34354\n",
      "iteration 700 loss 260.95407\n",
      "iteration 800 loss 260.6354\n",
      "iteration 900 loss 260.4843\n",
      "iteration 1000 loss 260.4259\n",
      "iteration 1100 loss 260.40106\n",
      "iteration 1200 loss 260.3872\n",
      "iteration 1300 loss 260.37485\n",
      "iteration 1400 loss 260.36346\n",
      "iteration 1500 loss 260.35278\n",
      "iteration 1600 loss 260.3431\n",
      "iteration 1700 loss 260.33932\n",
      "iteration 1800 loss 260.33566\n",
      "iteration 1900 loss 260.33218\n",
      "iteration 2000 loss 260.32874\n",
      "iteration 2100 loss 260.32532\n",
      "iteration 2200 loss 260.322\n",
      "iteration 2300 loss 260.31866\n",
      "iteration 2400 loss 260.31537\n",
      "iteration 2500 loss 260.31213\n",
      "iteration 2600 loss 260.30896\n",
      "iteration 2700 loss 260.3058\n",
      "iteration 2800 loss 260.30356\n",
      "iteration 2900 loss 260.30292\n",
      "iteration 3000 loss 260.30225\n",
      "iteration 3100 loss 260.30164\n",
      "iteration 3200 loss 260.3011\n",
      "iteration 3300 loss 260.30048\n",
      "iteration 3400 loss 260.2999\n",
      "iteration 3500 loss 260.29932\n",
      "iteration 3600 loss 260.2988\n",
      "iteration 3700 loss 260.29828\n",
      "iteration 3800 loss 260.29773\n",
      "iteration 3900 loss 260.29724\n",
      "iteration 4000 loss 260.2967\n",
      "iteration 4100 loss 260.29626\n",
      "iteration 4200 loss 260.29575\n",
      "iteration 4300 loss 260.2953\n",
      "iteration 4400 loss 260.2948\n",
      "iteration 4500 loss 260.29437\n",
      "iteration 4600 loss 260.29388\n",
      "iteration 4700 loss 260.29346\n",
      "iteration 4800 loss 260.29303\n",
      "iteration 4900 loss 260.2926\n",
      "iteration 5000 loss 260.29218Train finished\n",
      "Selected features are: [ 9 11 15 40 48 54 58 63 69 70]\n",
      "Selected weights are: [0.82444614 0.6047193  0.93951285 0.9993948  0.9993948  0.77644455\n",
      " 0.9993948  0.9993948  0.49424592 0.9993948 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0006082028292043088, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 27.900860013347945\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 258.36752\n",
      "iteration 200 loss 251.82254\n",
      "iteration 300 loss 249.17993\n",
      "iteration 400 loss 248.135\n",
      "iteration 500 loss 247.25926\n",
      "iteration 600 loss 246.59189\n",
      "iteration 700 loss 246.2206\n",
      "iteration 800 loss 245.90097\n",
      "iteration 900 loss 245.64005\n",
      "iteration 1000 loss 245.56183\n",
      "iteration 1100 loss 245.52765\n",
      "iteration 1200 loss 245.5051\n",
      "iteration 1300 loss 245.48633\n",
      "iteration 1400 loss 245.475\n",
      "iteration 1500 loss 245.46729\n",
      "iteration 1600 loss 245.45973\n",
      "iteration 1700 loss 245.4524\n",
      "iteration 1800 loss 245.44525\n",
      "iteration 1900 loss 245.439\n",
      "iteration 2000 loss 245.43864\n",
      "iteration 2100 loss 245.43828\n",
      "iteration 2200 loss 245.43799\n",
      "iteration 2300 loss 245.43773\n",
      "iteration 2400 loss 245.43753\n",
      "iteration 2500 loss 245.43732\n",
      "iteration 2600 loss 245.43716\n",
      "iteration 2700 loss 245.43698\n",
      "iteration 2800 loss 245.43686\n",
      "iteration 2900 loss 245.4367\n",
      "iteration 3000 loss 245.43655\n",
      "iteration 3100 loss 245.43646\n",
      "iteration 3200 loss 245.43634\n",
      "iteration 3300 loss 245.43623\n",
      "iteration 3400 loss 245.43614\n",
      "iteration 3500 loss 245.43602\n",
      "iteration 3600 loss 245.43594\n",
      "iteration 3700 loss 245.43584\n",
      "iteration 3800 loss 245.43576\n",
      "iteration 3900 loss 245.43568\n",
      "iteration 4000 loss 245.43561\n",
      "iteration 4100 loss 245.43547\n",
      "iteration 4200 loss 245.43542\n",
      "iteration 4300 loss 245.43536\n",
      "iteration 4400 loss 245.43529\n",
      "iteration 4500 loss 245.43518\n",
      "iteration 4600 loss 245.43515\n",
      "iteration 4700 loss 245.43504\n",
      "iteration 4800 loss 245.435\n",
      "iteration 4900 loss 245.43492\n",
      "iteration 5000 loss 245.43484Train finished\n",
      "Selected features are: [15 36 40 48 51 54 55 58 63 69]\n",
      "Selected weights are: [0.99944144 0.8313961  0.99944144 0.99944144 0.99944144 0.7267315\n",
      " 0.6353204  0.99944144 0.99944144 0.7628547 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0008217688882510024, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 33.6006343981785\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 261.20847\n",
      "iteration 200 loss 256.81717\n",
      "iteration 300 loss 255.67104\n",
      "iteration 400 loss 255.08926\n",
      "iteration 500 loss 254.64291\n",
      "iteration 600 loss 254.47546\n",
      "iteration 700 loss 254.32379\n",
      "iteration 800 loss 254.18178\n",
      "iteration 900 loss 254.04996\n",
      "iteration 1000 loss 253.92525\n",
      "iteration 1100 loss 253.80585\n",
      "iteration 1200 loss 253.69006\n",
      "iteration 1300 loss 253.59903\n",
      "iteration 1400 loss 253.52412\n",
      "iteration 1500 loss 253.45355\n",
      "iteration 1600 loss 253.39587\n",
      "iteration 1700 loss 253.34865\n",
      "iteration 1800 loss 253.3088\n",
      "iteration 1900 loss 253.27832\n",
      "iteration 2000 loss 253.25227\n",
      "iteration 2100 loss 253.23257\n",
      "iteration 2200 loss 253.21635\n",
      "iteration 2300 loss 253.21165\n",
      "iteration 2400 loss 253.20848\n",
      "iteration 2500 loss 253.20728\n",
      "iteration 2600 loss 253.2063\n",
      "iteration 2700 loss 253.20546\n",
      "iteration 2800 loss 253.20474\n",
      "iteration 2900 loss 253.20407\n",
      "iteration 3000 loss 253.20346\n",
      "iteration 3100 loss 253.20288\n",
      "iteration 3200 loss 253.20244\n",
      "iteration 3300 loss 253.20193\n",
      "iteration 3400 loss 253.2015\n",
      "iteration 3500 loss 253.20105\n",
      "iteration 3600 loss 253.20068\n",
      "iteration 3700 loss 253.20029\n",
      "iteration 3800 loss 253.19995\n",
      "iteration 3900 loss 253.1996\n",
      "iteration 4000 loss 253.19925\n",
      "iteration 4100 loss 253.19897\n",
      "iteration 4200 loss 253.19864\n",
      "iteration 4300 loss 253.19836\n",
      "iteration 4400 loss 253.19809\n",
      "iteration 4500 loss 253.19785\n",
      "iteration 4600 loss 253.19756\n",
      "iteration 4700 loss 253.19734\n",
      "iteration 4800 loss 253.1971\n",
      "iteration 4900 loss 253.19688\n",
      "iteration 5000 loss 253.19669Train finished\n",
      "Selected features are: [ 2  9 14 15 24 40 48 55 58 63]\n",
      "Selected weights are: [0.92962813 0.78979284 0.61718297 0.99960434 0.99960434 0.99960434\n",
      " 0.99960434 0.666274   0.99960434 0.99960434]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0005215551311212816, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 28.933182697153466\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 254.0913\n",
      "iteration 200 loss 250.06854\n",
      "iteration 300 loss 244.48663\n",
      "iteration 400 loss 242.76108\n",
      "iteration 500 loss 241.28816\n",
      "iteration 600 loss 239.81361\n",
      "iteration 700 loss 238.39018\n",
      "iteration 800 loss 237.78497\n",
      "iteration 900 loss 237.5835\n",
      "iteration 1000 loss 237.4477\n",
      "iteration 1100 loss 237.32085\n",
      "iteration 1200 loss 237.20691\n",
      "iteration 1300 loss 237.14362\n",
      "iteration 1400 loss 237.11026\n",
      "iteration 1500 loss 237.08249\n",
      "iteration 1600 loss 237.05904\n",
      "iteration 1700 loss 237.03891\n",
      "iteration 1800 loss 237.01941\n",
      "iteration 1900 loss 237.00056\n",
      "iteration 2000 loss 236.98409\n",
      "iteration 2100 loss 236.96803\n",
      "iteration 2200 loss 236.95471\n",
      "iteration 2300 loss 236.95276\n",
      "iteration 2400 loss 236.95114\n",
      "iteration 2500 loss 236.94955\n",
      "iteration 2600 loss 236.94795\n",
      "iteration 2700 loss 236.94643\n",
      "iteration 2800 loss 236.94489\n",
      "iteration 2900 loss 236.94334\n",
      "iteration 3000 loss 236.94186\n",
      "iteration 3100 loss 236.94034\n",
      "iteration 3200 loss 236.93887\n",
      "iteration 3300 loss 236.93742\n",
      "iteration 3400 loss 236.93597\n",
      "iteration 3500 loss 236.93456\n",
      "iteration 3600 loss 236.93314\n",
      "iteration 3700 loss 236.93175\n",
      "iteration 3800 loss 236.93036\n",
      "iteration 3900 loss 236.92896\n",
      "iteration 4000 loss 236.92764\n",
      "iteration 4100 loss 236.9263\n",
      "iteration 4200 loss 236.92548\n",
      "iteration 4300 loss 236.92474\n",
      "iteration 4400 loss 236.92404\n",
      "iteration 4500 loss 236.92331\n",
      "iteration 4600 loss 236.9226\n",
      "iteration 4700 loss 236.92192\n",
      "iteration 4800 loss 236.9212\n",
      "iteration 4900 loss 236.92047\n",
      "iteration 5000 loss 236.91977Train finished\n",
      "Selected features are: [11 14 26 37 44 46 49 56 62 70]\n",
      "Selected weights are: [0.53563774 0.99974895 0.99974895 0.99974895 0.60577637 0.7740756\n",
      " 0.99974895 0.99974895 0.99974895 0.99974895]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0006739910253517913, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 41.76872920410319\n",
      "\n",
      " 8 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 273.9746\n",
      "iteration 200 loss 266.70474\n",
      "iteration 300 loss 264.74548\n",
      "iteration 400 loss 263.72003\n",
      "iteration 500 loss 263.21393\n",
      "iteration 600 loss 262.75305\n",
      "iteration 700 loss 262.328\n",
      "iteration 800 loss 261.93805\n",
      "iteration 900 loss 261.6302\n",
      "iteration 1000 loss 261.43323\n",
      "iteration 1100 loss 261.32364\n",
      "iteration 1200 loss 261.26465\n",
      "iteration 1300 loss 261.2191\n",
      "iteration 1400 loss 261.184\n",
      "iteration 1500 loss 261.16934\n",
      "iteration 1600 loss 261.16534\n",
      "iteration 1700 loss 261.16418\n",
      "iteration 1800 loss 261.1635\n",
      "iteration 1900 loss 261.16388\n",
      "iteration 2000 loss 261.16388\n",
      "iteration 2100 loss 261.1639\n",
      "iteration 2200 loss 261.16388\n",
      "iteration 2300 loss 261.1639\n",
      "iteration 2400 loss 261.1639\n",
      "iteration 2500 loss 261.1639\n",
      "iteration 2600 loss 261.1639\n",
      "iteration 2700 loss 261.16388\n",
      "iteration 2800 loss 261.16388\n",
      "iteration 2900 loss 261.16388\n",
      "iteration 3000 loss 261.16388\n",
      "iteration 3100 loss 261.1639\n",
      "iteration 3200 loss 261.16388\n",
      "iteration 3300 loss 261.16388\n",
      "iteration 3400 loss 261.16388\n",
      "iteration 3500 loss 261.16388\n",
      "iteration 3600 loss 261.16385\n",
      "iteration 3700 loss 261.16388\n",
      "iteration 3800 loss 261.16388\n",
      "iteration 3900 loss 261.16385\n",
      "iteration 4000 loss 261.16385\n",
      "iteration 4100 loss 261.16385\n",
      "iteration 4200 loss 261.16385\n",
      "iteration 4300 loss 261.16385\n",
      "iteration 4400 loss 261.16385\n",
      "iteration 4500 loss 261.16388\n",
      "iteration 4600 loss 261.16385\n",
      "iteration 4700 loss 261.16385\n",
      "iteration 4800 loss 261.16382\n",
      "iteration 4900 loss 261.16382\n",
      "iteration 5000 loss 261.16382Train finished\n",
      "Selected features are: [ 2  9 15 24 40 48 58 63 69 70]\n",
      "Selected weights are: [0.5736948  0.9994843  0.9994843  0.57888484 0.9994843  0.9994843\n",
      " 0.9994843  0.9994843  0.9994843  0.9994843 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0013119787990256595, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 26.5149818535632\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 285.77618\n",
      "iteration 200 loss 278.58075\n",
      "iteration 300 loss 277.43573\n",
      "iteration 400 loss 276.9008\n",
      "iteration 500 loss 276.40997\n",
      "iteration 600 loss 275.9518\n",
      "iteration 700 loss 275.5304\n",
      "iteration 800 loss 275.28094\n",
      "iteration 900 loss 275.07367\n",
      "iteration 1000 loss 274.87695\n",
      "iteration 1100 loss 274.75116\n",
      "iteration 1200 loss 274.65564\n",
      "iteration 1300 loss 274.59174\n",
      "iteration 1400 loss 274.5624\n",
      "iteration 1500 loss 274.54446\n",
      "iteration 1600 loss 274.53162\n",
      "iteration 1700 loss 274.5257\n",
      "iteration 1800 loss 274.52026\n",
      "iteration 1900 loss 274.5153\n",
      "iteration 2000 loss 274.51062\n",
      "iteration 2100 loss 274.5062\n",
      "iteration 2200 loss 274.502\n",
      "iteration 2300 loss 274.49805\n",
      "iteration 2400 loss 274.49426\n",
      "iteration 2500 loss 274.4906\n",
      "iteration 2600 loss 274.48712\n",
      "iteration 2700 loss 274.48376\n",
      "iteration 2800 loss 274.48053\n",
      "iteration 2900 loss 274.47736\n",
      "iteration 3000 loss 274.47437\n",
      "iteration 3100 loss 274.47137\n",
      "iteration 3200 loss 274.46857\n",
      "iteration 3300 loss 274.46582\n",
      "iteration 3400 loss 274.46307\n",
      "iteration 3500 loss 274.46045\n",
      "iteration 3600 loss 274.4579\n",
      "iteration 3700 loss 274.4554\n",
      "iteration 3800 loss 274.45294\n",
      "iteration 3900 loss 274.45056\n",
      "iteration 4000 loss 274.44818\n",
      "iteration 4100 loss 274.44592\n",
      "iteration 4200 loss 274.44366\n",
      "iteration 4300 loss 274.44144\n",
      "iteration 4400 loss 274.43927\n",
      "iteration 4500 loss 274.43713\n",
      "iteration 4600 loss 274.435\n",
      "iteration 4700 loss 274.43298\n",
      "iteration 4800 loss 274.43094\n",
      "iteration 4900 loss 274.42896\n",
      "iteration 5000 loss 274.42694Train finished\n",
      "Selected features are: [ 2  9 15 40 48 54 55 58 63 69]\n",
      "Selected weights are: [0.7690781  0.9790229  0.9995993  0.9995993  0.9995993  0.64281464\n",
      " 0.64637804 0.9995993  0.9995993  0.6976647 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0011490879907400365, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 23.43326611459902\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 236.00331\n",
      "iteration 200 loss 230.18436\n",
      "iteration 300 loss 228.5925\n",
      "iteration 400 loss 227.18845\n",
      "iteration 500 loss 225.93146\n",
      "iteration 600 loss 225.10962\n",
      "iteration 700 loss 224.67635\n",
      "iteration 800 loss 224.50105\n",
      "iteration 900 loss 224.33554\n",
      "iteration 1000 loss 224.24107\n",
      "iteration 1100 loss 224.17068\n",
      "iteration 1200 loss 224.10165\n",
      "iteration 1300 loss 224.03374\n",
      "iteration 1400 loss 223.9715\n",
      "iteration 1500 loss 223.95448\n",
      "iteration 1600 loss 223.93842\n",
      "iteration 1700 loss 223.92279\n",
      "iteration 1800 loss 223.90828\n",
      "iteration 1900 loss 223.89497\n",
      "iteration 2000 loss 223.88264\n",
      "iteration 2100 loss 223.87163\n",
      "iteration 2200 loss 223.86223\n",
      "iteration 2300 loss 223.85303\n",
      "iteration 2400 loss 223.84384\n",
      "iteration 2500 loss 223.83461\n",
      "iteration 2600 loss 223.82532\n",
      "iteration 2700 loss 223.81654\n",
      "iteration 2800 loss 223.80783\n",
      "iteration 2900 loss 223.79984\n",
      "iteration 3000 loss 223.79272\n",
      "iteration 3100 loss 223.78664\n",
      "iteration 3200 loss 223.78087\n",
      "iteration 3300 loss 223.77591\n",
      "iteration 3400 loss 223.771\n",
      "iteration 3500 loss 223.76608\n",
      "iteration 3600 loss 223.76118\n",
      "iteration 3700 loss 223.75626\n",
      "iteration 3800 loss 223.75127\n",
      "iteration 3900 loss 223.74976\n",
      "iteration 4000 loss 223.74931\n",
      "iteration 4100 loss 223.7489\n",
      "iteration 4200 loss 223.74857\n",
      "iteration 4300 loss 223.74823\n",
      "iteration 4400 loss 223.74826\n",
      "iteration 4500 loss 223.74823\n",
      "iteration 4600 loss 223.7482\n",
      "iteration 4700 loss 223.7482\n",
      "iteration 4800 loss 223.7482\n",
      "iteration 4900 loss 223.7482\n",
      "iteration 5000 loss 223.7482Train finished\n",
      "Selected features are: [11 13 26 37 44 46 49 52 56 70]\n",
      "Selected weights are: [0.40447146 0.3891933  0.99986    0.99986    0.99986    0.99986\n",
      " 0.99986    0.99986    0.99986    0.99986   ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0016251976191600011, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 45.42018127726734\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 275.93823\n",
      "iteration 200 loss 269.23007\n",
      "iteration 300 loss 266.6329\n",
      "iteration 400 loss 264.92596\n",
      "iteration 500 loss 263.67688\n",
      "iteration 600 loss 263.30185\n",
      "iteration 700 loss 263.0612\n",
      "iteration 800 loss 262.8336\n",
      "iteration 900 loss 262.62015\n",
      "iteration 1000 loss 262.42072\n",
      "iteration 1100 loss 262.2334\n",
      "iteration 1200 loss 262.08722\n",
      "iteration 1300 loss 261.9886\n",
      "iteration 1400 loss 261.9201\n",
      "iteration 1500 loss 261.8783\n",
      "iteration 1600 loss 261.85822\n",
      "iteration 1700 loss 261.84692\n",
      "iteration 1800 loss 261.83582\n",
      "iteration 1900 loss 261.826\n",
      "iteration 2000 loss 261.8166\n",
      "iteration 2100 loss 261.8102\n",
      "iteration 2200 loss 261.8076\n",
      "iteration 2300 loss 261.80524\n",
      "iteration 2400 loss 261.80334\n",
      "iteration 2500 loss 261.8016\n",
      "iteration 2600 loss 261.80005\n",
      "iteration 2700 loss 261.7986\n",
      "iteration 2800 loss 261.7973\n",
      "iteration 2900 loss 261.79602\n",
      "iteration 3000 loss 261.79486\n",
      "iteration 3100 loss 261.79376\n",
      "iteration 3200 loss 261.79276\n",
      "iteration 3300 loss 261.7918\n",
      "iteration 3400 loss 261.791\n",
      "iteration 3500 loss 261.7902\n",
      "iteration 3600 loss 261.79004\n",
      "iteration 3700 loss 261.78998\n",
      "iteration 3800 loss 261.78986\n",
      "iteration 3900 loss 261.78973\n",
      "iteration 4000 loss 261.7896\n",
      "iteration 4100 loss 261.78952\n",
      "iteration 4200 loss 261.78943\n",
      "iteration 4300 loss 261.78937\n",
      "iteration 4400 loss 261.78925\n",
      "iteration 4500 loss 261.78912\n",
      "iteration 4600 loss 261.78906\n",
      "iteration 4700 loss 261.78897\n",
      "iteration 4800 loss 261.78888\n",
      "iteration 4900 loss 261.78876\n",
      "iteration 5000 loss 261.7887Train finished\n",
      "Selected features are: [ 9 15 24 33 40 48 54 58 63 69]\n",
      "Selected weights are: [0.9996544 0.9996544 0.3458306 0.9996544 0.9996544 0.9996544 0.9996544\n",
      " 0.9996544 0.9996544 0.9996544]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0009884447769549487, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 31.27854101768783\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 301.09653\n",
      "iteration 200 loss 297.645\n",
      "iteration 300 loss 295.93433\n",
      "iteration 400 loss 293.8008\n",
      "iteration 500 loss 292.09088\n",
      "iteration 600 loss 291.1111\n",
      "iteration 700 loss 290.74753\n",
      "iteration 800 loss 290.54236\n",
      "iteration 900 loss 290.44235\n",
      "iteration 1000 loss 290.35397\n",
      "iteration 1100 loss 290.2738\n",
      "iteration 1200 loss 290.20123\n",
      "iteration 1300 loss 290.13788\n",
      "iteration 1400 loss 290.09576\n",
      "iteration 1500 loss 290.0631\n",
      "iteration 1600 loss 290.0353\n",
      "iteration 1700 loss 290.00958\n",
      "iteration 1800 loss 289.98474\n",
      "iteration 1900 loss 289.96402\n",
      "iteration 2000 loss 289.94427\n",
      "iteration 2100 loss 289.9314\n",
      "iteration 2200 loss 289.92047\n",
      "iteration 2300 loss 289.9103\n",
      "iteration 2400 loss 289.9007\n",
      "iteration 2500 loss 289.89105\n",
      "iteration 2600 loss 289.88245\n",
      "iteration 2700 loss 289.87387\n",
      "iteration 2800 loss 289.8652\n",
      "iteration 2900 loss 289.8564\n",
      "iteration 3000 loss 289.85083\n",
      "iteration 3100 loss 289.84534\n",
      "iteration 3200 loss 289.83984\n",
      "iteration 3300 loss 289.83435\n",
      "iteration 3400 loss 289.82883\n",
      "iteration 3500 loss 289.82333\n",
      "iteration 3600 loss 289.81778\n",
      "iteration 3700 loss 289.81213\n",
      "iteration 3800 loss 289.8064\n",
      "iteration 3900 loss 289.80057\n",
      "iteration 4000 loss 289.7945\n",
      "iteration 4100 loss 289.78827\n",
      "iteration 4200 loss 289.78162\n",
      "iteration 4300 loss 289.77448\n",
      "iteration 4400 loss 289.76648\n",
      "iteration 4500 loss 289.75717\n",
      "iteration 4600 loss 289.7451\n",
      "iteration 4700 loss 289.72687\n",
      "iteration 4800 loss 289.69125\n",
      "iteration 4900 loss 289.5957\n",
      "iteration 5000 loss 289.37195Train finished\n",
      "Selected features are: [ 2 33 36 46 48 51 54 56 58 63]\n",
      "Selected weights are: [0.40383247 0.99989223 0.88681775 0.864309   0.584248   0.99989223\n",
      " 0.99989223 0.99989223 0.99989223 0.99989223]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00211121276816751, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 17.972137847830773\n",
      "\n",
      " 9 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 282.0226\n",
      "iteration 200 loss 278.9737\n",
      "iteration 300 loss 278.2697\n",
      "iteration 400 loss 277.69507\n",
      "iteration 500 loss 277.19537\n",
      "iteration 600 loss 276.7489\n",
      "iteration 700 loss 276.4864\n",
      "iteration 800 loss 276.39767\n",
      "iteration 900 loss 276.31702\n",
      "iteration 1000 loss 276.24286\n",
      "iteration 1100 loss 276.17252\n",
      "iteration 1200 loss 276.1048\n",
      "iteration 1300 loss 276.03934\n",
      "iteration 1400 loss 275.9763\n",
      "iteration 1500 loss 275.91718\n",
      "iteration 1600 loss 275.8673\n",
      "iteration 1700 loss 275.8255\n",
      "iteration 1800 loss 275.78662\n",
      "iteration 1900 loss 275.74908\n",
      "iteration 2000 loss 275.7133\n",
      "iteration 2100 loss 275.68317\n",
      "iteration 2200 loss 275.657\n",
      "iteration 2300 loss 275.63287\n",
      "iteration 2400 loss 275.6127\n",
      "iteration 2500 loss 275.5932\n",
      "iteration 2600 loss 275.57773\n",
      "iteration 2700 loss 275.56726\n",
      "iteration 2800 loss 275.55713\n",
      "iteration 2900 loss 275.54718\n",
      "iteration 3000 loss 275.53766\n",
      "iteration 3100 loss 275.529\n",
      "iteration 3200 loss 275.52118\n",
      "iteration 3300 loss 275.5149\n",
      "iteration 3400 loss 275.51093\n",
      "iteration 3500 loss 275.5099\n",
      "iteration 3600 loss 275.50992\n",
      "iteration 3700 loss 275.50986\n",
      "iteration 3800 loss 275.50983\n",
      "iteration 3900 loss 275.50983\n",
      "iteration 4000 loss 275.5098\n",
      "iteration 4100 loss 275.5098\n",
      "iteration 4200 loss 275.5098\n",
      "iteration 4300 loss 275.50983\n",
      "iteration 4400 loss 275.5098\n",
      "iteration 4500 loss 275.50977\n",
      "iteration 4600 loss 275.50977\n",
      "iteration 4700 loss 275.50977\n",
      "iteration 4800 loss 275.50974\n",
      "iteration 4900 loss 275.50974\n",
      "iteration 5000 loss 275.5097Train finished\n",
      "Selected features are: [ 2  9 14 15 24 40 48 58 63 69]\n",
      "Selected weights are: [0.9997656  0.9997656  0.59435856 0.9997656  0.9997656  0.741239\n",
      " 0.9997656  0.9997656  0.9997656  0.81937087]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0005802899272534777, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 20.802013192335004\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 228.86615\n",
      "iteration 200 loss 225.33691\n",
      "iteration 300 loss 222.96432\n",
      "iteration 400 loss 221.9011\n",
      "iteration 500 loss 221.4396\n",
      "iteration 600 loss 221.08632\n",
      "iteration 700 loss 220.73483\n",
      "iteration 800 loss 220.36462\n",
      "iteration 900 loss 219.97195\n",
      "iteration 1000 loss 219.62585\n",
      "iteration 1100 loss 219.37975\n",
      "iteration 1200 loss 219.15031\n",
      "iteration 1300 loss 218.92949\n",
      "iteration 1400 loss 218.7135\n",
      "iteration 1500 loss 218.57663\n",
      "iteration 1600 loss 218.48663\n",
      "iteration 1700 loss 218.40512\n",
      "iteration 1800 loss 218.33243\n",
      "iteration 1900 loss 218.27057\n",
      "iteration 2000 loss 218.23456\n",
      "iteration 2100 loss 218.21259\n",
      "iteration 2200 loss 218.19322\n",
      "iteration 2300 loss 218.17603\n",
      "iteration 2400 loss 218.16867\n",
      "iteration 2500 loss 218.16388\n",
      "iteration 2600 loss 218.15906\n",
      "iteration 2700 loss 218.15419\n",
      "iteration 2800 loss 218.14957\n",
      "iteration 2900 loss 218.1453\n",
      "iteration 3000 loss 218.14102\n",
      "iteration 3100 loss 218.13681\n",
      "iteration 3200 loss 218.13304\n",
      "iteration 3300 loss 218.12991\n",
      "iteration 3400 loss 218.12689\n",
      "iteration 3500 loss 218.12387\n",
      "iteration 3600 loss 218.12088\n",
      "iteration 3700 loss 218.11789\n",
      "iteration 3800 loss 218.11493\n",
      "iteration 3900 loss 218.11188\n",
      "iteration 4000 loss 218.10895\n",
      "iteration 4100 loss 218.10596\n",
      "iteration 4200 loss 218.103\n",
      "iteration 4300 loss 218.1\n",
      "iteration 4400 loss 218.09703\n",
      "iteration 4500 loss 218.09406\n",
      "iteration 4600 loss 218.09113\n",
      "iteration 4700 loss 218.08813\n",
      "iteration 4800 loss 218.0852\n",
      "iteration 4900 loss 218.08221\n",
      "iteration 5000 loss 218.07927Train finished\n",
      "Selected features are: [ 8 11 13 44 46 49 51 52 56 62]\n",
      "Selected weights are: [0.9998155  0.73833627 0.9998155  0.8839074  0.9998155  0.9998155\n",
      " 0.59784454 0.9998155  0.9998155  0.9998155 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0010771114969015461, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 50.96206024728642\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 253.63892\n",
      "iteration 200 loss 250.14925\n",
      "iteration 300 loss 249.0004\n",
      "iteration 400 loss 248.02266\n",
      "iteration 500 loss 247.16806\n",
      "iteration 600 loss 246.73627\n",
      "iteration 700 loss 246.33145\n",
      "iteration 800 loss 246.11682\n",
      "iteration 900 loss 245.93079\n",
      "iteration 1000 loss 245.7591\n",
      "iteration 1100 loss 245.62753\n",
      "iteration 1200 loss 245.52966\n",
      "iteration 1300 loss 245.45786\n",
      "iteration 1400 loss 245.41853\n",
      "iteration 1500 loss 245.38437\n",
      "iteration 1600 loss 245.36507\n",
      "iteration 1700 loss 245.35397\n",
      "iteration 1800 loss 245.34665\n",
      "iteration 1900 loss 245.34018\n",
      "iteration 2000 loss 245.33395\n",
      "iteration 2100 loss 245.32793\n",
      "iteration 2200 loss 245.32535\n",
      "iteration 2300 loss 245.32365\n",
      "iteration 2400 loss 245.32213\n",
      "iteration 2500 loss 245.3207\n",
      "iteration 2600 loss 245.31935\n",
      "iteration 2700 loss 245.31805\n",
      "iteration 2800 loss 245.31685\n",
      "iteration 2900 loss 245.31572\n",
      "iteration 3000 loss 245.3147\n",
      "iteration 3100 loss 245.31439\n",
      "iteration 3200 loss 245.31416\n",
      "iteration 3300 loss 245.31393\n",
      "iteration 3400 loss 245.31374\n",
      "iteration 3500 loss 245.31354\n",
      "iteration 3600 loss 245.31334\n",
      "iteration 3700 loss 245.3132\n",
      "iteration 3800 loss 245.31305\n",
      "iteration 3900 loss 245.31293\n",
      "iteration 4000 loss 245.31274\n",
      "iteration 4100 loss 245.31265\n",
      "iteration 4200 loss 245.3125\n",
      "iteration 4300 loss 245.31241\n",
      "iteration 4400 loss 245.3123\n",
      "iteration 4500 loss 245.3122\n",
      "iteration 4600 loss 245.3121\n",
      "iteration 4700 loss 245.31201\n",
      "iteration 4800 loss 245.31194\n",
      "iteration 4900 loss 245.31186\n",
      "iteration 5000 loss 245.31178Train finished\n",
      "Selected features are: [ 7 11 13 26 37 46 49 52 56 70]\n",
      "Selected weights are: [0.39441854 0.56597745 0.99965036 0.99965036 0.99965036 0.99965036\n",
      " 0.9414615  0.99965036 0.99965036 0.99965036]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 9.744516745448542e-05, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 36.75989536711185\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 298.60596\n",
      "iteration 200 loss 293.65808\n",
      "iteration 300 loss 291.61325\n",
      "iteration 400 loss 290.83087\n",
      "iteration 500 loss 290.3588\n",
      "iteration 600 loss 289.94586\n",
      "iteration 700 loss 289.57767\n",
      "iteration 800 loss 289.2475\n",
      "iteration 900 loss 288.9644\n",
      "iteration 1000 loss 288.82562\n",
      "iteration 1100 loss 288.75714\n",
      "iteration 1200 loss 288.72153\n",
      "iteration 1300 loss 288.70123\n",
      "iteration 1400 loss 288.6875\n",
      "iteration 1500 loss 288.68118\n",
      "iteration 1600 loss 288.67957\n",
      "iteration 1700 loss 288.67828\n",
      "iteration 1800 loss 288.6772\n",
      "iteration 1900 loss 288.67615\n",
      "iteration 2000 loss 288.67523\n",
      "iteration 2100 loss 288.67438\n",
      "iteration 2200 loss 288.67355\n",
      "iteration 2300 loss 288.67285\n",
      "iteration 2400 loss 288.6721\n",
      "iteration 2500 loss 288.6714\n",
      "iteration 2600 loss 288.67072\n",
      "iteration 2700 loss 288.67007\n",
      "iteration 2800 loss 288.66943\n",
      "iteration 2900 loss 288.66888\n",
      "iteration 3000 loss 288.66827\n",
      "iteration 3100 loss 288.66766\n",
      "iteration 3200 loss 288.6671\n",
      "iteration 3300 loss 288.66656\n",
      "iteration 3400 loss 288.666\n",
      "iteration 3500 loss 288.66547\n",
      "iteration 3600 loss 288.66498\n",
      "iteration 3700 loss 288.6645\n",
      "iteration 3800 loss 288.664\n",
      "iteration 3900 loss 288.6635\n",
      "iteration 4000 loss 288.66302\n",
      "iteration 4100 loss 288.6626\n",
      "iteration 4200 loss 288.6621\n",
      "iteration 4300 loss 288.66168\n",
      "iteration 4400 loss 288.66125\n",
      "iteration 4500 loss 288.66083\n",
      "iteration 4600 loss 288.66043\n",
      "iteration 4700 loss 288.66003\n",
      "iteration 4800 loss 288.65964\n",
      "iteration 4900 loss 288.65924\n",
      "iteration 5000 loss 288.65887Train finished\n",
      "Selected features are: [ 9 15 24 36 40 48 54 58 63 69]\n",
      "Selected weights are: [0.99941844 0.99941844 0.46958095 0.9947763  0.99941844 0.99941844\n",
      " 0.95334417 0.99941844 0.9051994  0.5302746 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0010429404326709186, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 16.083097510454202\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 297.9644\n",
      "iteration 200 loss 287.33966\n",
      "iteration 300 loss 285.37482\n",
      "iteration 400 loss 284.46497\n",
      "iteration 500 loss 283.6977\n",
      "iteration 600 loss 283.07245\n",
      "iteration 700 loss 282.74493\n",
      "iteration 800 loss 282.4729\n",
      "iteration 900 loss 282.22852\n",
      "iteration 1000 loss 282.04474\n",
      "iteration 1100 loss 281.92206\n",
      "iteration 1200 loss 281.8227\n",
      "iteration 1300 loss 281.7786\n",
      "iteration 1400 loss 281.7474\n",
      "iteration 1500 loss 281.72235\n",
      "iteration 1600 loss 281.70068\n",
      "iteration 1700 loss 281.68018\n",
      "iteration 1800 loss 281.66162\n",
      "iteration 1900 loss 281.6466\n",
      "iteration 2000 loss 281.638\n",
      "iteration 2100 loss 281.6298\n",
      "iteration 2200 loss 281.6219\n",
      "iteration 2300 loss 281.61426\n",
      "iteration 2400 loss 281.6068\n",
      "iteration 2500 loss 281.60287\n",
      "iteration 2600 loss 281.5997\n",
      "iteration 2700 loss 281.59683\n",
      "iteration 2800 loss 281.59418\n",
      "iteration 2900 loss 281.59167\n",
      "iteration 3000 loss 281.58942\n",
      "iteration 3100 loss 281.58728\n",
      "iteration 3200 loss 281.58527\n",
      "iteration 3300 loss 281.58344\n",
      "iteration 3400 loss 281.58167\n",
      "iteration 3500 loss 281.57996\n",
      "iteration 3600 loss 281.5784\n",
      "iteration 3700 loss 281.57684\n",
      "iteration 3800 loss 281.5754\n",
      "iteration 3900 loss 281.57397\n",
      "iteration 4000 loss 281.57266\n",
      "iteration 4100 loss 281.57138\n",
      "iteration 4200 loss 281.5701\n",
      "iteration 4300 loss 281.56885\n",
      "iteration 4400 loss 281.5677\n",
      "iteration 4500 loss 281.56653\n",
      "iteration 4600 loss 281.56543\n",
      "iteration 4700 loss 281.56427\n",
      "iteration 4800 loss 281.5631\n",
      "iteration 4900 loss 281.56247\n",
      "iteration 5000 loss 281.56186Train finished\n",
      "Selected features are: [ 9 14 15 40 48 54 55 58 63 69]\n",
      "Selected weights are: [0.9996396 0.5664303 0.9996396 0.9996396 0.9996396 0.5726421 0.8262067\n",
      " 0.9996396 0.9996396 0.9996396]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0005305689302650571, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 20.31667593626629\n",
      "\n",
      " 10 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 278.16278\n",
      "iteration 200 loss 271.5743\n",
      "iteration 300 loss 267.54025\n",
      "iteration 400 loss 266.44507\n",
      "iteration 500 loss 265.55795\n",
      "iteration 600 loss 264.88956\n",
      "iteration 700 loss 264.31406\n",
      "iteration 800 loss 264.01514\n",
      "iteration 900 loss 263.74176\n",
      "iteration 1000 loss 263.46576\n",
      "iteration 1100 loss 263.19403\n",
      "iteration 1200 loss 262.9524\n",
      "iteration 1300 loss 262.7726\n",
      "iteration 1400 loss 262.68747\n",
      "iteration 1500 loss 262.63373\n",
      "iteration 1600 loss 262.58984\n",
      "iteration 1700 loss 262.57944\n",
      "iteration 1800 loss 262.57538\n",
      "iteration 1900 loss 262.57166\n",
      "iteration 2000 loss 262.5681\n",
      "iteration 2100 loss 262.5647\n",
      "iteration 2200 loss 262.5615\n",
      "iteration 2300 loss 262.55835\n",
      "iteration 2400 loss 262.5553\n",
      "iteration 2500 loss 262.5525\n",
      "iteration 2600 loss 262.54974\n",
      "iteration 2700 loss 262.54712\n",
      "iteration 2800 loss 262.54456\n",
      "iteration 2900 loss 262.54254\n",
      "iteration 3000 loss 262.54053\n",
      "iteration 3100 loss 262.53864\n",
      "iteration 3200 loss 262.53674\n",
      "iteration 3300 loss 262.53485\n",
      "iteration 3400 loss 262.53308\n",
      "iteration 3500 loss 262.5313\n",
      "iteration 3600 loss 262.52957\n",
      "iteration 3700 loss 262.5279\n",
      "iteration 3800 loss 262.52625\n",
      "iteration 3900 loss 262.5246\n",
      "iteration 4000 loss 262.52304\n",
      "iteration 4100 loss 262.5218\n",
      "iteration 4200 loss 262.5205\n",
      "iteration 4300 loss 262.51923\n",
      "iteration 4400 loss 262.518\n",
      "iteration 4500 loss 262.51663\n",
      "iteration 4600 loss 262.51526\n",
      "iteration 4700 loss 262.5138\n",
      "iteration 4800 loss 262.5123\n",
      "iteration 4900 loss 262.5107\n",
      "iteration 5000 loss 262.5091Train finished\n",
      "Selected features are: [ 7  8 10 11 13 14 26 46 49 70]\n",
      "Selected weights are: [0.82350904 0.99973536 0.99973536 0.93969655 0.96209407 0.99973536\n",
      " 0.99973536 0.99973536 0.99973536 0.99973536]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 9.791571168298286e-05, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 31.515823016883225\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 284.30927\n",
      "iteration 200 loss 280.08688\n",
      "iteration 300 loss 277.5047\n",
      "iteration 400 loss 276.37714\n",
      "iteration 500 loss 275.51617\n",
      "iteration 600 loss 274.7403\n",
      "iteration 700 loss 274.0213\n",
      "iteration 800 loss 273.73022\n",
      "iteration 900 loss 273.53772\n",
      "iteration 1000 loss 273.46155\n",
      "iteration 1100 loss 273.40454\n",
      "iteration 1200 loss 273.36084\n",
      "iteration 1300 loss 273.3186\n",
      "iteration 1400 loss 273.2859\n",
      "iteration 1500 loss 273.25903\n",
      "iteration 1600 loss 273.2337\n",
      "iteration 1700 loss 273.2224\n",
      "iteration 1800 loss 273.21625\n",
      "iteration 1900 loss 273.21042\n",
      "iteration 2000 loss 273.2049\n",
      "iteration 2100 loss 273.19965\n",
      "iteration 2200 loss 273.1946\n",
      "iteration 2300 loss 273.18982\n",
      "iteration 2400 loss 273.18524\n",
      "iteration 2500 loss 273.18088\n",
      "iteration 2600 loss 273.1778\n",
      "iteration 2700 loss 273.17712\n",
      "iteration 2800 loss 273.17664\n",
      "iteration 2900 loss 273.17624\n",
      "iteration 3000 loss 273.17584\n",
      "iteration 3100 loss 273.17548\n",
      "iteration 3200 loss 273.17517\n",
      "iteration 3300 loss 273.17487\n",
      "iteration 3400 loss 273.17456\n",
      "iteration 3500 loss 273.1743\n",
      "iteration 3600 loss 273.17404\n",
      "iteration 3700 loss 273.17383\n",
      "iteration 3800 loss 273.1736\n",
      "iteration 3900 loss 273.1734\n",
      "iteration 4000 loss 273.17322\n",
      "iteration 4100 loss 273.17303\n",
      "iteration 4200 loss 273.17285\n",
      "iteration 4300 loss 273.17273\n",
      "iteration 4400 loss 273.17255\n",
      "iteration 4500 loss 273.1724\n",
      "iteration 4600 loss 273.17224\n",
      "iteration 4700 loss 273.1723\n",
      "iteration 4800 loss 273.17233\n",
      "iteration 4900 loss 273.17233\n",
      "iteration 5000 loss 273.17236Train finished\n",
      "Selected features are: [11 26 33 37 49 51 52 56 62 70]\n",
      "Selected weights are: [0.99965566 0.99965566 0.51842165 0.99965566 0.99965566 0.54339844\n",
      " 0.99965566 0.99965566 0.99965566 0.99965566]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0030887806087567197, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 22.78388796307014\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 191.75284\n",
      "iteration 200 loss 189.26472\n",
      "iteration 300 loss 188.35095\n",
      "iteration 400 loss 187.81155\n",
      "iteration 500 loss 187.43271\n",
      "iteration 600 loss 187.23035\n",
      "iteration 700 loss 187.02878\n",
      "iteration 800 loss 186.83005\n",
      "iteration 900 loss 186.62167\n",
      "iteration 1000 loss 186.39943\n",
      "iteration 1100 loss 186.23212\n",
      "iteration 1200 loss 186.07773\n",
      "iteration 1300 loss 185.95863\n",
      "iteration 1400 loss 185.86221\n",
      "iteration 1500 loss 185.78421\n",
      "iteration 1600 loss 185.72128\n",
      "iteration 1700 loss 185.6831\n",
      "iteration 1800 loss 185.65375\n",
      "iteration 1900 loss 185.62753\n",
      "iteration 2000 loss 185.60384\n",
      "iteration 2100 loss 185.58228\n",
      "iteration 2200 loss 185.56248\n",
      "iteration 2300 loss 185.54428\n",
      "iteration 2400 loss 185.5276\n",
      "iteration 2500 loss 185.51305\n",
      "iteration 2600 loss 185.50299\n",
      "iteration 2700 loss 185.49359\n",
      "iteration 2800 loss 185.48471\n",
      "iteration 2900 loss 185.47626\n",
      "iteration 3000 loss 185.46819\n",
      "iteration 3100 loss 185.46054\n",
      "iteration 3200 loss 185.45523\n",
      "iteration 3300 loss 185.45142\n",
      "iteration 3400 loss 185.44846\n",
      "iteration 3500 loss 185.44562\n",
      "iteration 3600 loss 185.44284\n",
      "iteration 3700 loss 185.44016\n",
      "iteration 3800 loss 185.4375\n",
      "iteration 3900 loss 185.4349\n",
      "iteration 4000 loss 185.43237\n",
      "iteration 4100 loss 185.42987\n",
      "iteration 4200 loss 185.4274\n",
      "iteration 4300 loss 185.42499\n",
      "iteration 4400 loss 185.42258\n",
      "iteration 4500 loss 185.42044\n",
      "iteration 4600 loss 185.41946\n",
      "iteration 4700 loss 185.41867\n",
      "iteration 4800 loss 185.41791\n",
      "iteration 4900 loss 185.4172\n",
      "iteration 5000 loss 185.41653Train finished\n",
      "Selected features are: [ 7  9 15 36 40 48 51 54 58 69]\n",
      "Selected weights are: [0.5416694  0.9996573  0.9996573  0.9996573  0.9996573  0.9996573\n",
      " 0.9996573  0.8296824  0.918896   0.84300035]\n",
      "Best params obtained by cv: {'alpha': 0.001, 'coef0': 1, 'degree': 3, 'gamma': 0.00011718471782561779, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 66.08840272400785\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 303.462\n",
      "iteration 200 loss 298.66235\n",
      "iteration 300 loss 296.48816\n",
      "iteration 400 loss 295.64725\n",
      "iteration 500 loss 295.02945\n",
      "iteration 600 loss 294.77316\n",
      "iteration 700 loss 294.5896\n",
      "iteration 800 loss 294.5033\n",
      "iteration 900 loss 294.42126\n",
      "iteration 1000 loss 294.34094\n",
      "iteration 1100 loss 294.26248\n",
      "iteration 1200 loss 294.18555\n",
      "iteration 1300 loss 294.1131\n",
      "iteration 1400 loss 294.05383\n",
      "iteration 1500 loss 294.00488\n",
      "iteration 1600 loss 293.96356\n",
      "iteration 1700 loss 293.92978\n",
      "iteration 1800 loss 293.90244\n",
      "iteration 1900 loss 293.878\n",
      "iteration 2000 loss 293.8603\n",
      "iteration 2100 loss 293.85046\n",
      "iteration 2200 loss 293.84085\n",
      "iteration 2300 loss 293.83148\n",
      "iteration 2400 loss 293.82224\n",
      "iteration 2500 loss 293.81314\n",
      "iteration 2600 loss 293.8042\n",
      "iteration 2700 loss 293.7968\n",
      "iteration 2800 loss 293.7923\n",
      "iteration 2900 loss 293.78796\n",
      "iteration 3000 loss 293.784\n",
      "iteration 3100 loss 293.7808\n",
      "iteration 3200 loss 293.77747\n",
      "iteration 3300 loss 293.77536\n",
      "iteration 3400 loss 293.77335\n",
      "iteration 3500 loss 293.7713\n",
      "iteration 3600 loss 293.7693\n",
      "iteration 3700 loss 293.76733\n",
      "iteration 3800 loss 293.76538\n",
      "iteration 3900 loss 293.7635\n",
      "iteration 4000 loss 293.7616\n",
      "iteration 4100 loss 293.7597\n",
      "iteration 4200 loss 293.75787\n",
      "iteration 4300 loss 293.75604\n",
      "iteration 4400 loss 293.7542\n",
      "iteration 4500 loss 293.7534\n",
      "iteration 4600 loss 293.7533\n",
      "iteration 4700 loss 293.75272\n",
      "iteration 4800 loss 293.7527\n",
      "iteration 4900 loss 293.75275\n",
      "iteration 5000 loss 293.7527Train finished\n",
      "Selected features are: [ 2  9 14 15 24 40 48 58 63 69]\n",
      "Selected weights are: [0.99970406 0.99970406 0.48799706 0.99970406 0.99970406 0.99970406\n",
      " 0.99970406 0.99970406 0.99970406 0.99970406]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0006797228997006646, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 10.884689223811085\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 302.08038\n",
      "iteration 200 loss 292.888\n",
      "iteration 300 loss 290.75226\n",
      "iteration 400 loss 290.22467\n",
      "iteration 500 loss 289.75665\n",
      "iteration 600 loss 289.32523\n",
      "iteration 700 loss 288.91214\n",
      "iteration 800 loss 288.52148\n",
      "iteration 900 loss 288.23395\n",
      "iteration 1000 loss 288.01892\n",
      "iteration 1100 loss 287.90143\n",
      "iteration 1200 loss 287.84808\n",
      "iteration 1300 loss 287.8036\n",
      "iteration 1400 loss 287.7697\n",
      "iteration 1500 loss 287.74655\n",
      "iteration 1600 loss 287.736\n",
      "iteration 1700 loss 287.72662\n",
      "iteration 1800 loss 287.71802\n",
      "iteration 1900 loss 287.70996\n",
      "iteration 2000 loss 287.70233\n",
      "iteration 2100 loss 287.69498\n",
      "iteration 2200 loss 287.6879\n",
      "iteration 2300 loss 287.6811\n",
      "iteration 2400 loss 287.67462\n",
      "iteration 2500 loss 287.6684\n",
      "iteration 2600 loss 287.66693\n",
      "iteration 2700 loss 287.66614\n",
      "iteration 2800 loss 287.66553\n",
      "iteration 2900 loss 287.66498\n",
      "iteration 3000 loss 287.66446\n",
      "iteration 3100 loss 287.664\n",
      "iteration 3200 loss 287.66357\n",
      "iteration 3300 loss 287.66318\n",
      "iteration 3400 loss 287.6628\n",
      "iteration 3500 loss 287.66254\n",
      "iteration 3600 loss 287.66223\n",
      "iteration 3700 loss 287.66196\n",
      "iteration 3800 loss 287.66168\n",
      "iteration 3900 loss 287.66147\n",
      "iteration 4000 loss 287.66125\n",
      "iteration 4100 loss 287.661\n",
      "iteration 4200 loss 287.66083\n",
      "iteration 4300 loss 287.66064\n",
      "iteration 4400 loss 287.6605\n",
      "iteration 4500 loss 287.6603\n",
      "iteration 4600 loss 287.6602\n",
      "iteration 4700 loss 287.66\n",
      "iteration 4800 loss 287.65985\n",
      "iteration 4900 loss 287.65976\n",
      "iteration 5000 loss 287.6596Train finished\n",
      "Selected features are: [ 9 11 14 15 40 48 54 58 63 69]\n",
      "Selected weights are: [0.99958104 0.5077674  0.5403898  0.99958104 0.99958104 0.99958104\n",
      " 0.5874028  0.99958104 0.99958104 0.8634799 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00027735373049851816, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 18.785971277124435\n",
      "28.983945206110114 \n",
      "\n",
      "\n",
      " 1 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 235.60374\n",
      "iteration 200 loss 235.59206\n",
      "iteration 300 loss 235.58109\n",
      "iteration 400 loss 235.57068\n",
      "iteration 500 loss 235.56093\n",
      "iteration 600 loss 235.55182\n",
      "iteration 700 loss 235.54333\n",
      "iteration 800 loss 235.53548\n",
      "iteration 900 loss 235.52814\n",
      "iteration 1000 loss 235.52129\n",
      "iteration 1100 loss 235.51486\n",
      "iteration 1200 loss 235.50876\n",
      "iteration 1300 loss 235.50287\n",
      "iteration 1400 loss 235.49722\n",
      "iteration 1500 loss 235.49164\n",
      "iteration 1600 loss 235.48636\n",
      "iteration 1700 loss 235.48112\n",
      "iteration 1800 loss 235.4759\n",
      "iteration 1900 loss 235.47064\n",
      "iteration 2000 loss 235.46535\n",
      "iteration 2100 loss 235.45984\n",
      "iteration 2200 loss 235.45416\n",
      "iteration 2300 loss 235.44824\n",
      "iteration 2400 loss 235.44205\n",
      "iteration 2500 loss 235.43547\n",
      "iteration 2600 loss 235.42857\n",
      "iteration 2700 loss 235.4212\n",
      "iteration 2800 loss 235.41339\n",
      "iteration 2900 loss 235.40509\n",
      "iteration 3000 loss 235.39621\n",
      "iteration 3100 loss 235.38675\n",
      "iteration 3200 loss 235.37657\n",
      "iteration 3300 loss 235.3656\n",
      "iteration 3400 loss 235.35376\n",
      "iteration 3500 loss 235.34113\n",
      "iteration 3600 loss 235.32741\n",
      "iteration 3700 loss 235.31247\n",
      "iteration 3800 loss 235.29596\n",
      "iteration 3900 loss 235.2775\n",
      "iteration 4000 loss 235.25671\n",
      "iteration 4100 loss 235.23273\n",
      "iteration 4200 loss 235.20227\n",
      "iteration 4300 loss 235.15579\n",
      "iteration 4400 loss 235.0596\n",
      "iteration 4500 loss 234.75629\n",
      "iteration 4600 loss 233.71364\n",
      "iteration 4700 loss 232.08163\n",
      "iteration 4800 loss 231.07529\n",
      "iteration 4900 loss 230.29301\n",
      "iteration 5000 loss 229.51492Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         0.99940723 1.         1.         0.9997454  0.99996966\n",
      " 0.9997002  0.01440681 0.         0.7741894  0.5438261  0.25407696\n",
      " 0.995362   1.         0.2053428  1.         1.         1.\n",
      " 1.         0.9991357  1.         1.         0.99808997 0.97132754\n",
      " 1.         0.99878246 0.6439214  0.9986232  1.         0.997231\n",
      " 0.99898815 1.         0.99874777 1.         1.         1.\n",
      " 0.757321   0.9999342  1.         1.         0.08577176 1.\n",
      " 1.         0.9952548  1.         1.         1.         1.\n",
      " 0.7713326  0.27326956 0.9990589  1.         0.9200438  0.9787522\n",
      " 1.         0.9879362  1.         1.         1.         0.9780372\n",
      " 1.         1.         0.9371576  1.         0.99976987 1.\n",
      " 0.9811024  1.         0.99969125 0.72945106 0.3996273  0.9823942\n",
      " 0.99375725 0.99702287 0.9992175  1.         1.        ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00013732136383690508, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 41.232183401752444\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 318.721\n",
      "iteration 200 loss 318.71393\n",
      "iteration 300 loss 318.70752\n",
      "iteration 400 loss 318.70163\n",
      "iteration 500 loss 318.69623\n",
      "iteration 600 loss 318.69135\n",
      "iteration 700 loss 318.68704\n",
      "iteration 800 loss 318.68304\n",
      "iteration 900 loss 318.67957\n",
      "iteration 1000 loss 318.67636\n",
      "iteration 1100 loss 318.67358\n",
      "iteration 1200 loss 318.67102\n",
      "iteration 1300 loss 318.66876\n",
      "iteration 1400 loss 318.6667\n",
      "iteration 1500 loss 318.66495\n",
      "iteration 1600 loss 318.66333\n",
      "iteration 1700 loss 318.66187\n",
      "iteration 1800 loss 318.66055\n",
      "iteration 1900 loss 318.6594\n",
      "iteration 2000 loss 318.65845\n",
      "iteration 2100 loss 318.65744\n",
      "iteration 2200 loss 318.6567\n",
      "iteration 2300 loss 318.65594\n",
      "iteration 2400 loss 318.65527\n",
      "iteration 2500 loss 318.65466\n",
      "iteration 2600 loss 318.65417\n",
      "iteration 2700 loss 318.6537\n",
      "iteration 2800 loss 318.65326\n",
      "iteration 2900 loss 318.65283\n",
      "iteration 3000 loss 318.65244\n",
      "iteration 3100 loss 318.6521\n",
      "iteration 3200 loss 318.6518\n",
      "iteration 3300 loss 318.65152\n",
      "iteration 3400 loss 318.65125\n",
      "iteration 3500 loss 318.65103\n",
      "iteration 3600 loss 318.65076\n",
      "iteration 3700 loss 318.65054\n",
      "iteration 3800 loss 318.65027\n",
      "iteration 3900 loss 318.65012\n",
      "iteration 4000 loss 318.6499\n",
      "iteration 4100 loss 318.6497\n",
      "iteration 4200 loss 318.64948\n",
      "iteration 4300 loss 318.6493\n",
      "iteration 4400 loss 318.6491\n",
      "iteration 4500 loss 318.64896\n",
      "iteration 4600 loss 318.64877\n",
      "iteration 4700 loss 318.64856\n",
      "iteration 4800 loss 318.6484\n",
      "iteration 4900 loss 318.64822\n",
      "iteration 5000 loss 318.64804Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         1.         1.         1.         0.99958634 1.\n",
      " 0.99714965 1.         0.51273865 1.         0.9677093  1.\n",
      " 0.9996269  1.         1.         1.         1.         1.\n",
      " 0.9997578  1.         1.         1.         0.99857736 0.9731889\n",
      " 1.         0.9987314  1.         1.         1.         0.9978829\n",
      " 0.9995247  1.         1.         1.         1.         1.\n",
      " 0.87086177 1.         1.         1.         1.         1.\n",
      " 1.         0.9988016  1.         1.         1.         1.\n",
      " 1.         1.         0.9994365  1.         1.         0.99274075\n",
      " 1.         1.         1.         1.         1.         0.99072796\n",
      " 1.         1.         1.         1.         1.         0.99998397\n",
      " 0.99859107 0.9997548  1.         1.         1.         0.99275935\n",
      " 0.99830186 0.99915737 0.99909574 1.         0.9989905 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00010761524962494646, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 9.67379017620879\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 259.47424\n",
      "iteration 200 loss 259.45648\n",
      "iteration 300 loss 259.4413\n",
      "iteration 400 loss 259.42825\n",
      "iteration 500 loss 259.41678\n",
      "iteration 600 loss 259.40674\n",
      "iteration 700 loss 259.39767\n",
      "iteration 800 loss 259.3895\n",
      "iteration 900 loss 259.382\n",
      "iteration 1000 loss 259.37497\n",
      "iteration 1100 loss 259.36838\n",
      "iteration 1200 loss 259.3621\n",
      "iteration 1300 loss 259.35608\n",
      "iteration 1400 loss 259.35016\n",
      "iteration 1500 loss 259.34448\n",
      "iteration 1600 loss 259.3388\n",
      "iteration 1700 loss 259.3332\n",
      "iteration 1800 loss 259.32764\n",
      "iteration 1900 loss 259.32202\n",
      "iteration 2000 loss 259.31635\n",
      "iteration 2100 loss 259.31055\n",
      "iteration 2200 loss 259.30463\n",
      "iteration 2300 loss 259.29852\n",
      "iteration 2400 loss 259.29224\n",
      "iteration 2500 loss 259.28555\n",
      "iteration 2600 loss 259.2785\n",
      "iteration 2700 loss 259.27112\n",
      "iteration 2800 loss 259.2632\n",
      "iteration 2900 loss 259.25467\n",
      "iteration 3000 loss 259.24536\n",
      "iteration 3100 loss 259.2348\n",
      "iteration 3200 loss 259.22247\n",
      "iteration 3300 loss 259.20764\n",
      "iteration 3400 loss 259.1892\n",
      "iteration 3500 loss 259.16577\n",
      "iteration 3600 loss 259.13525\n",
      "iteration 3700 loss 259.0948\n",
      "iteration 3800 loss 259.0404\n",
      "iteration 3900 loss 258.96692\n",
      "iteration 4000 loss 258.86774\n",
      "iteration 4100 loss 258.7351\n",
      "iteration 4200 loss 258.56097\n",
      "iteration 4300 loss 258.3382\n",
      "iteration 4400 loss 258.05243\n",
      "iteration 4500 loss 257.6289\n",
      "iteration 4600 loss 256.86844\n",
      "iteration 4700 loss 255.63867\n",
      "iteration 4800 loss 254.00195\n",
      "iteration 4900 loss 252.30508\n",
      "iteration 5000 loss 251.60254Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [0.9882082  0.9999802  0.7817017  0.9944772  1.         1.\n",
      " 1.         0.6014389  0.         0.4261126  0.32133922 0.67553824\n",
      " 1.         1.         1.         0.         0.995752   1.\n",
      " 1.         0.99818075 1.         0.99973226 0.9978166  0.8405385\n",
      " 0.77850914 1.         1.         1.         0.9981077  1.\n",
      " 1.         0.92841095 1.         0.99236137 0.9997407  0.9954631\n",
      " 0.         1.         0.9995362  0.99974966 0.28644463 1.\n",
      " 1.         0.9956578  1.         0.9679197  1.         0.9999503\n",
      " 0.         1.         0.9976048  0.75905603 1.         1.\n",
      " 0.         0.97738683 1.         1.         0.         1.\n",
      " 1.         0.9909128  1.         0.75692296 0.97987235 1.\n",
      " 0.96547425 1.         0.9948483  0.54487586 1.         0.9923575\n",
      " 1.         0.99397963 0.9985489  1.         0.9193067 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 8.767354876193864e-05, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 32.49369018523654\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 230.18771\n",
      "iteration 200 loss 230.18771\n",
      "iteration 300 loss 230.18771\n",
      "iteration 400 loss 230.18771\n",
      "iteration 500 loss 230.18771\n",
      "iteration 600 loss 230.18771\n",
      "iteration 700 loss 230.18771\n",
      "iteration 800 loss 230.18771\n",
      "iteration 900 loss 230.18771\n",
      "iteration 1000 loss 230.18771\n",
      "iteration 1100 loss 230.18771\n",
      "iteration 1200 loss 230.18771\n",
      "iteration 1300 loss 230.18771\n",
      "iteration 1400 loss 230.18771\n",
      "iteration 1500 loss 230.18771\n",
      "iteration 1600 loss 230.18771\n",
      "iteration 1700 loss 230.18771\n",
      "iteration 1800 loss 230.18771\n",
      "iteration 1900 loss 230.18771\n",
      "iteration 2000 loss 230.18771\n",
      "iteration 2100 loss 230.18771\n",
      "iteration 2200 loss 230.18771\n",
      "iteration 2300 loss 230.18771\n",
      "iteration 2400 loss 230.18771\n",
      "iteration 2500 loss 230.18771\n",
      "iteration 2600 loss 230.18771\n",
      "iteration 2700 loss 230.18773\n",
      "iteration 2800 loss 230.18771\n",
      "iteration 2900 loss 230.18771\n",
      "iteration 3000 loss 230.18771\n",
      "iteration 3100 loss 230.18771\n",
      "iteration 3200 loss 230.18771\n",
      "iteration 3300 loss 230.18771\n",
      "iteration 3400 loss 230.18771\n",
      "iteration 3500 loss 230.18771\n",
      "iteration 3600 loss 230.18771\n",
      "iteration 3700 loss 230.18771\n",
      "iteration 3800 loss 230.18771\n",
      "iteration 3900 loss 230.18771\n",
      "iteration 4000 loss 230.18771\n",
      "iteration 4100 loss 230.18771\n",
      "iteration 4200 loss 230.1877\n",
      "iteration 4300 loss 230.1877\n",
      "iteration 4400 loss 230.18771\n",
      "iteration 4500 loss 230.1877\n",
      "iteration 4600 loss 230.1877\n",
      "iteration 4700 loss 230.1877\n",
      "iteration 4800 loss 230.1877\n",
      "iteration 4900 loss 230.1877\n",
      "iteration 5000 loss 230.1877Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        0.9961257 1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.       ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00012592340568893195, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 48.22405000097481\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 313.54626\n",
      "iteration 200 loss 313.54623\n",
      "iteration 300 loss 313.54614\n",
      "iteration 400 loss 313.54608\n",
      "iteration 500 loss 313.54605\n",
      "iteration 600 loss 313.54596\n",
      "iteration 700 loss 313.5459\n",
      "iteration 800 loss 313.54584\n",
      "iteration 900 loss 313.5458\n",
      "iteration 1000 loss 313.54578\n",
      "iteration 1100 loss 313.54565\n",
      "iteration 1200 loss 313.5456\n",
      "iteration 1300 loss 313.54553\n",
      "iteration 1400 loss 313.54547\n",
      "iteration 1500 loss 313.5454\n",
      "iteration 1600 loss 313.5454\n",
      "iteration 1700 loss 313.5453\n",
      "iteration 1800 loss 313.5453\n",
      "iteration 1900 loss 313.5452\n",
      "iteration 2000 loss 313.54514\n",
      "iteration 2100 loss 313.5451\n",
      "iteration 2200 loss 313.54504\n",
      "iteration 2300 loss 313.54498\n",
      "iteration 2400 loss 313.54498\n",
      "iteration 2500 loss 313.54486\n",
      "iteration 2600 loss 313.54483\n",
      "iteration 2700 loss 313.5448\n",
      "iteration 2800 loss 313.54474\n",
      "iteration 2900 loss 313.54474\n",
      "iteration 3000 loss 313.54468\n",
      "iteration 3100 loss 313.54462\n",
      "iteration 3200 loss 313.54456\n",
      "iteration 3300 loss 313.54453\n",
      "iteration 3400 loss 313.54443\n",
      "iteration 3500 loss 313.5444\n",
      "iteration 3600 loss 313.5444\n",
      "iteration 3700 loss 313.5443\n",
      "iteration 3800 loss 313.54428\n",
      "iteration 3900 loss 313.54428\n",
      "iteration 4000 loss 313.54422\n",
      "iteration 4100 loss 313.54416\n",
      "iteration 4200 loss 313.54407\n",
      "iteration 4300 loss 313.544\n",
      "iteration 4400 loss 313.544\n",
      "iteration 4500 loss 313.54398\n",
      "iteration 4600 loss 313.5439\n",
      "iteration 4700 loss 313.54388\n",
      "iteration 4800 loss 313.54385\n",
      "iteration 4900 loss 313.5438\n",
      "iteration 5000 loss 313.54376Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         1.         0.9972206  1.         1.         1.\n",
      " 1.         1.         1.         0.9986038  1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.99942106 1.         1.         1.         1.\n",
      " 0.88692117 1.         1.         1.         1.         1.\n",
      " 1.         0.9999212  1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.9993948  1.         1.         1.         1.         0.9998012\n",
      " 1.         0.999758   1.         1.         0.99866325]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00011306289450764415, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 12.22268651862746\n",
      "\n",
      " 2 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 249.47043\n",
      "iteration 200 loss 249.47043\n",
      "iteration 300 loss 249.47043\n",
      "iteration 400 loss 249.47043\n",
      "iteration 500 loss 249.4704\n",
      "iteration 600 loss 249.47043\n",
      "iteration 700 loss 249.47043\n",
      "iteration 800 loss 249.47043\n",
      "iteration 900 loss 249.47043\n",
      "iteration 1000 loss 249.47043\n",
      "iteration 1100 loss 249.47044\n",
      "iteration 1200 loss 249.4704\n",
      "iteration 1300 loss 249.47043\n",
      "iteration 1400 loss 249.47041\n",
      "iteration 1500 loss 249.47043\n",
      "iteration 1600 loss 249.47043\n",
      "iteration 1700 loss 249.47043\n",
      "iteration 1800 loss 249.47043\n",
      "iteration 1900 loss 249.47044\n",
      "iteration 2000 loss 249.47043\n",
      "iteration 2100 loss 249.47043\n",
      "iteration 2200 loss 249.47043\n",
      "iteration 2300 loss 249.47043\n",
      "iteration 2400 loss 249.47043\n",
      "iteration 2500 loss 249.47043\n",
      "iteration 2600 loss 249.47043\n",
      "iteration 2700 loss 249.47043\n",
      "iteration 2800 loss 249.47043\n",
      "iteration 2900 loss 249.47043\n",
      "iteration 3000 loss 249.47043\n",
      "iteration 3100 loss 249.47043\n",
      "iteration 3200 loss 249.47043\n",
      "iteration 3300 loss 249.47043\n",
      "iteration 3400 loss 249.47043\n",
      "iteration 3500 loss 249.47043\n",
      "iteration 3600 loss 249.47043\n",
      "iteration 3700 loss 249.47043\n",
      "iteration 3800 loss 249.47041\n",
      "iteration 3900 loss 249.47041\n",
      "iteration 4000 loss 249.47043\n",
      "iteration 4100 loss 249.47041\n",
      "iteration 4200 loss 249.47043\n",
      "iteration 4300 loss 249.47041\n",
      "iteration 4400 loss 249.47041\n",
      "iteration 4500 loss 249.47043\n",
      "iteration 4600 loss 249.47043\n",
      "iteration 4700 loss 249.47043\n",
      "iteration 4800 loss 249.47043\n",
      "iteration 4900 loss 249.47043\n",
      "iteration 5000 loss 249.47041Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        0.9944946 1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.       ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00012514603015623439, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 40.533267761692926\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 297.21906\n",
      "iteration 200 loss 297.2155\n",
      "iteration 300 loss 297.21222\n",
      "iteration 400 loss 297.20895\n",
      "iteration 500 loss 297.20584\n",
      "iteration 600 loss 297.20276\n",
      "iteration 700 loss 297.1999\n",
      "iteration 800 loss 297.19708\n",
      "iteration 900 loss 297.19446\n",
      "iteration 1000 loss 297.1919\n",
      "iteration 1100 loss 297.18948\n",
      "iteration 1200 loss 297.18713\n",
      "iteration 1300 loss 297.18494\n",
      "iteration 1400 loss 297.18283\n",
      "iteration 1500 loss 297.18082\n",
      "iteration 1600 loss 297.1789\n",
      "iteration 1700 loss 297.17706\n",
      "iteration 1800 loss 297.17535\n",
      "iteration 1900 loss 297.17374\n",
      "iteration 2000 loss 297.17215\n",
      "iteration 2100 loss 297.17072\n",
      "iteration 2200 loss 297.16925\n",
      "iteration 2300 loss 297.16785\n",
      "iteration 2400 loss 297.1666\n",
      "iteration 2500 loss 297.1653\n",
      "iteration 2600 loss 297.16412\n",
      "iteration 2700 loss 297.16296\n",
      "iteration 2800 loss 297.16193\n",
      "iteration 2900 loss 297.16083\n",
      "iteration 3000 loss 297.1598\n",
      "iteration 3100 loss 297.15894\n",
      "iteration 3200 loss 297.15796\n",
      "iteration 3300 loss 297.15698\n",
      "iteration 3400 loss 297.15613\n",
      "iteration 3500 loss 297.1553\n",
      "iteration 3600 loss 297.15448\n",
      "iteration 3700 loss 297.1537\n",
      "iteration 3800 loss 297.15295\n",
      "iteration 3900 loss 297.15216\n",
      "iteration 4000 loss 297.15143\n",
      "iteration 4100 loss 297.15076\n",
      "iteration 4200 loss 297.15002\n",
      "iteration 4300 loss 297.14935\n",
      "iteration 4400 loss 297.14868\n",
      "iteration 4500 loss 297.148\n",
      "iteration 4600 loss 297.14737\n",
      "iteration 4700 loss 297.14676\n",
      "iteration 4800 loss 297.1461\n",
      "iteration 4900 loss 297.14545\n",
      "iteration 5000 loss 297.1447Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         1.         1.         1.         0.99963915 1.\n",
      " 0.99461013 1.         0.5252064  1.         0.90301085 1.\n",
      " 0.9991901  1.         1.         1.         1.         0.9970595\n",
      " 0.99622434 0.9998995  1.         1.         0.99923694 0.9883416\n",
      " 1.         0.99844944 1.         0.99959207 1.         0.9950579\n",
      " 0.99914694 1.         0.99890304 1.         1.         1.\n",
      " 0.68198115 1.         1.         1.         1.         1.\n",
      " 1.         0.99904704 1.         1.         1.         1.\n",
      " 1.         1.         0.9997     1.         1.         0.97868896\n",
      " 1.         1.         1.         0.9999367  1.         0.98703176\n",
      " 1.         1.         0.9722467  1.         1.         1.\n",
      " 0.99783266 0.9992066  1.         1.         0.989556   0.9963734\n",
      " 0.9973765  0.9989151  0.9993521  1.         1.        ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00012106928726438706, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 17.599559571840103\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 267.96698\n",
      "iteration 200 loss 267.9641\n",
      "iteration 300 loss 267.96057\n",
      "iteration 400 loss 267.95612\n",
      "iteration 500 loss 267.9506\n",
      "iteration 600 loss 267.9439\n",
      "iteration 700 loss 267.93585\n",
      "iteration 800 loss 267.92645\n",
      "iteration 900 loss 267.91574\n",
      "iteration 1000 loss 267.90396\n",
      "iteration 1100 loss 267.8913\n",
      "iteration 1200 loss 267.878\n",
      "iteration 1300 loss 267.86444\n",
      "iteration 1400 loss 267.8507\n",
      "iteration 1500 loss 267.83698\n",
      "iteration 1600 loss 267.82324\n",
      "iteration 1700 loss 267.8095\n",
      "iteration 1800 loss 267.7957\n",
      "iteration 1900 loss 267.78165\n",
      "iteration 2000 loss 267.76727\n",
      "iteration 2100 loss 267.75244\n",
      "iteration 2200 loss 267.73712\n",
      "iteration 2300 loss 267.72116\n",
      "iteration 2400 loss 267.70444\n",
      "iteration 2500 loss 267.68698\n",
      "iteration 2600 loss 267.66867\n",
      "iteration 2700 loss 267.64935\n",
      "iteration 2800 loss 267.62918\n",
      "iteration 2900 loss 267.60794\n",
      "iteration 3000 loss 267.5855\n",
      "iteration 3100 loss 267.56165\n",
      "iteration 3200 loss 267.536\n",
      "iteration 3300 loss 267.50793\n",
      "iteration 3400 loss 267.47647\n",
      "iteration 3500 loss 267.44006\n",
      "iteration 3600 loss 267.39648\n",
      "iteration 3700 loss 267.34247\n",
      "iteration 3800 loss 267.27316\n",
      "iteration 3900 loss 267.20496\n",
      "iteration 4000 loss 267.1007\n",
      "iteration 4100 loss 266.9293\n",
      "iteration 4200 loss 266.63602\n",
      "iteration 4300 loss 266.14655\n",
      "iteration 4400 loss 265.4636\n",
      "iteration 4500 loss 264.40765\n",
      "iteration 4600 loss 262.9938\n",
      "iteration 4700 loss 261.7581\n",
      "iteration 4800 loss 261.1314\n",
      "iteration 4900 loss 260.60507\n",
      "iteration 5000 loss 260.2353Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [0.98600376 0.9989589  0.6687319  1.         1.         1.\n",
      " 1.         0.7315316  1.         0.70063704 1.         1.\n",
      " 1.         0.9999952  1.         0.         0.9932975  1.\n",
      " 1.         1.         0.9853046  1.         0.9935183  0.82909644\n",
      " 0.8744147  1.         1.         1.         0.9989712  1.\n",
      " 1.         0.9108205  1.         0.95711786 0.99939734 0.9872035\n",
      " 0.37088132 0.90385807 1.         1.         0.         1.\n",
      " 1.         0.9965719  1.         0.9102577  1.         1.\n",
      " 0.         1.         0.9991656  0.         1.         1.\n",
      " 0.         1.         1.         1.         0.         1.\n",
      " 0.8877292  0.9763388  1.         0.3675695  0.97886074 0.9986159\n",
      " 1.         1.         0.99509025 0.         0.49971595 0.96137106\n",
      " 1.         1.         0.998253   0.9995085  0.99077904]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0001200559335241225, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 30.01714323538385\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 280.02444\n",
      "iteration 200 loss 279.9903\n",
      "iteration 300 loss 279.95728\n",
      "iteration 400 loss 279.92453\n",
      "iteration 500 loss 279.8911\n",
      "iteration 600 loss 279.8564\n",
      "iteration 700 loss 279.81995\n",
      "iteration 800 loss 279.78162\n",
      "iteration 900 loss 279.74237\n",
      "iteration 1000 loss 279.70306\n",
      "iteration 1100 loss 279.6647\n",
      "iteration 1200 loss 279.628\n",
      "iteration 1300 loss 279.5932\n",
      "iteration 1400 loss 279.56006\n",
      "iteration 1500 loss 279.52805\n",
      "iteration 1600 loss 279.4966\n",
      "iteration 1700 loss 279.46533\n",
      "iteration 1800 loss 279.43378\n",
      "iteration 1900 loss 279.40155\n",
      "iteration 2000 loss 279.3684\n",
      "iteration 2100 loss 279.33405\n",
      "iteration 2200 loss 279.29828\n",
      "iteration 2300 loss 279.2608\n",
      "iteration 2400 loss 279.22147\n",
      "iteration 2500 loss 279.17984\n",
      "iteration 2600 loss 279.13556\n",
      "iteration 2700 loss 279.08807\n",
      "iteration 2800 loss 279.042\n",
      "iteration 2900 loss 278.9934\n",
      "iteration 3000 loss 278.94012\n",
      "iteration 3100 loss 278.88126\n",
      "iteration 3200 loss 278.81573\n",
      "iteration 3300 loss 278.742\n",
      "iteration 3400 loss 278.65527\n",
      "iteration 3500 loss 278.5168\n",
      "iteration 3600 loss 277.88538\n",
      "iteration 3700 loss 270.65118\n",
      "iteration 3800 loss 269.3984\n",
      "iteration 3900 loss 268.55746\n",
      "iteration 4000 loss 267.92337\n",
      "iteration 4100 loss 267.87704\n",
      "iteration 4200 loss 267.859\n",
      "iteration 4300 loss 267.84744\n",
      "iteration 4400 loss 267.83902\n",
      "iteration 4500 loss 267.8319\n",
      "iteration 4600 loss 267.82553\n",
      "iteration 4700 loss 267.81946\n",
      "iteration 4800 loss 267.81348\n",
      "iteration 4900 loss 267.80762\n",
      "iteration 5000 loss 267.80173Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         1.         1.         1.         0.99816114 0.99923146\n",
      " 0.9596521  0.         0.         0.61734587 0.         0.\n",
      " 0.9938823  0.8983274  0.19708414 1.         0.99918956 0.9874301\n",
      " 0.9800249  1.         1.         1.         0.9919644  1.\n",
      " 1.         0.98805207 0.         0.9948932  1.         0.93943244\n",
      " 0.99305034 1.         0.99568504 1.         1.         1.\n",
      " 1.         1.         1.         1.         0.5259474  1.\n",
      " 1.         0.9884973  0.8038209  1.         1.         1.\n",
      " 0.970415   0.         0.9945112  1.         1.         0.83270335\n",
      " 1.         1.         1.         0.9826428  1.         0.9664075\n",
      " 1.         1.         1.         1.         0.8692602  1.\n",
      " 0.9332108  0.9885771  1.         1.         0.         0.9735338\n",
      " 0.9837102  0.9901222  1.         1.         1.        ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 8.951254460772456e-05, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 23.433120561147803\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 264.42252\n",
      "iteration 200 loss 264.40176\n",
      "iteration 300 loss 264.37982\n",
      "iteration 400 loss 264.3568\n",
      "iteration 500 loss 264.33307\n",
      "iteration 600 loss 264.30896\n",
      "iteration 700 loss 264.28473\n",
      "iteration 800 loss 264.26074\n",
      "iteration 900 loss 264.23706\n",
      "iteration 1000 loss 264.21387\n",
      "iteration 1100 loss 264.1913\n",
      "iteration 1200 loss 264.1695\n",
      "iteration 1300 loss 264.14838\n",
      "iteration 1400 loss 264.12793\n",
      "iteration 1500 loss 264.10822\n",
      "iteration 1600 loss 264.08917\n",
      "iteration 1700 loss 264.07068\n",
      "iteration 1800 loss 264.05286\n",
      "iteration 1900 loss 264.03546\n",
      "iteration 2000 loss 264.01855\n",
      "iteration 2100 loss 264.002\n",
      "iteration 2200 loss 263.98584\n",
      "iteration 2300 loss 263.9699\n",
      "iteration 2400 loss 263.95422\n",
      "iteration 2500 loss 263.93872\n",
      "iteration 2600 loss 263.92328\n",
      "iteration 2700 loss 263.90793\n",
      "iteration 2800 loss 263.89246\n",
      "iteration 2900 loss 263.87677\n",
      "iteration 3000 loss 263.8605\n",
      "iteration 3100 loss 263.84344\n",
      "iteration 3200 loss 263.82538\n",
      "iteration 3300 loss 263.80585\n",
      "iteration 3400 loss 263.7845\n",
      "iteration 3500 loss 263.76074\n",
      "iteration 3600 loss 263.7337\n",
      "iteration 3700 loss 263.70227\n",
      "iteration 3800 loss 263.66473\n",
      "iteration 3900 loss 263.6188\n",
      "iteration 4000 loss 263.56073\n",
      "iteration 4100 loss 263.4859\n",
      "iteration 4200 loss 263.38638\n",
      "iteration 4300 loss 263.24875\n",
      "iteration 4400 loss 263.05304\n",
      "iteration 4500 loss 262.7771\n",
      "iteration 4600 loss 262.4099\n",
      "iteration 4700 loss 261.97388\n",
      "iteration 4800 loss 261.5552\n",
      "iteration 4900 loss 261.2107\n",
      "iteration 5000 loss 260.8799Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [9.9257642e-01 9.9959213e-01 7.8990918e-01 1.0000000e+00 1.0000000e+00\n",
      " 1.0000000e+00 1.0000000e+00 1.0000000e+00 3.3809778e-03 4.8995355e-01\n",
      " 7.8705186e-01 2.0460605e-04 1.0000000e+00 1.0000000e+00 1.0000000e+00\n",
      " 4.2173007e-01 1.0000000e+00 1.0000000e+00 1.0000000e+00 9.9877602e-01\n",
      " 1.0000000e+00 1.0000000e+00 9.9430704e-01 9.4385302e-01 8.3387440e-01\n",
      " 9.9936539e-01 1.0000000e+00 1.0000000e+00 9.9843568e-01 9.9905211e-01\n",
      " 1.0000000e+00 9.6867281e-01 1.0000000e+00 1.0000000e+00 9.9997056e-01\n",
      " 9.8562139e-01 3.6898026e-01 1.0000000e+00 9.9966466e-01 9.9906659e-01\n",
      " 0.0000000e+00 1.0000000e+00 1.0000000e+00 9.9468499e-01 1.0000000e+00\n",
      " 9.6025205e-01 1.0000000e+00 1.0000000e+00 0.0000000e+00 1.0000000e+00\n",
      " 9.9883264e-01 8.7571329e-01 1.0000000e+00 1.0000000e+00 7.4787545e-01\n",
      " 8.1083208e-01 1.0000000e+00 1.0000000e+00 0.0000000e+00 1.0000000e+00\n",
      " 1.0000000e+00 9.8259443e-01 1.0000000e+00 6.7014110e-01 9.2116338e-01\n",
      " 9.9866170e-01 9.7792321e-01 9.9728602e-01 9.9534714e-01 9.8305148e-01\n",
      " 1.0000000e+00 9.7543603e-01 1.0000000e+00 9.9526477e-01 9.9356657e-01\n",
      " 1.0000000e+00 9.3614852e-01]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00010272053536227772, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 32.297908915777484\n",
      "\n",
      " 3 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 294.5933\n",
      "iteration 200 loss 294.5929\n",
      "iteration 300 loss 294.59256\n",
      "iteration 400 loss 294.59216\n",
      "iteration 500 loss 294.5918\n",
      "iteration 600 loss 294.59143\n",
      "iteration 700 loss 294.59113\n",
      "iteration 800 loss 294.59076\n",
      "iteration 900 loss 294.59045\n",
      "iteration 1000 loss 294.59015\n",
      "iteration 1100 loss 294.58987\n",
      "iteration 1200 loss 294.5896\n",
      "iteration 1300 loss 294.5893\n",
      "iteration 1400 loss 294.58902\n",
      "iteration 1500 loss 294.58875\n",
      "iteration 1600 loss 294.58844\n",
      "iteration 1700 loss 294.5882\n",
      "iteration 1800 loss 294.58792\n",
      "iteration 1900 loss 294.5877\n",
      "iteration 2000 loss 294.58743\n",
      "iteration 2100 loss 294.5872\n",
      "iteration 2200 loss 294.58698\n",
      "iteration 2300 loss 294.58673\n",
      "iteration 2400 loss 294.58652\n",
      "iteration 2500 loss 294.5863\n",
      "iteration 2600 loss 294.5861\n",
      "iteration 2700 loss 294.58588\n",
      "iteration 2800 loss 294.58566\n",
      "iteration 2900 loss 294.58548\n",
      "iteration 3000 loss 294.58533\n",
      "iteration 3100 loss 294.5851\n",
      "iteration 3200 loss 294.58496\n",
      "iteration 3300 loss 294.58478\n",
      "iteration 3400 loss 294.5846\n",
      "iteration 3500 loss 294.5844\n",
      "iteration 3600 loss 294.58423\n",
      "iteration 3700 loss 294.58408\n",
      "iteration 3800 loss 294.58392\n",
      "iteration 3900 loss 294.58377\n",
      "iteration 4000 loss 294.58362\n",
      "iteration 4100 loss 294.58347\n",
      "iteration 4200 loss 294.58334\n",
      "iteration 4300 loss 294.5832\n",
      "iteration 4400 loss 294.58304\n",
      "iteration 4500 loss 294.58295\n",
      "iteration 4600 loss 294.5828\n",
      "iteration 4700 loss 294.5827\n",
      "iteration 4800 loss 294.58255\n",
      "iteration 4900 loss 294.5824\n",
      "iteration 5000 loss 294.58234Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         1.         0.99950427 0.9997341  1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.9860455\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.9970179  1.         1.         1.         0.99990666\n",
      " 0.76579046 1.         1.         0.999911   1.         1.\n",
      " 1.         0.99995565 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.99929285\n",
      " 1.         1.         0.9998862  1.         1.         1.\n",
      " 1.         0.99956983 1.         1.         0.9999134 ]\n",
      "Best params obtained by cv: {'alpha': 0.0001, 'coef0': 1, 'degree': 3, 'gamma': 9.697433093266935e-05, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 29.34160842725093\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 238.08382\n",
      "iteration 200 loss 238.05463\n",
      "iteration 300 loss 238.02646\n",
      "iteration 400 loss 238.00003\n",
      "iteration 500 loss 237.97601\n",
      "iteration 600 loss 237.95483\n",
      "iteration 700 loss 237.93663\n",
      "iteration 800 loss 237.92117\n",
      "iteration 900 loss 237.90826\n",
      "iteration 1000 loss 237.89737\n",
      "iteration 1100 loss 237.8882\n",
      "iteration 1200 loss 237.88031\n",
      "iteration 1300 loss 237.8734\n",
      "iteration 1400 loss 237.8673\n",
      "iteration 1500 loss 237.86174\n",
      "iteration 1600 loss 237.85658\n",
      "iteration 1700 loss 237.85179\n",
      "iteration 1800 loss 237.84717\n",
      "iteration 1900 loss 237.8428\n",
      "iteration 2000 loss 237.83853\n",
      "iteration 2100 loss 237.83441\n",
      "iteration 2200 loss 237.83038\n",
      "iteration 2300 loss 237.82639\n",
      "iteration 2400 loss 237.82248\n",
      "iteration 2500 loss 237.81853\n",
      "iteration 2600 loss 237.81467\n",
      "iteration 2700 loss 237.8108\n",
      "iteration 2800 loss 237.807\n",
      "iteration 2900 loss 237.8031\n",
      "iteration 3000 loss 237.79932\n",
      "iteration 3100 loss 237.7955\n",
      "iteration 3200 loss 237.79164\n",
      "iteration 3300 loss 237.78781\n",
      "iteration 3400 loss 237.78395\n",
      "iteration 3500 loss 237.78009\n",
      "iteration 3600 loss 237.77621\n",
      "iteration 3700 loss 237.77237\n",
      "iteration 3800 loss 237.76843\n",
      "iteration 3900 loss 237.76453\n",
      "iteration 4000 loss 237.76056\n",
      "iteration 4100 loss 237.75656\n",
      "iteration 4200 loss 237.75261\n",
      "iteration 4300 loss 237.74858\n",
      "iteration 4400 loss 237.74452\n",
      "iteration 4500 loss 237.74045\n",
      "iteration 4600 loss 237.73633\n",
      "iteration 4700 loss 237.73216\n",
      "iteration 4800 loss 237.72797\n",
      "iteration 4900 loss 237.72377\n",
      "iteration 5000 loss 237.71951Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [0.99680966 1.         0.90587354 1.         1.         1.\n",
      " 1.         1.         0.815052   0.99339217 1.         0.64626163\n",
      " 1.         1.         1.         1.         0.9993589  1.\n",
      " 1.         1.         1.         1.         0.99830174 0.92975116\n",
      " 0.95003885 1.         1.         1.         0.99994385 0.999891\n",
      " 1.         0.9950682  1.         1.         0.999799   0.99455255\n",
      " 0.54407763 0.98213774 1.         1.         1.         1.\n",
      " 1.         0.99921936 1.         0.9445264  1.         1.\n",
      " 1.         1.         1.         0.4505018  1.         1.\n",
      " 0.30203974 1.         1.         1.         1.         1.\n",
      " 0.9696772  0.9937463  1.         0.81613004 0.9951311  0.99631965\n",
      " 1.         0.9993869  0.99922657 1.         0.845992   0.99603975\n",
      " 1.         1.         0.99899715 0.9995332  0.9773855 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00010005852144129582, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 41.20582179905559\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 306.92014\n",
      "iteration 200 loss 306.9124\n",
      "iteration 300 loss 306.90533\n",
      "iteration 400 loss 306.89883\n",
      "iteration 500 loss 306.89294\n",
      "iteration 600 loss 306.88757\n",
      "iteration 700 loss 306.88275\n",
      "iteration 800 loss 306.87836\n",
      "iteration 900 loss 306.87442\n",
      "iteration 1000 loss 306.8709\n",
      "iteration 1100 loss 306.86768\n",
      "iteration 1200 loss 306.86484\n",
      "iteration 1300 loss 306.86227\n",
      "iteration 1400 loss 306.85995\n",
      "iteration 1500 loss 306.85788\n",
      "iteration 1600 loss 306.85602\n",
      "iteration 1700 loss 306.85437\n",
      "iteration 1800 loss 306.85278\n",
      "iteration 1900 loss 306.85138\n",
      "iteration 2000 loss 306.84998\n",
      "iteration 2100 loss 306.84863\n",
      "iteration 2200 loss 306.8473\n",
      "iteration 2300 loss 306.84595\n",
      "iteration 2400 loss 306.84442\n",
      "iteration 2500 loss 306.84286\n",
      "iteration 2600 loss 306.84106\n",
      "iteration 2700 loss 306.839\n",
      "iteration 2800 loss 306.83664\n",
      "iteration 2900 loss 306.83392\n",
      "iteration 3000 loss 306.83072\n",
      "iteration 3100 loss 306.827\n",
      "iteration 3200 loss 306.8228\n",
      "iteration 3300 loss 306.81796\n",
      "iteration 3400 loss 306.8125\n",
      "iteration 3500 loss 306.80652\n",
      "iteration 3600 loss 306.7999\n",
      "iteration 3700 loss 306.79272\n",
      "iteration 3800 loss 306.78506\n",
      "iteration 3900 loss 306.77704\n",
      "iteration 4000 loss 306.76855\n",
      "iteration 4100 loss 306.75974\n",
      "iteration 4200 loss 306.75067\n",
      "iteration 4300 loss 306.74136\n",
      "iteration 4400 loss 306.73193\n",
      "iteration 4500 loss 306.72247\n",
      "iteration 4600 loss 306.7129\n",
      "iteration 4700 loss 306.7033\n",
      "iteration 4800 loss 306.69373\n",
      "iteration 4900 loss 306.6842\n",
      "iteration 5000 loss 306.6747Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         1.         1.         1.         0.99990475 1.\n",
      " 0.9942037  1.         0.38600942 1.         0.9392904  0.42298537\n",
      " 0.99951506 0.90284616 1.         1.         1.         0.9970838\n",
      " 0.99631083 0.9999181  1.         1.         0.9993142  1.\n",
      " 1.         0.9976201  1.         0.9993419  1.         0.991184\n",
      " 0.9988735  1.         0.9973576  1.         1.         1.\n",
      " 0.63734496 1.         1.         1.         1.         1.\n",
      " 1.         0.9992085  0.99869055 1.         1.         1.\n",
      " 1.         1.         0.99954057 1.         1.         0.9771143\n",
      " 1.         1.         1.         0.9910469  1.         0.9944444\n",
      " 1.         1.         1.         1.         0.99541134 0.99947846\n",
      " 0.99532276 0.9975654  1.         1.         0.93013793 0.9939995\n",
      " 0.9976088  0.9980791  0.99933064 1.         1.        ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0001225299251302834, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 13.183448567207085\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 296.0799\n",
      "iteration 200 loss 296.07977\n",
      "iteration 300 loss 296.07968\n",
      "iteration 400 loss 296.0796\n",
      "iteration 500 loss 296.07947\n",
      "iteration 600 loss 296.07935\n",
      "iteration 700 loss 296.07922\n",
      "iteration 800 loss 296.07913\n",
      "iteration 900 loss 296.07892\n",
      "iteration 1000 loss 296.07883\n",
      "iteration 1100 loss 296.07864\n",
      "iteration 1200 loss 296.07846\n",
      "iteration 1300 loss 296.07825\n",
      "iteration 1400 loss 296.078\n",
      "iteration 1500 loss 296.07782\n",
      "iteration 1600 loss 296.07758\n",
      "iteration 1700 loss 296.0773\n",
      "iteration 1800 loss 296.07703\n",
      "iteration 1900 loss 296.07675\n",
      "iteration 2000 loss 296.0764\n",
      "iteration 2100 loss 296.07602\n",
      "iteration 2200 loss 296.0756\n",
      "iteration 2300 loss 296.0751\n",
      "iteration 2400 loss 296.0746\n",
      "iteration 2500 loss 296.07404\n",
      "iteration 2600 loss 296.07343\n",
      "iteration 2700 loss 296.0728\n",
      "iteration 2800 loss 296.07202\n",
      "iteration 2900 loss 296.07123\n",
      "iteration 3000 loss 296.07034\n",
      "iteration 3100 loss 296.06934\n",
      "iteration 3200 loss 296.0682\n",
      "iteration 3300 loss 296.06708\n",
      "iteration 3400 loss 296.06573\n",
      "iteration 3500 loss 296.0643\n",
      "iteration 3600 loss 296.06274\n",
      "iteration 3700 loss 296.06097\n",
      "iteration 3800 loss 296.05914\n",
      "iteration 3900 loss 296.05707\n",
      "iteration 4000 loss 296.05487\n",
      "iteration 4100 loss 296.05243\n",
      "iteration 4200 loss 296.04984\n",
      "iteration 4300 loss 296.047\n",
      "iteration 4400 loss 296.0439\n",
      "iteration 4500 loss 296.04056\n",
      "iteration 4600 loss 296.03693\n",
      "iteration 4700 loss 296.03314\n",
      "iteration 4800 loss 296.02905\n",
      "iteration 4900 loss 296.0246\n",
      "iteration 5000 loss 296.01987Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         1.         1.         1.         0.99978656 0.99993473\n",
      " 0.9996452  1.         0.78864306 1.         0.99947876 0.641318\n",
      " 0.9995823  1.         1.         1.         1.         1.\n",
      " 1.         0.9999451  1.         1.         1.         0.98386997\n",
      " 1.         0.9994821  0.9870696  0.999708   1.         0.9981911\n",
      " 0.99969023 1.         0.9998895  1.         1.         1.\n",
      " 0.83360696 1.         1.         0.9999276  1.         1.\n",
      " 1.         0.9997088  1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.9975966\n",
      " 1.         1.         1.         0.9999351  1.         0.9964173\n",
      " 1.         1.         1.         1.         0.9983211  1.\n",
      " 0.9974535  0.9996834  0.999822   1.         1.         1.\n",
      " 0.99943995 0.9993081  1.         0.9999489  1.        ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00011266586968851422, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 19.048565344189527\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 217.87314\n",
      "iteration 200 loss 217.87067\n",
      "iteration 300 loss 217.86824\n",
      "iteration 400 loss 217.866\n",
      "iteration 500 loss 217.86386\n",
      "iteration 600 loss 217.86186\n",
      "iteration 700 loss 217.85992\n",
      "iteration 800 loss 217.85806\n",
      "iteration 900 loss 217.85632\n",
      "iteration 1000 loss 217.85468\n",
      "iteration 1100 loss 217.85313\n",
      "iteration 1200 loss 217.85161\n",
      "iteration 1300 loss 217.85014\n",
      "iteration 1400 loss 217.8488\n",
      "iteration 1500 loss 217.84749\n",
      "iteration 1600 loss 217.84622\n",
      "iteration 1700 loss 217.84505\n",
      "iteration 1800 loss 217.8439\n",
      "iteration 1900 loss 217.84277\n",
      "iteration 2000 loss 217.84174\n",
      "iteration 2100 loss 217.84073\n",
      "iteration 2200 loss 217.83975\n",
      "iteration 2300 loss 217.8388\n",
      "iteration 2400 loss 217.83786\n",
      "iteration 2500 loss 217.8369\n",
      "iteration 2600 loss 217.83595\n",
      "iteration 2700 loss 217.83502\n",
      "iteration 2800 loss 217.83408\n",
      "iteration 2900 loss 217.8331\n",
      "iteration 3000 loss 217.83212\n",
      "iteration 3100 loss 217.83107\n",
      "iteration 3200 loss 217.83002\n",
      "iteration 3300 loss 217.82884\n",
      "iteration 3400 loss 217.82767\n",
      "iteration 3500 loss 217.82645\n",
      "iteration 3600 loss 217.82509\n",
      "iteration 3700 loss 217.82364\n",
      "iteration 3800 loss 217.82213\n",
      "iteration 3900 loss 217.82053\n",
      "iteration 4000 loss 217.81877\n",
      "iteration 4100 loss 217.8169\n",
      "iteration 4200 loss 217.81488\n",
      "iteration 4300 loss 217.81268\n",
      "iteration 4400 loss 217.8103\n",
      "iteration 4500 loss 217.8077\n",
      "iteration 4600 loss 217.80493\n",
      "iteration 4700 loss 217.8019\n",
      "iteration 4800 loss 217.79858\n",
      "iteration 4900 loss 217.79498\n",
      "iteration 5000 loss 217.79109Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         0.99972963 1.         1.         0.9996667  1.\n",
      " 0.9970855  1.         0.54745007 0.9670634  0.88406885 1.\n",
      " 0.99918234 1.         1.         1.         0.9997591  0.9977114\n",
      " 0.9969486  0.9997244  1.         1.         0.999653   0.98469895\n",
      " 1.         0.9990567  0.9994703  1.         1.         1.\n",
      " 0.99993145 1.         1.         1.         1.         1.\n",
      " 0.7815384  1.         1.         1.         1.         1.\n",
      " 1.         0.99837023 1.         1.         1.         1.\n",
      " 1.         1.         0.9995745  1.         1.         0.98866934\n",
      " 1.         0.96300656 1.         1.         1.         0.99162984\n",
      " 0.9892708  1.         0.9777     1.         1.         0.9987637\n",
      " 0.99000967 1.         1.         0.81222457 0.789879   0.99214137\n",
      " 0.9974561  0.99902236 0.9987531  1.         0.99854225]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00013235325789651424, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 52.08897388963284\n",
      "\n",
      " 4 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 226.09366\n",
      "iteration 200 loss 226.09222\n",
      "iteration 300 loss 226.09087\n",
      "iteration 400 loss 226.08958\n",
      "iteration 500 loss 226.08836\n",
      "iteration 600 loss 226.08719\n",
      "iteration 700 loss 226.08609\n",
      "iteration 800 loss 226.08499\n",
      "iteration 900 loss 226.08398\n",
      "iteration 1000 loss 226.08301\n",
      "iteration 1100 loss 226.08208\n",
      "iteration 1200 loss 226.08118\n",
      "iteration 1300 loss 226.08034\n",
      "iteration 1400 loss 226.0795\n",
      "iteration 1500 loss 226.0787\n",
      "iteration 1600 loss 226.07796\n",
      "iteration 1700 loss 226.07722\n",
      "iteration 1800 loss 226.07657\n",
      "iteration 1900 loss 226.07585\n",
      "iteration 2000 loss 226.0752\n",
      "iteration 2100 loss 226.0746\n",
      "iteration 2200 loss 226.074\n",
      "iteration 2300 loss 226.0734\n",
      "iteration 2400 loss 226.07285\n",
      "iteration 2500 loss 226.07228\n",
      "iteration 2600 loss 226.07178\n",
      "iteration 2700 loss 226.07123\n",
      "iteration 2800 loss 226.07077\n",
      "iteration 2900 loss 226.07027\n",
      "iteration 3000 loss 226.06981\n",
      "iteration 3100 loss 226.06932\n",
      "iteration 3200 loss 226.06891\n",
      "iteration 3300 loss 226.06845\n",
      "iteration 3400 loss 226.06805\n",
      "iteration 3500 loss 226.06761\n",
      "iteration 3600 loss 226.06721\n",
      "iteration 3700 loss 226.0668\n",
      "iteration 3800 loss 226.06647\n",
      "iteration 3900 loss 226.06601\n",
      "iteration 4000 loss 226.06567\n",
      "iteration 4100 loss 226.06528\n",
      "iteration 4200 loss 226.0649\n",
      "iteration 4300 loss 226.06453\n",
      "iteration 4400 loss 226.0642\n",
      "iteration 4500 loss 226.0638\n",
      "iteration 4600 loss 226.06346\n",
      "iteration 4700 loss 226.06311\n",
      "iteration 4800 loss 226.06274\n",
      "iteration 4900 loss 226.06241\n",
      "iteration 5000 loss 226.06204Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         0.9998198  1.         1.         1.         1.\n",
      " 0.99723196 1.         1.         1.         0.87678236 1.\n",
      " 1.         1.         1.         1.         1.         0.99805284\n",
      " 0.9980263  0.9998025  1.         1.         1.         0.99357206\n",
      " 1.         0.99929476 1.         1.         1.         0.999578\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.9995817  1.         1.         0.6335165  1.\n",
      " 1.         1.         1.         1.         0.9818982  0.98963034\n",
      " 1.         1.         0.99198747 1.         1.         0.98753464\n",
      " 1.         1.         0.99426955 1.         1.         0.9991316\n",
      " 0.99679905 1.         1.         1.         0.99997604 0.9990152\n",
      " 0.9982419  1.         1.         1.         1.        ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0001272263102059085, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 48.2152032174579\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 307.8751\n",
      "iteration 200 loss 307.87335\n",
      "iteration 300 loss 307.87173\n",
      "iteration 400 loss 307.87012\n",
      "iteration 500 loss 307.86853\n",
      "iteration 600 loss 307.8671\n",
      "iteration 700 loss 307.86575\n",
      "iteration 800 loss 307.86444\n",
      "iteration 900 loss 307.8632\n",
      "iteration 1000 loss 307.86206\n",
      "iteration 1100 loss 307.86096\n",
      "iteration 1200 loss 307.85992\n",
      "iteration 1300 loss 307.85898\n",
      "iteration 1400 loss 307.85803\n",
      "iteration 1500 loss 307.85724\n",
      "iteration 1600 loss 307.85648\n",
      "iteration 1700 loss 307.85574\n",
      "iteration 1800 loss 307.85507\n",
      "iteration 1900 loss 307.85443\n",
      "iteration 2000 loss 307.85382\n",
      "iteration 2100 loss 307.8532\n",
      "iteration 2200 loss 307.85266\n",
      "iteration 2300 loss 307.85217\n",
      "iteration 2400 loss 307.8517\n",
      "iteration 2500 loss 307.85123\n",
      "iteration 2600 loss 307.85083\n",
      "iteration 2700 loss 307.85037\n",
      "iteration 2800 loss 307.84998\n",
      "iteration 2900 loss 307.84955\n",
      "iteration 3000 loss 307.84912\n",
      "iteration 3100 loss 307.84875\n",
      "iteration 3200 loss 307.8484\n",
      "iteration 3300 loss 307.84802\n",
      "iteration 3400 loss 307.84766\n",
      "iteration 3500 loss 307.8473\n",
      "iteration 3600 loss 307.847\n",
      "iteration 3700 loss 307.84662\n",
      "iteration 3800 loss 307.84625\n",
      "iteration 3900 loss 307.8459\n",
      "iteration 4000 loss 307.84555\n",
      "iteration 4100 loss 307.84515\n",
      "iteration 4200 loss 307.8448\n",
      "iteration 4300 loss 307.84445\n",
      "iteration 4400 loss 307.8441\n",
      "iteration 4500 loss 307.84366\n",
      "iteration 4600 loss 307.8433\n",
      "iteration 4700 loss 307.84296\n",
      "iteration 4800 loss 307.84256\n",
      "iteration 4900 loss 307.8421\n",
      "iteration 5000 loss 307.84174Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         1.         1.         1.         0.9998803  0.9997794\n",
      " 0.996188   1.         1.         1.         1.         0.6278405\n",
      " 0.9994744  0.941473   1.         1.         1.         0.99869865\n",
      " 0.9975975  1.         1.         1.         1.         0.991673\n",
      " 1.         0.9985084  1.         0.99944973 1.         0.99565125\n",
      " 0.9994115  1.         0.99933314 1.         1.         1.\n",
      " 0.9160594  1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.9858799\n",
      " 1.         1.         1.         0.999886   1.         0.9919462\n",
      " 1.         1.         1.         1.         0.9994428  0.99893713\n",
      " 0.99494016 0.9999069  1.         1.         1.         1.\n",
      " 0.9986395  1.         1.         0.9998203  1.        ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.0001110795358979804, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 13.26742707140626\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 276.09818\n",
      "iteration 200 loss 276.09808\n",
      "iteration 300 loss 276.09796\n",
      "iteration 400 loss 276.09784\n",
      "iteration 500 loss 276.0978\n",
      "iteration 600 loss 276.09772\n",
      "iteration 700 loss 276.09763\n",
      "iteration 800 loss 276.0975\n",
      "iteration 900 loss 276.09735\n",
      "iteration 1000 loss 276.09732\n",
      "iteration 1100 loss 276.09726\n",
      "iteration 1200 loss 276.09714\n",
      "iteration 1300 loss 276.097\n",
      "iteration 1400 loss 276.09695\n",
      "iteration 1500 loss 276.0969\n",
      "iteration 1600 loss 276.0968\n",
      "iteration 1700 loss 276.09668\n",
      "iteration 1800 loss 276.09662\n",
      "iteration 1900 loss 276.09653\n",
      "iteration 2000 loss 276.09644\n",
      "iteration 2100 loss 276.0963\n",
      "iteration 2200 loss 276.09625\n",
      "iteration 2300 loss 276.0962\n",
      "iteration 2400 loss 276.0961\n",
      "iteration 2500 loss 276.09598\n",
      "iteration 2600 loss 276.09592\n",
      "iteration 2700 loss 276.0958\n",
      "iteration 2800 loss 276.09576\n",
      "iteration 2900 loss 276.09564\n",
      "iteration 3000 loss 276.09558\n",
      "iteration 3100 loss 276.09552\n",
      "iteration 3200 loss 276.0954\n",
      "iteration 3300 loss 276.0953\n",
      "iteration 3400 loss 276.0952\n",
      "iteration 3500 loss 276.09515\n",
      "iteration 3600 loss 276.0951\n",
      "iteration 3700 loss 276.09497\n",
      "iteration 3800 loss 276.0949\n",
      "iteration 3900 loss 276.09482\n",
      "iteration 4000 loss 276.09476\n",
      "iteration 4100 loss 276.09464\n",
      "iteration 4200 loss 276.09457\n",
      "iteration 4300 loss 276.09448\n",
      "iteration 4400 loss 276.09442\n",
      "iteration 4500 loss 276.09436\n",
      "iteration 4600 loss 276.09424\n",
      "iteration 4700 loss 276.09424\n",
      "iteration 4800 loss 276.09412\n",
      "iteration 4900 loss 276.09406\n",
      "iteration 5000 loss 276.09396Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [0.9998279  1.         0.98397106 1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         0.99986917 1.\n",
      " 1.         1.         1.         1.         1.         0.9839994\n",
      " 0.99399495 1.         1.         1.         1.         1.\n",
      " 1.         0.9972339  1.         1.         1.         0.99989545\n",
      " 0.9921421  1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         0.9988694  1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.8786912  0.9994004  1.         1.         1.         1.\n",
      " 1.         0.9998485  1.         0.955544   1.         0.999702\n",
      " 0.9998783  0.99990386 1.         1.         0.934698   0.9994426\n",
      " 1.         0.9997883  0.99989897 1.         0.99686307]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 8.55315494178665e-05, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 27.384449279332628\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 289.90256\n",
      "iteration 200 loss 289.84406\n",
      "iteration 300 loss 289.77966\n",
      "iteration 400 loss 289.71332\n",
      "iteration 500 loss 289.6494\n",
      "iteration 600 loss 289.59125\n",
      "iteration 700 loss 289.54047\n",
      "iteration 800 loss 289.49716\n",
      "iteration 900 loss 289.45996\n",
      "iteration 1000 loss 289.4273\n",
      "iteration 1100 loss 289.39783\n",
      "iteration 1200 loss 289.37012\n",
      "iteration 1300 loss 289.34332\n",
      "iteration 1400 loss 289.31677\n",
      "iteration 1500 loss 289.28998\n",
      "iteration 1600 loss 289.26248\n",
      "iteration 1700 loss 289.23395\n",
      "iteration 1800 loss 289.20435\n",
      "iteration 1900 loss 289.17313\n",
      "iteration 2000 loss 289.14032\n",
      "iteration 2100 loss 289.10556\n",
      "iteration 2200 loss 289.06873\n",
      "iteration 2300 loss 289.02945\n",
      "iteration 2400 loss 288.98764\n",
      "iteration 2500 loss 288.94287\n",
      "iteration 2600 loss 288.8948\n",
      "iteration 2700 loss 288.84308\n",
      "iteration 2800 loss 288.78717\n",
      "iteration 2900 loss 288.72644\n",
      "iteration 3000 loss 288.66028\n",
      "iteration 3100 loss 288.58783\n",
      "iteration 3200 loss 288.51843\n",
      "iteration 3300 loss 288.45276\n",
      "iteration 3400 loss 288.38193\n",
      "iteration 3500 loss 288.305\n",
      "iteration 3600 loss 288.22095\n",
      "iteration 3700 loss 288.14545\n",
      "iteration 3800 loss 288.0633\n",
      "iteration 3900 loss 287.9641\n",
      "iteration 4000 loss 287.8395\n",
      "iteration 4100 loss 287.68408\n",
      "iteration 4200 loss 287.5482\n",
      "iteration 4300 loss 287.398\n",
      "iteration 4400 loss 287.22858\n",
      "iteration 4500 loss 287.04303\n",
      "iteration 4600 loss 286.88293\n",
      "iteration 4700 loss 286.84848\n",
      "iteration 4800 loss 286.81903\n",
      "iteration 4900 loss 286.7926\n",
      "iteration 5000 loss 286.7693Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [0.97807634 0.99922144 0.5832908  1.         0.99997044 1.\n",
      " 1.         0.82275885 1.         0.7958335  1.         0.27912003\n",
      " 1.         1.         1.         1.         0.99345577 0.99167925\n",
      " 0.99400586 0.99979055 0.99707144 1.         0.99388945 0.8269433\n",
      " 0.7316919  1.         1.         1.         0.9974346  1.\n",
      " 1.         0.9310944  1.         1.         0.999121   0.9778372\n",
      " 0.         1.         0.99961346 1.         0.         0.9999592\n",
      " 1.         1.         1.         0.85222876 1.         1.\n",
      " 0.         1.         0.99985635 0.         1.         1.\n",
      " 0.         1.         1.         1.         0.         1.\n",
      " 0.9412334  0.9665905  1.         0.28693882 0.9594592  0.99293786\n",
      " 0.94915    1.         0.9940306  0.82017803 1.         0.9927777\n",
      " 1.         0.9994919  0.99833983 1.         0.8779292 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00013308585325242208, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 20.075255074648272\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 253.0341\n",
      "iteration 200 loss 253.00429\n",
      "iteration 300 loss 252.9755\n",
      "iteration 400 loss 252.9467\n",
      "iteration 500 loss 252.91708\n",
      "iteration 600 loss 252.88593\n",
      "iteration 700 loss 252.8532\n",
      "iteration 800 loss 252.81972\n",
      "iteration 900 loss 252.78687\n",
      "iteration 1000 loss 252.75629\n",
      "iteration 1100 loss 252.7289\n",
      "iteration 1200 loss 252.70508\n",
      "iteration 1300 loss 252.68451\n",
      "iteration 1400 loss 252.66652\n",
      "iteration 1500 loss 252.64847\n",
      "iteration 1600 loss 252.6224\n",
      "iteration 1700 loss 252.57016\n",
      "iteration 1800 loss 252.4386\n",
      "iteration 1900 loss 252.08809\n",
      "iteration 2000 loss 251.04927\n",
      "iteration 2100 loss 247.24963\n",
      "iteration 2200 loss 245.68048\n",
      "iteration 2300 loss 243.79842\n",
      "iteration 2400 loss 241.55669\n",
      "iteration 2500 loss 240.71692\n",
      "iteration 2600 loss 240.24774\n",
      "iteration 2700 loss 239.93546\n",
      "iteration 2800 loss 239.86295\n",
      "iteration 2900 loss 239.79634\n",
      "iteration 3000 loss 239.742\n",
      "iteration 3100 loss 239.70758\n",
      "iteration 3200 loss 239.69113\n",
      "iteration 3300 loss 239.67802\n",
      "iteration 3400 loss 239.66605\n",
      "iteration 3500 loss 239.65472\n",
      "iteration 3600 loss 239.64383\n",
      "iteration 3700 loss 239.63333\n",
      "iteration 3800 loss 239.62318\n",
      "iteration 3900 loss 239.61327\n",
      "iteration 4000 loss 239.60367\n",
      "iteration 4100 loss 239.59425\n",
      "iteration 4200 loss 239.58511\n",
      "iteration 4300 loss 239.57614\n",
      "iteration 4400 loss 239.56738\n",
      "iteration 4500 loss 239.55876\n",
      "iteration 4600 loss 239.55034\n",
      "iteration 4700 loss 239.54205\n",
      "iteration 4800 loss 239.53387\n",
      "iteration 4900 loss 239.52585\n",
      "iteration 5000 loss 239.51794Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         1.         1.         1.         0.9989926  1.\n",
      " 0.9264907  0.         0.         0.26020002 0.         0.21549445\n",
      " 0.97856545 0.81483823 0.09299168 0.17889592 0.9986818  0.93716806\n",
      " 0.9442357  1.         1.         1.         0.9975187  1.\n",
      " 1.         0.9714679  0.         0.9862538  1.         0.8600999\n",
      " 0.98164994 1.         0.96669114 1.         1.         1.\n",
      " 1.         1.         0.99979556 1.         0.5927896  0.9996272\n",
      " 0.99789995 0.984404   0.56345624 1.         1.         1.\n",
      " 1.         0.         0.9929209  1.         1.         0.68646115\n",
      " 1.         1.         1.         0.9622231  0.5819397  0.9302435\n",
      " 1.         1.         1.         1.         1.         0.9712955\n",
      " 0.959632   0.9588353  1.         1.         0.         0.94280446\n",
      " 0.96501017 0.98774236 0.98699594 0.9992823  1.        ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00011082029078460452, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 34.53707126021173\n",
      "\n",
      " 5 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 276.30872\n",
      "iteration 200 loss 276.30872\n",
      "iteration 300 loss 276.3087\n",
      "iteration 400 loss 276.3087\n",
      "iteration 500 loss 276.30872\n",
      "iteration 600 loss 276.30865\n",
      "iteration 700 loss 276.30865\n",
      "iteration 800 loss 276.30865\n",
      "iteration 900 loss 276.30865\n",
      "iteration 1000 loss 276.30862\n",
      "iteration 1100 loss 276.3086\n",
      "iteration 1200 loss 276.3086\n",
      "iteration 1300 loss 276.3086\n",
      "iteration 1400 loss 276.3086\n",
      "iteration 1500 loss 276.3086\n",
      "iteration 1600 loss 276.30856\n",
      "iteration 1700 loss 276.30856\n",
      "iteration 1800 loss 276.30856\n",
      "iteration 1900 loss 276.30853\n",
      "iteration 2000 loss 276.30853\n",
      "iteration 2100 loss 276.30856\n",
      "iteration 2200 loss 276.30853\n",
      "iteration 2300 loss 276.30853\n",
      "iteration 2400 loss 276.3085\n",
      "iteration 2500 loss 276.30847\n",
      "iteration 2600 loss 276.30847\n",
      "iteration 2700 loss 276.30847\n",
      "iteration 2800 loss 276.30847\n",
      "iteration 2900 loss 276.30847\n",
      "iteration 3000 loss 276.30847\n",
      "iteration 3100 loss 276.30847\n",
      "iteration 3200 loss 276.30844\n",
      "iteration 3300 loss 276.30847\n",
      "iteration 3400 loss 276.30844\n",
      "iteration 3500 loss 276.3084\n",
      "iteration 3600 loss 276.3084\n",
      "iteration 3700 loss 276.3084\n",
      "iteration 3800 loss 276.30838\n",
      "iteration 3900 loss 276.30838\n",
      "iteration 4000 loss 276.30838\n",
      "iteration 4100 loss 276.30838\n",
      "iteration 4200 loss 276.30835\n",
      "iteration 4300 loss 276.30835\n",
      "iteration 4400 loss 276.30832\n",
      "iteration 4500 loss 276.30832\n",
      "iteration 4600 loss 276.3083\n",
      "iteration 4700 loss 276.30832\n",
      "iteration 4800 loss 276.3083\n",
      "iteration 4900 loss 276.3083\n",
      "iteration 5000 loss 276.3083Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.98864496\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.9997825  1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         0.9563241  1.\n",
      " 1.         1.         1.         1.         0.9999264 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00012359706209969152, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 27.930344730673664\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 313.0586\n",
      "iteration 200 loss 313.0581\n",
      "iteration 300 loss 313.05762\n",
      "iteration 400 loss 313.05716\n",
      "iteration 500 loss 313.0567\n",
      "iteration 600 loss 313.0562\n",
      "iteration 700 loss 313.05582\n",
      "iteration 800 loss 313.05542\n",
      "iteration 900 loss 313.055\n",
      "iteration 1000 loss 313.05457\n",
      "iteration 1100 loss 313.0542\n",
      "iteration 1200 loss 313.0538\n",
      "iteration 1300 loss 313.0534\n",
      "iteration 1400 loss 313.0531\n",
      "iteration 1500 loss 313.05273\n",
      "iteration 1600 loss 313.05237\n",
      "iteration 1700 loss 313.0521\n",
      "iteration 1800 loss 313.05176\n",
      "iteration 1900 loss 313.05145\n",
      "iteration 2000 loss 313.05115\n",
      "iteration 2100 loss 313.05084\n",
      "iteration 2200 loss 313.0506\n",
      "iteration 2300 loss 313.05026\n",
      "iteration 2400 loss 313.05005\n",
      "iteration 2500 loss 313.04974\n",
      "iteration 2600 loss 313.04953\n",
      "iteration 2700 loss 313.04926\n",
      "iteration 2800 loss 313.04904\n",
      "iteration 2900 loss 313.04877\n",
      "iteration 3000 loss 313.04855\n",
      "iteration 3100 loss 313.04834\n",
      "iteration 3200 loss 313.04813\n",
      "iteration 3300 loss 313.0479\n",
      "iteration 3400 loss 313.04773\n",
      "iteration 3500 loss 313.04755\n",
      "iteration 3600 loss 313.04733\n",
      "iteration 3700 loss 313.04712\n",
      "iteration 3800 loss 313.04697\n",
      "iteration 3900 loss 313.0468\n",
      "iteration 4000 loss 313.04657\n",
      "iteration 4100 loss 313.04645\n",
      "iteration 4200 loss 313.04626\n",
      "iteration 4300 loss 313.04608\n",
      "iteration 4400 loss 313.04596\n",
      "iteration 4500 loss 313.04578\n",
      "iteration 4600 loss 313.04565\n",
      "iteration 4700 loss 313.04547\n",
      "iteration 4800 loss 313.0454\n",
      "iteration 4900 loss 313.04526\n",
      "iteration 5000 loss 313.0451Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [0.99979585 1.         1.         0.99993145 0.9999014  1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         0.9997847  1.\n",
      " 1.         1.         1.         1.         0.9996261  0.98469776\n",
      " 0.99998647 1.         1.         1.         1.         1.\n",
      " 1.         0.995151   1.         1.         1.         1.\n",
      " 0.74113905 1.         1.         1.         1.         1.\n",
      " 1.         0.9996053  1.         1.         1.         1.\n",
      " 1.         1.         0.9998702  1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.99954414\n",
      " 0.9975062  1.         0.9999427  1.         1.         0.9980506\n",
      " 1.         0.9998562  0.9996999  1.         0.99956363]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00012372511760600855, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 11.547321518140034\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 217.07266\n",
      "iteration 200 loss 217.04337\n",
      "iteration 300 loss 217.0193\n",
      "iteration 400 loss 216.99954\n",
      "iteration 500 loss 216.98341\n",
      "iteration 600 loss 216.97015\n",
      "iteration 700 loss 216.95924\n",
      "iteration 800 loss 216.95026\n",
      "iteration 900 loss 216.94278\n",
      "iteration 1000 loss 216.93661\n",
      "iteration 1100 loss 216.93149\n",
      "iteration 1200 loss 216.92715\n",
      "iteration 1300 loss 216.92357\n",
      "iteration 1400 loss 216.92052\n",
      "iteration 1500 loss 216.9179\n",
      "iteration 1600 loss 216.91571\n",
      "iteration 1700 loss 216.9138\n",
      "iteration 1800 loss 216.91214\n",
      "iteration 1900 loss 216.91075\n",
      "iteration 2000 loss 216.90953\n",
      "iteration 2100 loss 216.90845\n",
      "iteration 2200 loss 216.90749\n",
      "iteration 2300 loss 216.90665\n",
      "iteration 2400 loss 216.9059\n",
      "iteration 2500 loss 216.90524\n",
      "iteration 2600 loss 216.90462\n",
      "iteration 2700 loss 216.90407\n",
      "iteration 2800 loss 216.9036\n",
      "iteration 2900 loss 216.90312\n",
      "iteration 3000 loss 216.9027\n",
      "iteration 3100 loss 216.90231\n",
      "iteration 3200 loss 216.90198\n",
      "iteration 3300 loss 216.90161\n",
      "iteration 3400 loss 216.9013\n",
      "iteration 3500 loss 216.90102\n",
      "iteration 3600 loss 216.90073\n",
      "iteration 3700 loss 216.90044\n",
      "iteration 3800 loss 216.90013\n",
      "iteration 3900 loss 216.89986\n",
      "iteration 4000 loss 216.8996\n",
      "iteration 4100 loss 216.89938\n",
      "iteration 4200 loss 216.89912\n",
      "iteration 4300 loss 216.89886\n",
      "iteration 4400 loss 216.89862\n",
      "iteration 4500 loss 216.89838\n",
      "iteration 4600 loss 216.89813\n",
      "iteration 4700 loss 216.89789\n",
      "iteration 4800 loss 216.89763\n",
      "iteration 4900 loss 216.89737\n",
      "iteration 5000 loss 216.8971Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         1.         1.         1.         0.99978393 1.\n",
      " 0.99985826 1.         0.506513   0.96301585 0.974282   1.\n",
      " 0.9997971  1.         0.5568165  1.         1.         0.99958384\n",
      " 1.         0.99973106 1.         1.         0.99927384 0.9482089\n",
      " 0.9955339  0.99857545 1.         1.         1.         0.995987\n",
      " 0.9991354  1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         0.9980347  1.\n",
      " 1.         0.9994725  1.         1.         1.         1.\n",
      " 1.         1.         0.99905014 1.         1.         1.\n",
      " 1.         1.         1.         0.99714655 1.         0.9997553\n",
      " 0.9892289  1.         1.         1.         0.99634194 1.\n",
      " 1.         0.99813116 1.         0.91447985 0.8803953  0.99022084\n",
      " 0.99845326 0.99798286 0.99822164 1.         0.99677086]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00010779821183946395, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 51.682021780758284\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 271.76697\n",
      "iteration 200 loss 271.76324\n",
      "iteration 300 loss 271.7595\n",
      "iteration 400 loss 271.75568\n",
      "iteration 500 loss 271.75186\n",
      "iteration 600 loss 271.748\n",
      "iteration 700 loss 271.74408\n",
      "iteration 800 loss 271.7402\n",
      "iteration 900 loss 271.73627\n",
      "iteration 1000 loss 271.73233\n",
      "iteration 1100 loss 271.72833\n",
      "iteration 1200 loss 271.7244\n",
      "iteration 1300 loss 271.72046\n",
      "iteration 1400 loss 271.71646\n",
      "iteration 1500 loss 271.71252\n",
      "iteration 1600 loss 271.7085\n",
      "iteration 1700 loss 271.7046\n",
      "iteration 1800 loss 271.70056\n",
      "iteration 1900 loss 271.69666\n",
      "iteration 2000 loss 271.69263\n",
      "iteration 2100 loss 271.68863\n",
      "iteration 2200 loss 271.68463\n",
      "iteration 2300 loss 271.68066\n",
      "iteration 2400 loss 271.6766\n",
      "iteration 2500 loss 271.6726\n",
      "iteration 2600 loss 271.66855\n",
      "iteration 2700 loss 271.6645\n",
      "iteration 2800 loss 271.6604\n",
      "iteration 2900 loss 271.65625\n",
      "iteration 3000 loss 271.6521\n",
      "iteration 3100 loss 271.64792\n",
      "iteration 3200 loss 271.64368\n",
      "iteration 3300 loss 271.63947\n",
      "iteration 3400 loss 271.63513\n",
      "iteration 3500 loss 271.63074\n",
      "iteration 3600 loss 271.62634\n",
      "iteration 3700 loss 271.62183\n",
      "iteration 3800 loss 271.61728\n",
      "iteration 3900 loss 271.6126\n",
      "iteration 4000 loss 271.60785\n",
      "iteration 4100 loss 271.60297\n",
      "iteration 4200 loss 271.59793\n",
      "iteration 4300 loss 271.59277\n",
      "iteration 4400 loss 271.58743\n",
      "iteration 4500 loss 271.58185\n",
      "iteration 4600 loss 271.57605\n",
      "iteration 4700 loss 271.57\n",
      "iteration 4800 loss 271.5636\n",
      "iteration 4900 loss 271.55685\n",
      "iteration 5000 loss 271.54974Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [0.9970401  0.9999111  0.9938999  1.         1.         1.\n",
      " 1.         1.         0.79283994 0.9886639  1.         0.47186095\n",
      " 1.         1.         1.         1.         0.9986546  1.\n",
      " 1.         1.         1.         1.         0.997809   0.9534889\n",
      " 0.9386384  1.         1.         1.         0.9995248  0.99953544\n",
      " 1.         0.97831964 1.         1.         1.         0.9961605\n",
      " 0.5565666  1.         0.99996674 0.9999689  1.         1.\n",
      " 1.         0.9985971  1.         0.9811842  1.         1.\n",
      " 1.         1.         0.99995303 0.59636134 1.         1.\n",
      " 0.5064563  1.         1.         1.         0.9677731  1.\n",
      " 0.99472535 0.99568206 1.         0.9765372  0.98776495 1.\n",
      " 0.98942727 0.99885    0.9998983  0.966099   1.         0.9944936\n",
      " 1.         0.9986556  0.9984017  0.99987864 0.98193485]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 8.734047346943082e-05, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 29.888603166576473\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 281.11578\n",
      "iteration 200 loss 281.08588\n",
      "iteration 300 loss 281.05734\n",
      "iteration 400 loss 281.03043\n",
      "iteration 500 loss 281.00537\n",
      "iteration 600 loss 280.9824\n",
      "iteration 700 loss 280.96146\n",
      "iteration 800 loss 280.94257\n",
      "iteration 900 loss 280.92557\n",
      "iteration 1000 loss 280.91034\n",
      "iteration 1100 loss 280.89648\n",
      "iteration 1200 loss 280.88403\n",
      "iteration 1300 loss 280.87256\n",
      "iteration 1400 loss 280.86194\n",
      "iteration 1500 loss 280.8521\n",
      "iteration 1600 loss 280.8429\n",
      "iteration 1700 loss 280.83423\n",
      "iteration 1800 loss 280.82593\n",
      "iteration 1900 loss 280.818\n",
      "iteration 2000 loss 280.81036\n",
      "iteration 2100 loss 280.80298\n",
      "iteration 2200 loss 280.79575\n",
      "iteration 2300 loss 280.78876\n",
      "iteration 2400 loss 280.78183\n",
      "iteration 2500 loss 280.77496\n",
      "iteration 2600 loss 280.7682\n",
      "iteration 2700 loss 280.76144\n",
      "iteration 2800 loss 280.7547\n",
      "iteration 2900 loss 280.7479\n",
      "iteration 3000 loss 280.7409\n",
      "iteration 3100 loss 280.7338\n",
      "iteration 3200 loss 280.72647\n",
      "iteration 3300 loss 280.71887\n",
      "iteration 3400 loss 280.7109\n",
      "iteration 3500 loss 280.7025\n",
      "iteration 3600 loss 280.6935\n",
      "iteration 3700 loss 280.68393\n",
      "iteration 3800 loss 280.67346\n",
      "iteration 3900 loss 280.6621\n",
      "iteration 4000 loss 280.64957\n",
      "iteration 4100 loss 280.63574\n",
      "iteration 4200 loss 280.6204\n",
      "iteration 4300 loss 280.6032\n",
      "iteration 4400 loss 280.58408\n",
      "iteration 4500 loss 280.56256\n",
      "iteration 4600 loss 280.53833\n",
      "iteration 4700 loss 280.51086\n",
      "iteration 4800 loss 280.4796\n",
      "iteration 4900 loss 280.4441\n",
      "iteration 5000 loss 280.40277Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [0.9977245  1.         0.902602   0.9991441  1.         1.\n",
      " 1.         1.         0.8244114  0.96540385 1.         0.7814462\n",
      " 0.9998327  1.         1.         1.         0.99897325 0.99748856\n",
      " 0.9975082  0.99974775 1.         0.99994844 0.99719346 0.94743305\n",
      " 0.9461886  1.         0.99754393 1.         0.9991438  1.\n",
      " 1.         0.9828213  1.         1.         0.9999836  0.9937058\n",
      " 0.25016657 1.         0.99983925 1.         0.945377   1.\n",
      " 1.         0.9971941  1.         0.947274   1.         1.\n",
      " 0.9928652  1.         0.99983567 0.36469665 1.         1.\n",
      " 0.08943545 0.9696386  1.         1.         0.3562195  1.\n",
      " 0.97516435 0.9935034  1.         0.9153074  0.98769957 0.9984136\n",
      " 1.         1.         0.9981055  0.88061094 1.         0.98762876\n",
      " 1.         0.99947315 0.9989666  1.         0.964302  ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00011392743858563875, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 23.49667170002062\n",
      "\n",
      " 6 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 230.37596\n",
      "iteration 200 loss 230.37431\n",
      "iteration 300 loss 230.37277\n",
      "iteration 400 loss 230.37125\n",
      "iteration 500 loss 230.3697\n",
      "iteration 600 loss 230.3682\n",
      "iteration 700 loss 230.36671\n",
      "iteration 800 loss 230.36523\n",
      "iteration 900 loss 230.36377\n",
      "iteration 1000 loss 230.36235\n",
      "iteration 1100 loss 230.3609\n",
      "iteration 1200 loss 230.35947\n",
      "iteration 1300 loss 230.35803\n",
      "iteration 1400 loss 230.35661\n",
      "iteration 1500 loss 230.3552\n",
      "iteration 1600 loss 230.35379\n",
      "iteration 1700 loss 230.35237\n",
      "iteration 1800 loss 230.35095\n",
      "iteration 1900 loss 230.34958\n",
      "iteration 2000 loss 230.34813\n",
      "iteration 2100 loss 230.34674\n",
      "iteration 2200 loss 230.34534\n",
      "iteration 2300 loss 230.34393\n",
      "iteration 2400 loss 230.34251\n",
      "iteration 2500 loss 230.34111\n",
      "iteration 2600 loss 230.33969\n",
      "iteration 2700 loss 230.33823\n",
      "iteration 2800 loss 230.33682\n",
      "iteration 2900 loss 230.33536\n",
      "iteration 3000 loss 230.33392\n",
      "iteration 3100 loss 230.33246\n",
      "iteration 3200 loss 230.331\n",
      "iteration 3300 loss 230.32954\n",
      "iteration 3400 loss 230.32802\n",
      "iteration 3500 loss 230.32654\n",
      "iteration 3600 loss 230.32504\n",
      "iteration 3700 loss 230.32352\n",
      "iteration 3800 loss 230.32196\n",
      "iteration 3900 loss 230.32045\n",
      "iteration 4000 loss 230.31888\n",
      "iteration 4100 loss 230.31732\n",
      "iteration 4200 loss 230.3157\n",
      "iteration 4300 loss 230.31409\n",
      "iteration 4400 loss 230.31247\n",
      "iteration 4500 loss 230.31085\n",
      "iteration 4600 loss 230.30916\n",
      "iteration 4700 loss 230.3075\n",
      "iteration 4800 loss 230.30579\n",
      "iteration 4900 loss 230.30406\n",
      "iteration 5000 loss 230.30229Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         1.         1.         1.         0.99979055 1.\n",
      " 0.99815184 1.         0.559325   0.9538881  1.         1.\n",
      " 0.9995435  1.         0.9849897  1.         0.9994634  1.\n",
      " 1.         0.9998195  1.         1.         0.9985905  0.9796713\n",
      " 0.996506   0.9997048  1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.5860819  1.         1.         1.         1.         1.\n",
      " 1.         0.9985968  1.         1.         1.         1.\n",
      " 1.         1.         0.99948794 1.         1.         0.9966339\n",
      " 1.         0.999544   1.         1.         1.         0.99555683\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.9919176  1.         1.         0.9833842  1.         0.9944397\n",
      " 1.         0.99893546 0.9990376  1.         1.        ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00013796971076988603, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 43.888546337514626\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 267.15622\n",
      "iteration 200 loss 267.13577\n",
      "iteration 300 loss 267.11826\n",
      "iteration 400 loss 267.1034\n",
      "iteration 500 loss 267.09088\n",
      "iteration 600 loss 267.08038\n",
      "iteration 700 loss 267.0715\n",
      "iteration 800 loss 267.06403\n",
      "iteration 900 loss 267.05777\n",
      "iteration 1000 loss 267.05237\n",
      "iteration 1100 loss 267.0478\n",
      "iteration 1200 loss 267.04385\n",
      "iteration 1300 loss 267.04047\n",
      "iteration 1400 loss 267.03748\n",
      "iteration 1500 loss 267.0348\n",
      "iteration 1600 loss 267.03238\n",
      "iteration 1700 loss 267.0302\n",
      "iteration 1800 loss 267.0282\n",
      "iteration 1900 loss 267.0263\n",
      "iteration 2000 loss 267.02448\n",
      "iteration 2100 loss 267.02277\n",
      "iteration 2200 loss 267.02106\n",
      "iteration 2300 loss 267.01935\n",
      "iteration 2400 loss 267.01773\n",
      "iteration 2500 loss 267.01605\n",
      "iteration 2600 loss 267.01428\n",
      "iteration 2700 loss 267.01254\n",
      "iteration 2800 loss 267.01068\n",
      "iteration 2900 loss 267.0088\n",
      "iteration 3000 loss 267.00687\n",
      "iteration 3100 loss 267.00482\n",
      "iteration 3200 loss 267.00262\n",
      "iteration 3300 loss 267.00037\n",
      "iteration 3400 loss 266.998\n",
      "iteration 3500 loss 266.99542\n",
      "iteration 3600 loss 266.99274\n",
      "iteration 3700 loss 266.98987\n",
      "iteration 3800 loss 266.9868\n",
      "iteration 3900 loss 266.98352\n",
      "iteration 4000 loss 266.97998\n",
      "iteration 4100 loss 266.9761\n",
      "iteration 4200 loss 266.97186\n",
      "iteration 4300 loss 266.96704\n",
      "iteration 4400 loss 266.96173\n",
      "iteration 4500 loss 266.95575\n",
      "iteration 4600 loss 266.9488\n",
      "iteration 4700 loss 266.94086\n",
      "iteration 4800 loss 266.9317\n",
      "iteration 4900 loss 266.9209\n",
      "iteration 5000 loss 266.90823Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [0.9942149  0.99985546 0.9917116  0.99742794 1.         1.\n",
      " 1.         1.         0.47562584 0.9144365  0.9837852  1.\n",
      " 0.99976647 1.         1.         1.         0.998877   1.\n",
      " 1.         1.         1.         1.         0.9969901  0.9280984\n",
      " 0.933784   1.         1.         1.         0.9986224  1.\n",
      " 1.         0.9568644  1.         1.         0.999993   0.99293786\n",
      " 0.61283237 1.         0.99988914 1.         1.         1.\n",
      " 1.         0.9981047  1.         0.9873665  1.         1.\n",
      " 1.         1.         0.99988526 0.90394294 1.         1.\n",
      " 0.5207959  0.9466358  1.         1.         0.7285223  1.\n",
      " 1.         0.9966152  1.         0.8107098  0.98937446 1.\n",
      " 0.9980416  1.         0.9985144  0.99087507 1.         0.98746365\n",
      " 1.         0.9983841  0.9981862  1.         0.95644397]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00010747227428478816, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 29.72590703747059\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 256.66663\n",
      "iteration 200 loss 256.6663\n",
      "iteration 300 loss 256.66595\n",
      "iteration 400 loss 256.6654\n",
      "iteration 500 loss 256.66473\n",
      "iteration 600 loss 256.66373\n",
      "iteration 700 loss 256.66223\n",
      "iteration 800 loss 256.65997\n",
      "iteration 900 loss 256.6565\n",
      "iteration 1000 loss 256.6515\n",
      "iteration 1100 loss 256.6444\n",
      "iteration 1200 loss 256.63504\n",
      "iteration 1300 loss 256.6233\n",
      "iteration 1400 loss 256.6094\n",
      "iteration 1500 loss 256.59387\n",
      "iteration 1600 loss 256.57736\n",
      "iteration 1700 loss 256.56042\n",
      "iteration 1800 loss 256.54343\n",
      "iteration 1900 loss 256.5265\n",
      "iteration 2000 loss 256.5099\n",
      "iteration 2100 loss 256.49344\n",
      "iteration 2200 loss 256.4772\n",
      "iteration 2300 loss 256.4611\n",
      "iteration 2400 loss 256.44507\n",
      "iteration 2500 loss 256.42923\n",
      "iteration 2600 loss 256.41357\n",
      "iteration 2700 loss 256.39807\n",
      "iteration 2800 loss 256.38293\n",
      "iteration 2900 loss 256.36804\n",
      "iteration 3000 loss 256.35345\n",
      "iteration 3100 loss 256.33917\n",
      "iteration 3200 loss 256.32526\n",
      "iteration 3300 loss 256.31165\n",
      "iteration 3400 loss 256.29837\n",
      "iteration 3500 loss 256.28534\n",
      "iteration 3600 loss 256.2726\n",
      "iteration 3700 loss 256.26016\n",
      "iteration 3800 loss 256.248\n",
      "iteration 3900 loss 256.23593\n",
      "iteration 4000 loss 256.22418\n",
      "iteration 4100 loss 256.21252\n",
      "iteration 4200 loss 256.20105\n",
      "iteration 4300 loss 256.1897\n",
      "iteration 4400 loss 256.17847\n",
      "iteration 4500 loss 256.16736\n",
      "iteration 4600 loss 256.15637\n",
      "iteration 4700 loss 256.1454\n",
      "iteration 4800 loss 256.13443\n",
      "iteration 4900 loss 256.12354\n",
      "iteration 5000 loss 256.11264Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         1.         1.         1.         1.         0.9992748\n",
      " 0.98865074 0.26153046 1.         0.80264133 0.64028156 0.30798003\n",
      " 0.9984386  0.8663036  1.         1.         1.         0.9918879\n",
      " 0.9951241  1.         1.         1.         0.99922335 0.9788697\n",
      " 1.         0.9992726  0.9142334  0.9987005  1.         0.99367326\n",
      " 0.9991013  1.         1.         1.         1.         1.\n",
      " 0.09674563 0.9027798  1.         0.99977666 1.         1.\n",
      " 1.         0.9977614  1.         1.         1.         1.\n",
      " 1.         1.         0.9978935  1.         0.8207451  0.9667976\n",
      " 1.         0.9451848  1.         1.         1.         0.9951902\n",
      " 1.         1.         1.         1.         0.9939515  0.99876285\n",
      " 0.99892724 1.         1.         1.         1.         0.99911314\n",
      " 0.99683344 0.99687284 0.99994147 1.         1.        ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00010883446593333163, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 37.28462181480594\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 317.939\n",
      "iteration 200 loss 317.9383\n",
      "iteration 300 loss 317.93762\n",
      "iteration 400 loss 317.93695\n",
      "iteration 500 loss 317.93634\n",
      "iteration 600 loss 317.93576\n",
      "iteration 700 loss 317.93518\n",
      "iteration 800 loss 317.93463\n",
      "iteration 900 loss 317.93408\n",
      "iteration 1000 loss 317.93353\n",
      "iteration 1100 loss 317.93304\n",
      "iteration 1200 loss 317.93256\n",
      "iteration 1300 loss 317.93204\n",
      "iteration 1400 loss 317.93155\n",
      "iteration 1500 loss 317.93115\n",
      "iteration 1600 loss 317.93066\n",
      "iteration 1700 loss 317.93024\n",
      "iteration 1800 loss 317.92987\n",
      "iteration 1900 loss 317.9295\n",
      "iteration 2000 loss 317.92908\n",
      "iteration 2100 loss 317.9287\n",
      "iteration 2200 loss 317.9284\n",
      "iteration 2300 loss 317.92804\n",
      "iteration 2400 loss 317.92767\n",
      "iteration 2500 loss 317.92737\n",
      "iteration 2600 loss 317.9271\n",
      "iteration 2700 loss 317.92676\n",
      "iteration 2800 loss 317.92648\n",
      "iteration 2900 loss 317.9262\n",
      "iteration 3000 loss 317.92593\n",
      "iteration 3100 loss 317.9257\n",
      "iteration 3200 loss 317.9254\n",
      "iteration 3300 loss 317.92517\n",
      "iteration 3400 loss 317.92493\n",
      "iteration 3500 loss 317.9247\n",
      "iteration 3600 loss 317.9245\n",
      "iteration 3700 loss 317.92426\n",
      "iteration 3800 loss 317.924\n",
      "iteration 3900 loss 317.92386\n",
      "iteration 4000 loss 317.92365\n",
      "iteration 4100 loss 317.92346\n",
      "iteration 4200 loss 317.92328\n",
      "iteration 4300 loss 317.9231\n",
      "iteration 4400 loss 317.92294\n",
      "iteration 4500 loss 317.9228\n",
      "iteration 4600 loss 317.92264\n",
      "iteration 4700 loss 317.9225\n",
      "iteration 4800 loss 317.92233\n",
      "iteration 4900 loss 317.92215\n",
      "iteration 5000 loss 317.92203Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [0.9999247  1.         1.         0.99954385 1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         0.9998219  0.9826311\n",
      " 1.         1.         1.         1.         0.99999636 1.\n",
      " 1.         0.99350756 1.         1.         1.         1.\n",
      " 0.71160245 1.         1.         0.9998929  1.         1.\n",
      " 1.         0.9997906  1.         1.         1.         1.\n",
      " 1.         1.         0.9999222  1.         1.         1.\n",
      " 1.         0.9986869  1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.9992416\n",
      " 0.9991948  1.         0.9998488  1.         1.         0.9985464\n",
      " 1.         0.9993938  0.9998625  1.         1.        ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 8.82878234386963e-05, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 8.774938881621416\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 285.4501\n",
      "iteration 200 loss 285.44977\n",
      "iteration 300 loss 285.44946\n",
      "iteration 400 loss 285.44913\n",
      "iteration 500 loss 285.44873\n",
      "iteration 600 loss 285.44836\n",
      "iteration 700 loss 285.44806\n",
      "iteration 800 loss 285.44766\n",
      "iteration 900 loss 285.44736\n",
      "iteration 1000 loss 285.44696\n",
      "iteration 1100 loss 285.4466\n",
      "iteration 1200 loss 285.4462\n",
      "iteration 1300 loss 285.4458\n",
      "iteration 1400 loss 285.44537\n",
      "iteration 1500 loss 285.44495\n",
      "iteration 1600 loss 285.4445\n",
      "iteration 1700 loss 285.44406\n",
      "iteration 1800 loss 285.44354\n",
      "iteration 1900 loss 285.44305\n",
      "iteration 2000 loss 285.44254\n",
      "iteration 2100 loss 285.44196\n",
      "iteration 2200 loss 285.44138\n",
      "iteration 2300 loss 285.44073\n",
      "iteration 2400 loss 285.4401\n",
      "iteration 2500 loss 285.4394\n",
      "iteration 2600 loss 285.4387\n",
      "iteration 2700 loss 285.4379\n",
      "iteration 2800 loss 285.43707\n",
      "iteration 2900 loss 285.43628\n",
      "iteration 3000 loss 285.43536\n",
      "iteration 3100 loss 285.4344\n",
      "iteration 3200 loss 285.4334\n",
      "iteration 3300 loss 285.43234\n",
      "iteration 3400 loss 285.43127\n",
      "iteration 3500 loss 285.43005\n",
      "iteration 3600 loss 285.42883\n",
      "iteration 3700 loss 285.42755\n",
      "iteration 3800 loss 285.42618\n",
      "iteration 3900 loss 285.4248\n",
      "iteration 4000 loss 285.42325\n",
      "iteration 4100 loss 285.4217\n",
      "iteration 4200 loss 285.41998\n",
      "iteration 4300 loss 285.4182\n",
      "iteration 4400 loss 285.4164\n",
      "iteration 4500 loss 285.4145\n",
      "iteration 4600 loss 285.4124\n",
      "iteration 4700 loss 285.41028\n",
      "iteration 4800 loss 285.408\n",
      "iteration 4900 loss 285.40558\n",
      "iteration 5000 loss 285.4031Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         1.         1.         1.         0.99984324 1.\n",
      " 1.         1.         0.6864499  1.         0.9934655  0.7612484\n",
      " 0.99968094 1.         1.         1.         1.         1.\n",
      " 1.         0.99986583 1.         1.         0.9996269  1.\n",
      " 1.         0.99953216 1.         0.99991465 1.         0.99981856\n",
      " 0.9999086  1.         1.         1.         1.         1.\n",
      " 0.7801737  1.         1.         1.         1.         1.\n",
      " 1.         0.9995945  1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.99881727\n",
      " 1.         1.         1.         1.         1.         0.99584204\n",
      " 1.         1.         0.98363155 1.         0.99754685 1.\n",
      " 0.99759924 1.         1.         1.         1.         0.9976773\n",
      " 0.99932927 1.         0.99976015 1.         1.        ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00013448182109152468, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 24.032051245019176\n",
      "\n",
      " 7 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 313.499\n",
      "iteration 200 loss 313.499\n",
      "iteration 300 loss 313.49896\n",
      "iteration 400 loss 313.49896\n",
      "iteration 500 loss 313.49896\n",
      "iteration 600 loss 313.49896\n",
      "iteration 700 loss 313.49896\n",
      "iteration 800 loss 313.49896\n",
      "iteration 900 loss 313.49896\n",
      "iteration 1000 loss 313.49896\n",
      "iteration 1100 loss 313.49896\n",
      "iteration 1200 loss 313.49896\n",
      "iteration 1300 loss 313.49896\n",
      "iteration 1400 loss 313.49896\n",
      "iteration 1500 loss 313.49896\n",
      "iteration 1600 loss 313.499\n",
      "iteration 1700 loss 313.49896\n",
      "iteration 1800 loss 313.49896\n",
      "iteration 1900 loss 313.49896\n",
      "iteration 2000 loss 313.49893\n",
      "iteration 2100 loss 313.49893\n",
      "iteration 2200 loss 313.49896\n",
      "iteration 2300 loss 313.49896\n",
      "iteration 2400 loss 313.49893\n",
      "iteration 2500 loss 313.49893\n",
      "iteration 2600 loss 313.49893\n",
      "iteration 2700 loss 313.49896\n",
      "iteration 2800 loss 313.49893\n",
      "iteration 2900 loss 313.49896\n",
      "iteration 3000 loss 313.49893\n",
      "iteration 3100 loss 313.49893\n",
      "iteration 3200 loss 313.49893\n",
      "iteration 3300 loss 313.4989\n",
      "iteration 3400 loss 313.49896\n",
      "iteration 3500 loss 313.49893\n",
      "iteration 3600 loss 313.49893\n",
      "iteration 3700 loss 313.49893\n",
      "iteration 3800 loss 313.4989\n",
      "iteration 3900 loss 313.4989\n",
      "iteration 4000 loss 313.49893\n",
      "iteration 4100 loss 313.49893\n",
      "iteration 4200 loss 313.49893\n",
      "iteration 4300 loss 313.49893\n",
      "iteration 4400 loss 313.49893\n",
      "iteration 4500 loss 313.49893\n",
      "iteration 4600 loss 313.49893\n",
      "iteration 4700 loss 313.49893\n",
      "iteration 4800 loss 313.49896\n",
      "iteration 4900 loss 313.49896\n",
      "iteration 5000 loss 313.49896Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.9856949\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.9988079  1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.999702\n",
      " 0.99910593 1.         1.         0.99386376 1.         0.999702\n",
      " 1.         1.         1.         1.         1.        ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00011246187538827717, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 13.122057625029495\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 272.2457\n",
      "iteration 200 loss 272.24524\n",
      "iteration 300 loss 272.24487\n",
      "iteration 400 loss 272.24448\n",
      "iteration 500 loss 272.24408\n",
      "iteration 600 loss 272.24368\n",
      "iteration 700 loss 272.24335\n",
      "iteration 800 loss 272.24298\n",
      "iteration 900 loss 272.2426\n",
      "iteration 1000 loss 272.2423\n",
      "iteration 1100 loss 272.24194\n",
      "iteration 1200 loss 272.2416\n",
      "iteration 1300 loss 272.2413\n",
      "iteration 1400 loss 272.24103\n",
      "iteration 1500 loss 272.2407\n",
      "iteration 1600 loss 272.2404\n",
      "iteration 1700 loss 272.24008\n",
      "iteration 1800 loss 272.2398\n",
      "iteration 1900 loss 272.23956\n",
      "iteration 2000 loss 272.23932\n",
      "iteration 2100 loss 272.239\n",
      "iteration 2200 loss 272.23877\n",
      "iteration 2300 loss 272.23853\n",
      "iteration 2400 loss 272.23828\n",
      "iteration 2500 loss 272.23807\n",
      "iteration 2600 loss 272.23785\n",
      "iteration 2700 loss 272.2376\n",
      "iteration 2800 loss 272.23737\n",
      "iteration 2900 loss 272.23718\n",
      "iteration 3000 loss 272.23694\n",
      "iteration 3100 loss 272.23676\n",
      "iteration 3200 loss 272.23657\n",
      "iteration 3300 loss 272.2364\n",
      "iteration 3400 loss 272.2362\n",
      "iteration 3500 loss 272.236\n",
      "iteration 3600 loss 272.23584\n",
      "iteration 3700 loss 272.23566\n",
      "iteration 3800 loss 272.23547\n",
      "iteration 3900 loss 272.23532\n",
      "iteration 4000 loss 272.23517\n",
      "iteration 4100 loss 272.235\n",
      "iteration 4200 loss 272.23486\n",
      "iteration 4300 loss 272.23468\n",
      "iteration 4400 loss 272.23456\n",
      "iteration 4500 loss 272.23444\n",
      "iteration 4600 loss 272.2343\n",
      "iteration 4700 loss 272.2342\n",
      "iteration 4800 loss 272.23407\n",
      "iteration 4900 loss 272.23395\n",
      "iteration 5000 loss 272.23383Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [0.9998673  1.         1.         0.99961704 1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         0.99986625 1.\n",
      " 1.         1.         1.         1.         0.9998033  0.9816807\n",
      " 1.         1.         1.         1.         0.99995583 1.\n",
      " 1.         0.99353886 1.         1.         1.         1.\n",
      " 0.7566241  1.         1.         1.         1.         1.\n",
      " 1.         0.9998267  1.         1.         1.         1.\n",
      " 1.         1.         0.9999818  1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.9987052  1.         1.         1.         1.         0.9992094\n",
      " 0.99905217 1.         1.         1.         1.         0.99873644\n",
      " 1.         0.9995826  0.9998173  1.         0.9999566 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00011511764920938082, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 27.900860016731503\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 258.9009\n",
      "iteration 200 loss 258.88776\n",
      "iteration 300 loss 258.87634\n",
      "iteration 400 loss 258.86646\n",
      "iteration 500 loss 258.85794\n",
      "iteration 600 loss 258.8506\n",
      "iteration 700 loss 258.84418\n",
      "iteration 800 loss 258.83868\n",
      "iteration 900 loss 258.8339\n",
      "iteration 1000 loss 258.82928\n",
      "iteration 1100 loss 258.82434\n",
      "iteration 1200 loss 258.81866\n",
      "iteration 1300 loss 258.81158\n",
      "iteration 1400 loss 258.8026\n",
      "iteration 1500 loss 258.79065\n",
      "iteration 1600 loss 258.77414\n",
      "iteration 1700 loss 258.75055\n",
      "iteration 1800 loss 258.7159\n",
      "iteration 1900 loss 258.65988\n",
      "iteration 2000 loss 258.55707\n",
      "iteration 2100 loss 258.34363\n",
      "iteration 2200 loss 257.8578\n",
      "iteration 2300 loss 256.96265\n",
      "iteration 2400 loss 255.98755\n",
      "iteration 2500 loss 255.17691\n",
      "iteration 2600 loss 254.46455\n",
      "iteration 2700 loss 253.83203\n",
      "iteration 2800 loss 253.37949\n",
      "iteration 2900 loss 253.11823\n",
      "iteration 3000 loss 252.8832\n",
      "iteration 3100 loss 252.6461\n",
      "iteration 3200 loss 252.44946\n",
      "iteration 3300 loss 252.40555\n",
      "iteration 3400 loss 252.36487\n",
      "iteration 3500 loss 252.32681\n",
      "iteration 3600 loss 252.29077\n",
      "iteration 3700 loss 252.25616\n",
      "iteration 3800 loss 252.22258\n",
      "iteration 3900 loss 252.18977\n",
      "iteration 4000 loss 252.15727\n",
      "iteration 4100 loss 252.125\n",
      "iteration 4200 loss 252.09262\n",
      "iteration 4300 loss 252.06003\n",
      "iteration 4400 loss 252.02707\n",
      "iteration 4500 loss 251.99368\n",
      "iteration 4600 loss 251.96182\n",
      "iteration 4700 loss 251.9558\n",
      "iteration 4800 loss 251.9501\n",
      "iteration 4900 loss 251.94466\n",
      "iteration 5000 loss 251.93947Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         0.9984599  0.97754544 1.         0.9999137  1.\n",
      " 1.         0.         0.         0.7376735  0.         0.\n",
      " 0.9997848  1.         1.         1.         1.         1.\n",
      " 0.99856645 1.         1.         1.         0.9972469  1.\n",
      " 1.         0.98543334 0.5090411  0.9993402  1.         0.99509233\n",
      " 0.99916804 1.         1.         1.         1.         1.\n",
      " 0.2991582  1.         1.         1.         0.7714132  1.\n",
      " 1.         0.9830429  1.         1.         1.         1.\n",
      " 0.4001388  0.         0.9952417  1.         0.         0.9261861\n",
      " 1.         1.         1.         1.         1.         0.94804037\n",
      " 1.         1.         0.8744142  1.         0.9172593  0.9955429\n",
      " 0.9223585  1.         1.         0.9005819  0.         0.9510724\n",
      " 0.976653   0.99120504 0.996576   1.         1.        ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00010683117507497003, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 33.6006342647717\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 271.1023\n",
      "iteration 200 loss 271.0962\n",
      "iteration 300 loss 271.08734\n",
      "iteration 400 loss 271.07175\n",
      "iteration 500 loss 271.04193\n",
      "iteration 600 loss 270.98145\n",
      "iteration 700 loss 270.8521\n",
      "iteration 800 loss 270.55762\n",
      "iteration 900 loss 269.86786\n",
      "iteration 1000 loss 268.8098\n",
      "iteration 1100 loss 267.66843\n",
      "iteration 1200 loss 266.50848\n",
      "iteration 1300 loss 265.34088\n",
      "iteration 1400 loss 263.9065\n",
      "iteration 1500 loss 261.8446\n",
      "iteration 1600 loss 260.54816\n",
      "iteration 1700 loss 259.07428\n",
      "iteration 1800 loss 258.5274\n",
      "iteration 1900 loss 258.27386\n",
      "iteration 2000 loss 258.22748\n",
      "iteration 2100 loss 258.2145\n",
      "iteration 2200 loss 258.20337\n",
      "iteration 2300 loss 258.19324\n",
      "iteration 2400 loss 258.18378\n",
      "iteration 2500 loss 258.17468\n",
      "iteration 2600 loss 258.16595\n",
      "iteration 2700 loss 258.1574\n",
      "iteration 2800 loss 258.1491\n",
      "iteration 2900 loss 258.1409\n",
      "iteration 3000 loss 258.1328\n",
      "iteration 3100 loss 258.12488\n",
      "iteration 3200 loss 258.11703\n",
      "iteration 3300 loss 258.10928\n",
      "iteration 3400 loss 258.10156\n",
      "iteration 3500 loss 258.094\n",
      "iteration 3600 loss 258.0865\n",
      "iteration 3700 loss 258.07904\n",
      "iteration 3800 loss 258.07166\n",
      "iteration 3900 loss 258.06433\n",
      "iteration 4000 loss 258.05704\n",
      "iteration 4100 loss 258.04987\n",
      "iteration 4200 loss 258.04272\n",
      "iteration 4300 loss 258.03564\n",
      "iteration 4400 loss 258.0286\n",
      "iteration 4500 loss 258.0216\n",
      "iteration 4600 loss 258.0147\n",
      "iteration 4700 loss 258.0078\n",
      "iteration 4800 loss 258.00098\n",
      "iteration 4900 loss 257.99774\n",
      "iteration 5000 loss 257.99576Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         1.         1.         1.         0.9906189  1.\n",
      " 0.9173822  0.00123717 0.         0.         0.         0.\n",
      " 0.97864336 1.         0.1525967  0.92150724 1.         0.9632176\n",
      " 0.962994   1.         1.         1.         0.9846224  0.9144803\n",
      " 1.         0.9799915  0.         0.9870236  1.         0.8957425\n",
      " 0.98496526 1.         1.         1.         1.         1.\n",
      " 0.7962782  0.89828795 0.99977434 1.         0.09440441 1.\n",
      " 1.         0.96666205 0.7484301  1.         1.         1.\n",
      " 0.84630495 0.         0.9888617  1.         1.         0.6963484\n",
      " 1.         1.         1.         0.97694576 1.         0.83642524\n",
      " 1.         1.         1.         1.         0.9188785  1.\n",
      " 0.9622483  0.9913387  1.         1.         0.         0.9234646\n",
      " 0.9668076  0.98393416 0.9946228  1.         1.        ]\n",
      "Best params obtained by cv: {'alpha': 1e-05, 'coef0': 1, 'degree': 3, 'gamma': 8.763948087232004e-05, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 40.13417762436063\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 243.58273\n",
      "iteration 200 loss 243.58237\n",
      "iteration 300 loss 243.58197\n",
      "iteration 400 loss 243.58157\n",
      "iteration 500 loss 243.5812\n",
      "iteration 600 loss 243.58087\n",
      "iteration 700 loss 243.5805\n",
      "iteration 800 loss 243.58015\n",
      "iteration 900 loss 243.57977\n",
      "iteration 1000 loss 243.57944\n",
      "iteration 1100 loss 243.57907\n",
      "iteration 1200 loss 243.57874\n",
      "iteration 1300 loss 243.5784\n",
      "iteration 1400 loss 243.57806\n",
      "iteration 1500 loss 243.5777\n",
      "iteration 1600 loss 243.5774\n",
      "iteration 1700 loss 243.57706\n",
      "iteration 1800 loss 243.57677\n",
      "iteration 1900 loss 243.57645\n",
      "iteration 2000 loss 243.57611\n",
      "iteration 2100 loss 243.5758\n",
      "iteration 2200 loss 243.5755\n",
      "iteration 2300 loss 243.5752\n",
      "iteration 2400 loss 243.5749\n",
      "iteration 2500 loss 243.57458\n",
      "iteration 2600 loss 243.57422\n",
      "iteration 2700 loss 243.57391\n",
      "iteration 2800 loss 243.57361\n",
      "iteration 2900 loss 243.57332\n",
      "iteration 3000 loss 243.57306\n",
      "iteration 3100 loss 243.57275\n",
      "iteration 3200 loss 243.57245\n",
      "iteration 3300 loss 243.57211\n",
      "iteration 3400 loss 243.57181\n",
      "iteration 3500 loss 243.5715\n",
      "iteration 3600 loss 243.57123\n",
      "iteration 3700 loss 243.57086\n",
      "iteration 3800 loss 243.57062\n",
      "iteration 3900 loss 243.57025\n",
      "iteration 4000 loss 243.56995\n",
      "iteration 4100 loss 243.56964\n",
      "iteration 4200 loss 243.56934\n",
      "iteration 4300 loss 243.56903\n",
      "iteration 4400 loss 243.56871\n",
      "iteration 4500 loss 243.5684\n",
      "iteration 4600 loss 243.56807\n",
      "iteration 4700 loss 243.56772\n",
      "iteration 4800 loss 243.56741\n",
      "iteration 4900 loss 243.56708\n",
      "iteration 5000 loss 243.56674Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [0.999515   1.         0.9905234  0.99952704 1.         1.\n",
      " 1.         1.         1.         1.         0.98654795 1.\n",
      " 1.         1.         1.         1.         0.9998823  1.\n",
      " 1.         1.         1.         1.         0.9998687  0.9983312\n",
      " 0.9980525  1.         1.         1.         0.99988824 1.\n",
      " 1.         0.994724   1.         1.         1.         0.9998024\n",
      " 0.7445558  1.         1.         1.         1.         1.\n",
      " 1.         0.999584   1.         0.99964917 1.         1.\n",
      " 1.         1.         1.         1.         0.9343345  1.\n",
      " 0.90229386 0.9973959  1.         1.         1.         1.\n",
      " 0.99918294 0.9998064  1.         1.         1.         0.99975777\n",
      " 0.99958986 1.         0.99983513 1.         1.         0.9995304\n",
      " 1.         0.9997086  1.         1.         0.9966299 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00014948068117108578, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 41.768729461795786\n",
      "\n",
      " 8 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 276.98755\n",
      "iteration 200 loss 276.98688\n",
      "iteration 300 loss 276.9862\n",
      "iteration 400 loss 276.98553\n",
      "iteration 500 loss 276.9849\n",
      "iteration 600 loss 276.98425\n",
      "iteration 700 loss 276.98358\n",
      "iteration 800 loss 276.98297\n",
      "iteration 900 loss 276.98233\n",
      "iteration 1000 loss 276.98172\n",
      "iteration 1100 loss 276.98105\n",
      "iteration 1200 loss 276.9804\n",
      "iteration 1300 loss 276.97977\n",
      "iteration 1400 loss 276.9792\n",
      "iteration 1500 loss 276.97852\n",
      "iteration 1600 loss 276.9779\n",
      "iteration 1700 loss 276.9773\n",
      "iteration 1800 loss 276.97668\n",
      "iteration 1900 loss 276.97607\n",
      "iteration 2000 loss 276.97546\n",
      "iteration 2100 loss 276.97485\n",
      "iteration 2200 loss 276.97418\n",
      "iteration 2300 loss 276.97363\n",
      "iteration 2400 loss 276.97296\n",
      "iteration 2500 loss 276.9724\n",
      "iteration 2600 loss 276.97177\n",
      "iteration 2700 loss 276.9712\n",
      "iteration 2800 loss 276.97058\n",
      "iteration 2900 loss 276.96997\n",
      "iteration 3000 loss 276.96936\n",
      "iteration 3100 loss 276.96875\n",
      "iteration 3200 loss 276.9682\n",
      "iteration 3300 loss 276.9676\n",
      "iteration 3400 loss 276.96698\n",
      "iteration 3500 loss 276.96643\n",
      "iteration 3600 loss 276.96582\n",
      "iteration 3700 loss 276.96524\n",
      "iteration 3800 loss 276.96466\n",
      "iteration 3900 loss 276.96405\n",
      "iteration 4000 loss 276.9635\n",
      "iteration 4100 loss 276.96298\n",
      "iteration 4200 loss 276.96237\n",
      "iteration 4300 loss 276.9618\n",
      "iteration 4400 loss 276.9612\n",
      "iteration 4500 loss 276.96066\n",
      "iteration 4600 loss 276.9601\n",
      "iteration 4700 loss 276.95953\n",
      "iteration 4800 loss 276.95898\n",
      "iteration 4900 loss 276.9584\n",
      "iteration 5000 loss 276.95782Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         1.         1.         1.         1.         1.\n",
      " 0.9994739  1.         1.         1.         1.         0.91558063\n",
      " 0.9998631  1.         1.         1.         1.         0.99993753\n",
      " 0.99974716 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.99901736 1.         1.         1.         1.\n",
      " 0.7041304  1.         1.         0.9997284  1.         1.\n",
      " 1.         0.9993016  1.         1.         1.         1.\n",
      " 1.         1.         0.9998563  1.         1.         1.\n",
      " 0.7642103  0.9992137  1.         1.         1.         0.9998758\n",
      " 0.9983841  1.         1.         1.         1.         1.\n",
      " 0.9974304  1.         1.         1.         0.96785814 0.99948865\n",
      " 1.         0.9989832  1.         1.         1.        ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00010934243230359633, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 26.51498188066005\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 283.83746\n",
      "iteration 200 loss 283.82513\n",
      "iteration 300 loss 283.8142\n",
      "iteration 400 loss 283.80453\n",
      "iteration 500 loss 283.79596\n",
      "iteration 600 loss 283.7884\n",
      "iteration 700 loss 283.7818\n",
      "iteration 800 loss 283.776\n",
      "iteration 900 loss 283.77087\n",
      "iteration 1000 loss 283.76648\n",
      "iteration 1100 loss 283.76254\n",
      "iteration 1200 loss 283.7591\n",
      "iteration 1300 loss 283.7561\n",
      "iteration 1400 loss 283.75342\n",
      "iteration 1500 loss 283.75104\n",
      "iteration 1600 loss 283.7489\n",
      "iteration 1700 loss 283.747\n",
      "iteration 1800 loss 283.74536\n",
      "iteration 1900 loss 283.74384\n",
      "iteration 2000 loss 283.7425\n",
      "iteration 2100 loss 283.7412\n",
      "iteration 2200 loss 283.74005\n",
      "iteration 2300 loss 283.739\n",
      "iteration 2400 loss 283.7381\n",
      "iteration 2500 loss 283.73718\n",
      "iteration 2600 loss 283.73636\n",
      "iteration 2700 loss 283.7356\n",
      "iteration 2800 loss 283.73483\n",
      "iteration 2900 loss 283.73416\n",
      "iteration 3000 loss 283.73352\n",
      "iteration 3100 loss 283.73294\n",
      "iteration 3200 loss 283.73236\n",
      "iteration 3300 loss 283.7318\n",
      "iteration 3400 loss 283.7312\n",
      "iteration 3500 loss 283.7307\n",
      "iteration 3600 loss 283.73026\n",
      "iteration 3700 loss 283.72974\n",
      "iteration 3800 loss 283.72928\n",
      "iteration 3900 loss 283.72885\n",
      "iteration 4000 loss 283.7284\n",
      "iteration 4100 loss 283.728\n",
      "iteration 4200 loss 283.72754\n",
      "iteration 4300 loss 283.7271\n",
      "iteration 4400 loss 283.72668\n",
      "iteration 4500 loss 283.72638\n",
      "iteration 4600 loss 283.72595\n",
      "iteration 4700 loss 283.7256\n",
      "iteration 4800 loss 283.72525\n",
      "iteration 4900 loss 283.72485\n",
      "iteration 5000 loss 283.72455Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         1.         1.         1.         1.         1.\n",
      " 0.99985105 1.         0.48840067 1.         0.9595521  1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.9998765  1.         1.         0.9991703  0.9877236\n",
      " 1.         0.99856085 0.87800026 1.         1.         0.99369675\n",
      " 0.9992505  1.         0.9987033  1.         1.         1.\n",
      " 0.67813    1.         1.         1.         1.         1.\n",
      " 1.         0.99994004 1.         1.         1.         1.\n",
      " 1.         1.         0.99983376 1.         1.         0.99576575\n",
      " 1.         1.         1.         0.9946957  1.         0.99110067\n",
      " 1.         1.         1.         1.         0.9975615  0.999816\n",
      " 1.         0.9978037  1.         1.         1.         0.9954019\n",
      " 0.9985352  0.99873465 0.9993742  1.         1.        ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 9.362754025235552e-05, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 23.43326612365642\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 232.67976\n",
      "iteration 200 loss 232.67952\n",
      "iteration 300 loss 232.67921\n",
      "iteration 400 loss 232.67897\n",
      "iteration 500 loss 232.67873\n",
      "iteration 600 loss 232.67845\n",
      "iteration 700 loss 232.67824\n",
      "iteration 800 loss 232.67796\n",
      "iteration 900 loss 232.6777\n",
      "iteration 1000 loss 232.67747\n",
      "iteration 1100 loss 232.67723\n",
      "iteration 1200 loss 232.677\n",
      "iteration 1300 loss 232.6768\n",
      "iteration 1400 loss 232.6766\n",
      "iteration 1500 loss 232.67639\n",
      "iteration 1600 loss 232.67618\n",
      "iteration 1700 loss 232.67595\n",
      "iteration 1800 loss 232.67577\n",
      "iteration 1900 loss 232.67555\n",
      "iteration 2000 loss 232.67538\n",
      "iteration 2100 loss 232.6752\n",
      "iteration 2200 loss 232.67503\n",
      "iteration 2300 loss 232.67484\n",
      "iteration 2400 loss 232.67467\n",
      "iteration 2500 loss 232.67447\n",
      "iteration 2600 loss 232.67432\n",
      "iteration 2700 loss 232.67413\n",
      "iteration 2800 loss 232.674\n",
      "iteration 2900 loss 232.67381\n",
      "iteration 3000 loss 232.67368\n",
      "iteration 3100 loss 232.67351\n",
      "iteration 3200 loss 232.67337\n",
      "iteration 3300 loss 232.67322\n",
      "iteration 3400 loss 232.6731\n",
      "iteration 3500 loss 232.67294\n",
      "iteration 3600 loss 232.6728\n",
      "iteration 3700 loss 232.67265\n",
      "iteration 3800 loss 232.67255\n",
      "iteration 3900 loss 232.67238\n",
      "iteration 4000 loss 232.67227\n",
      "iteration 4100 loss 232.6721\n",
      "iteration 4200 loss 232.67203\n",
      "iteration 4300 loss 232.67192\n",
      "iteration 4400 loss 232.67181\n",
      "iteration 4500 loss 232.67168\n",
      "iteration 4600 loss 232.67157\n",
      "iteration 4700 loss 232.67145\n",
      "iteration 4800 loss 232.67136\n",
      "iteration 4900 loss 232.67125\n",
      "iteration 5000 loss 232.67114Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [0.9998328  1.         1.         0.99962336 1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.98855954\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.99798465 1.         1.         1.         0.99993587\n",
      " 0.7917128  1.         1.         1.         1.         1.\n",
      " 1.         0.99990237 1.         1.         1.         1.\n",
      " 1.         1.         1.         0.9949567  1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         0.9899324  1.         1.         0.99976224\n",
      " 0.99991524 1.         1.         1.         1.         1.\n",
      " 1.         0.9997754  1.         1.         1.        ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00011359525041782551, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 45.42018120225755\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 270.6471\n",
      "iteration 200 loss 270.64713\n",
      "iteration 300 loss 270.64713\n",
      "iteration 400 loss 270.64713\n",
      "iteration 500 loss 270.6471\n",
      "iteration 600 loss 270.64713\n",
      "iteration 700 loss 270.6471\n",
      "iteration 800 loss 270.6471\n",
      "iteration 900 loss 270.6471\n",
      "iteration 1000 loss 270.6471\n",
      "iteration 1100 loss 270.6471\n",
      "iteration 1200 loss 270.6471\n",
      "iteration 1300 loss 270.6471\n",
      "iteration 1400 loss 270.6471\n",
      "iteration 1500 loss 270.6471\n",
      "iteration 1600 loss 270.6471\n",
      "iteration 1700 loss 270.6471\n",
      "iteration 1800 loss 270.6471\n",
      "iteration 1900 loss 270.6471\n",
      "iteration 2000 loss 270.6471\n",
      "iteration 2100 loss 270.6471\n",
      "iteration 2200 loss 270.6471\n",
      "iteration 2300 loss 270.6471\n",
      "iteration 2400 loss 270.6471\n",
      "iteration 2500 loss 270.6471\n",
      "iteration 2600 loss 270.6471\n",
      "iteration 2700 loss 270.6471\n",
      "iteration 2800 loss 270.6471\n",
      "iteration 2900 loss 270.6471\n",
      "iteration 3000 loss 270.6471\n",
      "iteration 3100 loss 270.6471\n",
      "iteration 3200 loss 270.6471\n",
      "iteration 3300 loss 270.6471\n",
      "iteration 3400 loss 270.6471\n",
      "iteration 3500 loss 270.6471\n",
      "iteration 3600 loss 270.6471\n",
      "iteration 3700 loss 270.6471\n",
      "iteration 3800 loss 270.6471\n",
      "iteration 3900 loss 270.6471\n",
      "iteration 4000 loss 270.6471\n",
      "iteration 4100 loss 270.6471\n",
      "iteration 4200 loss 270.6471\n",
      "iteration 4300 loss 270.6471\n",
      "iteration 4400 loss 270.6471\n",
      "iteration 4500 loss 270.6471\n",
      "iteration 4600 loss 270.6471\n",
      "iteration 4700 loss 270.6471\n",
      "iteration 4800 loss 270.6471\n",
      "iteration 4900 loss 270.6471\n",
      "iteration 5000 loss 270.6471Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        0.9887404 1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.\n",
      " 1.        1.        1.        1.        1.        1.        1.       ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00011517222997650214, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 31.27854104430203\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 296.97446\n",
      "iteration 200 loss 296.9654\n",
      "iteration 300 loss 296.95703\n",
      "iteration 400 loss 296.9496\n",
      "iteration 500 loss 296.94287\n",
      "iteration 600 loss 296.9369\n",
      "iteration 700 loss 296.93158\n",
      "iteration 800 loss 296.92664\n",
      "iteration 900 loss 296.9219\n",
      "iteration 1000 loss 296.91724\n",
      "iteration 1100 loss 296.91238\n",
      "iteration 1200 loss 296.90726\n",
      "iteration 1300 loss 296.90164\n",
      "iteration 1400 loss 296.8954\n",
      "iteration 1500 loss 296.8883\n",
      "iteration 1600 loss 296.8802\n",
      "iteration 1700 loss 296.8709\n",
      "iteration 1800 loss 296.86008\n",
      "iteration 1900 loss 296.8475\n",
      "iteration 2000 loss 296.83282\n",
      "iteration 2100 loss 296.8158\n",
      "iteration 2200 loss 296.79602\n",
      "iteration 2300 loss 296.77332\n",
      "iteration 2400 loss 296.74744\n",
      "iteration 2500 loss 296.71826\n",
      "iteration 2600 loss 296.6856\n",
      "iteration 2700 loss 296.6493\n",
      "iteration 2800 loss 296.6089\n",
      "iteration 2900 loss 296.56366\n",
      "iteration 3000 loss 296.51245\n",
      "iteration 3100 loss 296.454\n",
      "iteration 3200 loss 296.3864\n",
      "iteration 3300 loss 296.30743\n",
      "iteration 3400 loss 296.214\n",
      "iteration 3500 loss 296.10205\n",
      "iteration 3600 loss 295.98074\n",
      "iteration 3700 loss 295.8521\n",
      "iteration 3800 loss 295.68616\n",
      "iteration 3900 loss 295.4712\n",
      "iteration 4000 loss 295.1937\n",
      "iteration 4100 loss 294.838\n",
      "iteration 4200 loss 294.3885\n",
      "iteration 4300 loss 293.96136\n",
      "iteration 4400 loss 293.56018\n",
      "iteration 4500 loss 293.36148\n",
      "iteration 4600 loss 293.33655\n",
      "iteration 4700 loss 293.31177\n",
      "iteration 4800 loss 293.28702\n",
      "iteration 4900 loss 293.2624\n",
      "iteration 5000 loss 293.2378Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         1.         1.         1.         0.99756175 1.\n",
      " 0.96808785 1.         0.         1.         0.19444013 0.11382473\n",
      " 0.9927911  0.95431    1.         1.         1.         0.9786291\n",
      " 0.97292227 0.9998271  1.         1.         1.         1.\n",
      " 1.         0.9904943  0.         0.9956626  1.         0.9650313\n",
      " 0.9952304  1.         0.9965915  1.         0.9999534  1.\n",
      " 0.48212942 0.8712153  1.         1.         1.         1.\n",
      " 1.         0.9884582  0.89814097 1.         1.         1.\n",
      " 1.         0.         1.         1.         1.         0.8624046\n",
      " 1.         1.         1.         0.9803268  1.         0.9106826\n",
      " 1.         1.         0.89895797 1.         1.         0.9945764\n",
      " 0.9815707  0.99531955 1.         1.         0.         1.\n",
      " 0.98739886 1.         1.         1.         1.        ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00013708788602855205, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 17.972138000118253\n",
      "\n",
      " 9 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 291.0194\n",
      "iteration 200 loss 290.97208\n",
      "iteration 300 loss 290.93842\n",
      "iteration 400 loss 290.91473\n",
      "iteration 500 loss 290.89725\n",
      "iteration 600 loss 290.8831\n",
      "iteration 700 loss 290.87036\n",
      "iteration 800 loss 290.85745\n",
      "iteration 900 loss 290.84314\n",
      "iteration 1000 loss 290.8266\n",
      "iteration 1100 loss 290.8069\n",
      "iteration 1200 loss 290.78317\n",
      "iteration 1300 loss 290.75476\n",
      "iteration 1400 loss 290.72095\n",
      "iteration 1500 loss 290.6809\n",
      "iteration 1600 loss 290.63318\n",
      "iteration 1700 loss 290.562\n",
      "iteration 1800 loss 290.34525\n",
      "iteration 1900 loss 289.59375\n",
      "iteration 2000 loss 288.2959\n",
      "iteration 2100 loss 286.2716\n",
      "iteration 2200 loss 284.7401\n",
      "iteration 2300 loss 283.5305\n",
      "iteration 2400 loss 282.42648\n",
      "iteration 2500 loss 281.3022\n",
      "iteration 2600 loss 280.1917\n",
      "iteration 2700 loss 279.7339\n",
      "iteration 2800 loss 279.60562\n",
      "iteration 2900 loss 279.47754\n",
      "iteration 3000 loss 279.35034\n",
      "iteration 3100 loss 279.22437\n",
      "iteration 3200 loss 279.2048\n",
      "iteration 3300 loss 279.18796\n",
      "iteration 3400 loss 279.17175\n",
      "iteration 3500 loss 279.1562\n",
      "iteration 3600 loss 279.1411\n",
      "iteration 3700 loss 279.12646\n",
      "iteration 3800 loss 279.11227\n",
      "iteration 3900 loss 279.09845\n",
      "iteration 4000 loss 279.08502\n",
      "iteration 4100 loss 279.0719\n",
      "iteration 4200 loss 279.05905\n",
      "iteration 4300 loss 279.04648\n",
      "iteration 4400 loss 279.03418\n",
      "iteration 4500 loss 279.02213\n",
      "iteration 4600 loss 279.01028\n",
      "iteration 4700 loss 278.9986\n",
      "iteration 4800 loss 278.98712\n",
      "iteration 4900 loss 278.9809\n",
      "iteration 5000 loss 278.97733Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         1.         1.         1.         0.98745066 1.\n",
      " 0.9392884  0.         0.         0.         0.         0.\n",
      " 0.9886692  1.         0.16184135 1.         0.9963344  0.96886814\n",
      " 0.97010493 1.         1.         1.         0.99366033 0.79323274\n",
      " 1.         0.9837992  0.         0.9944432  1.         0.98991907\n",
      " 0.9979724  1.         1.         1.         1.         1.\n",
      " 0.7234914  1.         1.         1.         0.         1.\n",
      " 1.         0.9814576  1.         1.         1.         1.\n",
      " 0.86321944 0.         0.99275976 0.978061   0.76172477 0.74503\n",
      " 1.         0.74659157 1.         0.9865884  1.         0.93172014\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.9051234  1.         1.         0.77596307 0.         0.97229964\n",
      " 0.9748272  0.98732805 1.         1.         1.        ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 9.846734506821978e-05, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 20.802013130862527\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 220.06021\n",
      "iteration 200 loss 220.06024\n",
      "iteration 300 loss 220.0602\n",
      "iteration 400 loss 220.0602\n",
      "iteration 500 loss 220.0602\n",
      "iteration 600 loss 220.0602\n",
      "iteration 700 loss 220.06015\n",
      "iteration 800 loss 220.06013\n",
      "iteration 900 loss 220.06013\n",
      "iteration 1000 loss 220.06017\n",
      "iteration 1100 loss 220.06012\n",
      "iteration 1200 loss 220.06012\n",
      "iteration 1300 loss 220.0601\n",
      "iteration 1400 loss 220.0601\n",
      "iteration 1500 loss 220.06009\n",
      "iteration 1600 loss 220.06009\n",
      "iteration 1700 loss 220.06004\n",
      "iteration 1800 loss 220.06006\n",
      "iteration 1900 loss 220.06006\n",
      "iteration 2000 loss 220.06006\n",
      "iteration 2100 loss 220.06003\n",
      "iteration 2200 loss 220.06003\n",
      "iteration 2300 loss 220.06001\n",
      "iteration 2400 loss 220.06001\n",
      "iteration 2500 loss 220.06004\n",
      "iteration 2600 loss 220.06004\n",
      "iteration 2700 loss 220.06003\n",
      "iteration 2800 loss 220.06003\n",
      "iteration 2900 loss 220.06003\n",
      "iteration 3000 loss 220.06001\n",
      "iteration 3100 loss 220.06001\n",
      "iteration 3200 loss 220.06\n",
      "iteration 3300 loss 220.06\n",
      "iteration 3400 loss 220.06\n",
      "iteration 3500 loss 220.05998\n",
      "iteration 3600 loss 220.05997\n",
      "iteration 3700 loss 220.05997\n",
      "iteration 3800 loss 220.05997\n",
      "iteration 3900 loss 220.05995\n",
      "iteration 4000 loss 220.05995\n",
      "iteration 4100 loss 220.05995\n",
      "iteration 4200 loss 220.05994\n",
      "iteration 4300 loss 220.05992\n",
      "iteration 4400 loss 220.0599\n",
      "iteration 4500 loss 220.0599\n",
      "iteration 4600 loss 220.0599\n",
      "iteration 4700 loss 220.05988\n",
      "iteration 4800 loss 220.05989\n",
      "iteration 4900 loss 220.0599\n",
      "iteration 5000 loss 220.05989Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.9938466\n",
      " 1.         1.         0.984719   1.         1.         1.\n",
      " 1.         0.99910593 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         0.9621844  1.\n",
      " 1.         1.         1.         1.         1.        ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00010016859707446944, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 50.962060082942195\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 251.66885\n",
      "iteration 200 loss 251.66833\n",
      "iteration 300 loss 251.66791\n",
      "iteration 400 loss 251.66747\n",
      "iteration 500 loss 251.66704\n",
      "iteration 600 loss 251.66656\n",
      "iteration 700 loss 251.66617\n",
      "iteration 800 loss 251.66577\n",
      "iteration 900 loss 251.66533\n",
      "iteration 1000 loss 251.66493\n",
      "iteration 1100 loss 251.66452\n",
      "iteration 1200 loss 251.66412\n",
      "iteration 1300 loss 251.6637\n",
      "iteration 1400 loss 251.6633\n",
      "iteration 1500 loss 251.6629\n",
      "iteration 1600 loss 251.66252\n",
      "iteration 1700 loss 251.66208\n",
      "iteration 1800 loss 251.66167\n",
      "iteration 1900 loss 251.66129\n",
      "iteration 2000 loss 251.66086\n",
      "iteration 2100 loss 251.66043\n",
      "iteration 2200 loss 251.66006\n",
      "iteration 2300 loss 251.65959\n",
      "iteration 2400 loss 251.65915\n",
      "iteration 2500 loss 251.6587\n",
      "iteration 2600 loss 251.65823\n",
      "iteration 2700 loss 251.65775\n",
      "iteration 2800 loss 251.65723\n",
      "iteration 2900 loss 251.65675\n",
      "iteration 3000 loss 251.65619\n",
      "iteration 3100 loss 251.65558\n",
      "iteration 3200 loss 251.655\n",
      "iteration 3300 loss 251.65437\n",
      "iteration 3400 loss 251.65366\n",
      "iteration 3500 loss 251.65294\n",
      "iteration 3600 loss 251.65211\n",
      "iteration 3700 loss 251.65128\n",
      "iteration 3800 loss 251.65033\n",
      "iteration 3900 loss 251.64934\n",
      "iteration 4000 loss 251.6482\n",
      "iteration 4100 loss 251.64699\n",
      "iteration 4200 loss 251.64563\n",
      "iteration 4300 loss 251.64417\n",
      "iteration 4400 loss 251.6425\n",
      "iteration 4500 loss 251.64069\n",
      "iteration 4600 loss 251.63858\n",
      "iteration 4700 loss 251.63626\n",
      "iteration 4800 loss 251.63364\n",
      "iteration 4900 loss 251.63058\n",
      "iteration 5000 loss 251.62715Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [0.99868506 0.99977803 0.9820002  0.9984836  1.         1.\n",
      " 1.         1.         0.9117566  0.9992247  0.97332716 1.\n",
      " 0.99995476 1.         1.         1.         0.9997215  1.\n",
      " 1.         0.999958   1.         1.         0.9994444  0.97171295\n",
      " 0.98344743 1.         1.         1.         0.9996322  1.\n",
      " 1.         0.9845887  1.         1.         1.         0.99871564\n",
      " 0.70231825 1.         1.         1.         1.         1.\n",
      " 1.         0.9994948  1.         0.9973668  1.         1.\n",
      " 1.         1.         1.         0.9865641  1.         1.\n",
      " 0.8431228  1.         1.         1.         0.80845207 1.\n",
      " 1.         0.998838   1.         1.         0.9994979  0.9996651\n",
      " 0.9978036  1.         0.9994821  0.9741921  1.         0.9969583\n",
      " 1.         0.9996029  0.99956125 1.         0.9920183 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00012809626584569717, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 36.759895351953155\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 299.2687\n",
      "iteration 200 loss 299.2627\n",
      "iteration 300 loss 299.25696\n",
      "iteration 400 loss 299.25156\n",
      "iteration 500 loss 299.2464\n",
      "iteration 600 loss 299.24152\n",
      "iteration 700 loss 299.23688\n",
      "iteration 800 loss 299.23254\n",
      "iteration 900 loss 299.2284\n",
      "iteration 1000 loss 299.2245\n",
      "iteration 1100 loss 299.22083\n",
      "iteration 1200 loss 299.2173\n",
      "iteration 1300 loss 299.214\n",
      "iteration 1400 loss 299.21088\n",
      "iteration 1500 loss 299.20795\n",
      "iteration 1600 loss 299.20505\n",
      "iteration 1700 loss 299.20236\n",
      "iteration 1800 loss 299.19983\n",
      "iteration 1900 loss 299.1974\n",
      "iteration 2000 loss 299.19507\n",
      "iteration 2100 loss 299.19275\n",
      "iteration 2200 loss 299.1906\n",
      "iteration 2300 loss 299.18854\n",
      "iteration 2400 loss 299.18655\n",
      "iteration 2500 loss 299.18457\n",
      "iteration 2600 loss 299.18268\n",
      "iteration 2700 loss 299.18085\n",
      "iteration 2800 loss 299.17905\n",
      "iteration 2900 loss 299.17725\n",
      "iteration 3000 loss 299.17554\n",
      "iteration 3100 loss 299.17383\n",
      "iteration 3200 loss 299.17218\n",
      "iteration 3300 loss 299.17053\n",
      "iteration 3400 loss 299.16895\n",
      "iteration 3500 loss 299.1673\n",
      "iteration 3600 loss 299.16568\n",
      "iteration 3700 loss 299.16412\n",
      "iteration 3800 loss 299.16263\n",
      "iteration 3900 loss 299.16107\n",
      "iteration 4000 loss 299.15958\n",
      "iteration 4100 loss 299.15805\n",
      "iteration 4200 loss 299.15656\n",
      "iteration 4300 loss 299.15503\n",
      "iteration 4400 loss 299.15356\n",
      "iteration 4500 loss 299.15207\n",
      "iteration 4600 loss 299.1506\n",
      "iteration 4700 loss 299.14908\n",
      "iteration 4800 loss 299.14758\n",
      "iteration 4900 loss 299.14612\n",
      "iteration 5000 loss 299.14462Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         1.         1.         1.         0.9998387  0.99967796\n",
      " 0.99379456 1.         0.45097405 1.         0.92948127 1.\n",
      " 0.9995332  0.99952644 1.         1.         1.         0.99823153\n",
      " 0.99665815 1.         1.         1.         1.         1.\n",
      " 1.         0.99770516 0.9934908  0.9995993  1.         0.9943137\n",
      " 0.99931794 1.         0.9979832  1.         1.         1.\n",
      " 0.7280223  1.         1.         1.         1.         1.\n",
      " 1.         0.998079   0.9957059  1.         0.5797483  1.\n",
      " 1.         1.         0.9997949  1.         1.         0.97213733\n",
      " 1.         0.9997051  1.         0.99269634 1.         0.9826752\n",
      " 1.         1.         1.         1.         1.         0.9991704\n",
      " 1.         0.99833935 1.         1.         1.         0.99874735\n",
      " 0.99708    0.99908894 1.         1.         1.        ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00012816766749807206, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 16.083097546513127\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 293.292\n",
      "iteration 200 loss 293.28897\n",
      "iteration 300 loss 293.286\n",
      "iteration 400 loss 293.28323\n",
      "iteration 500 loss 293.2806\n",
      "iteration 600 loss 293.27808\n",
      "iteration 700 loss 293.27573\n",
      "iteration 800 loss 293.27353\n",
      "iteration 900 loss 293.27145\n",
      "iteration 1000 loss 293.26953\n",
      "iteration 1100 loss 293.2678\n",
      "iteration 1200 loss 293.26614\n",
      "iteration 1300 loss 293.26468\n",
      "iteration 1400 loss 293.2632\n",
      "iteration 1500 loss 293.2619\n",
      "iteration 1600 loss 293.2607\n",
      "iteration 1700 loss 293.25958\n",
      "iteration 1800 loss 293.25858\n",
      "iteration 1900 loss 293.2577\n",
      "iteration 2000 loss 293.25677\n",
      "iteration 2100 loss 293.25592\n",
      "iteration 2200 loss 293.25522\n",
      "iteration 2300 loss 293.25452\n",
      "iteration 2400 loss 293.2539\n",
      "iteration 2500 loss 293.25327\n",
      "iteration 2600 loss 293.25275\n",
      "iteration 2700 loss 293.2522\n",
      "iteration 2800 loss 293.25174\n",
      "iteration 2900 loss 293.25125\n",
      "iteration 3000 loss 293.25085\n",
      "iteration 3100 loss 293.2505\n",
      "iteration 3200 loss 293.2501\n",
      "iteration 3300 loss 293.24976\n",
      "iteration 3400 loss 293.24945\n",
      "iteration 3500 loss 293.24918\n",
      "iteration 3600 loss 293.24887\n",
      "iteration 3700 loss 293.24863\n",
      "iteration 3800 loss 293.24835\n",
      "iteration 3900 loss 293.24817\n",
      "iteration 4000 loss 293.24792\n",
      "iteration 4100 loss 293.24768\n",
      "iteration 4200 loss 293.2475\n",
      "iteration 4300 loss 293.24734\n",
      "iteration 4400 loss 293.24716\n",
      "iteration 4500 loss 293.24698\n",
      "iteration 4600 loss 293.2469\n",
      "iteration 4700 loss 293.24667\n",
      "iteration 4800 loss 293.24652\n",
      "iteration 4900 loss 293.24643\n",
      "iteration 5000 loss 293.24634Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         0.99963695 1.         1.         1.         1.\n",
      " 0.99839455 1.         1.         1.         1.         0.5714328\n",
      " 0.99933857 0.9117892  1.         1.         1.         0.99999756\n",
      " 0.9995697  0.9998596  1.         1.         0.9997488  0.98770744\n",
      " 1.         0.99925    1.         0.9994731  1.         0.99670345\n",
      " 0.9994661  1.         0.999032   1.         1.         1.\n",
      " 1.         1.         1.         0.99977815 1.         0.99983895\n",
      " 0.9998537  0.9990892  1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.99650943\n",
      " 1.         1.         1.         1.         1.         0.99823105\n",
      " 1.         1.         1.         1.         1.         0.99863017\n",
      " 0.99429655 0.9990517  1.         1.         1.         0.9976494\n",
      " 0.9990725  0.99901485 0.9997568  0.99976677 1.        ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00010351606647791397, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 20.316676046171565\n",
      "\n",
      " 10 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "\n",
      "iteration 100 loss 269.39038\n",
      "iteration 200 loss 269.3899\n",
      "iteration 300 loss 269.3894\n",
      "iteration 400 loss 269.38898\n",
      "iteration 500 loss 269.38852\n",
      "iteration 600 loss 269.38806\n",
      "iteration 700 loss 269.38763\n",
      "iteration 800 loss 269.38718\n",
      "iteration 900 loss 269.38678\n",
      "iteration 1000 loss 269.38638\n",
      "iteration 1100 loss 269.386\n",
      "iteration 1200 loss 269.38568\n",
      "iteration 1300 loss 269.38525\n",
      "iteration 1400 loss 269.3849\n",
      "iteration 1500 loss 269.38455\n",
      "iteration 1600 loss 269.38422\n",
      "iteration 1700 loss 269.3839\n",
      "iteration 1800 loss 269.3836\n",
      "iteration 1900 loss 269.3833\n",
      "iteration 2000 loss 269.383\n",
      "iteration 2100 loss 269.38266\n",
      "iteration 2200 loss 269.38242\n",
      "iteration 2300 loss 269.38214\n",
      "iteration 2400 loss 269.38184\n",
      "iteration 2500 loss 269.3816\n",
      "iteration 2600 loss 269.38132\n",
      "iteration 2700 loss 269.38107\n",
      "iteration 2800 loss 269.38086\n",
      "iteration 2900 loss 269.3806\n",
      "iteration 3000 loss 269.38037\n",
      "iteration 3100 loss 269.3802\n",
      "iteration 3200 loss 269.37994\n",
      "iteration 3300 loss 269.37976\n",
      "iteration 3400 loss 269.37955\n",
      "iteration 3500 loss 269.37933\n",
      "iteration 3600 loss 269.37915\n",
      "iteration 3700 loss 269.3789\n",
      "iteration 3800 loss 269.37875\n",
      "iteration 3900 loss 269.3786\n",
      "iteration 4000 loss 269.37845\n",
      "iteration 4100 loss 269.3783\n",
      "iteration 4200 loss 269.3781\n",
      "iteration 4300 loss 269.37793\n",
      "iteration 4400 loss 269.37778\n",
      "iteration 4500 loss 269.37762\n",
      "iteration 4600 loss 269.3775\n",
      "iteration 4700 loss 269.37738\n",
      "iteration 4800 loss 269.37723\n",
      "iteration 4900 loss 269.37708\n",
      "iteration 5000 loss 269.37695Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         1.         1.         0.99951315 1.         1.\n",
      " 0.9999536  1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.9998992  1.         1.         1.         1.         0.997446\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         0.99924356 1.         1.         1.         1.\n",
      " 0.7412621  1.         1.         1.         1.         1.\n",
      " 1.         0.9998551  1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.9989912  1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.9999975\n",
      " 1.         0.99965763 1.         1.         0.9998162 ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00011505568886010896, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 31.515822999708934\n",
      "\n",
      " Fold 1:\n",
      "\n",
      "iteration 100 loss 280.86475\n",
      "iteration 200 loss 280.83322\n",
      "iteration 300 loss 280.80383\n",
      "iteration 400 loss 280.776\n",
      "iteration 500 loss 280.7492\n",
      "iteration 600 loss 280.723\n",
      "iteration 700 loss 280.69675\n",
      "iteration 800 loss 280.66986\n",
      "iteration 900 loss 280.64194\n",
      "iteration 1000 loss 280.61194\n",
      "iteration 1100 loss 280.57925\n",
      "iteration 1200 loss 280.54333\n",
      "iteration 1300 loss 280.50427\n",
      "iteration 1400 loss 280.46246\n",
      "iteration 1500 loss 280.41895\n",
      "iteration 1600 loss 280.37463\n",
      "iteration 1700 loss 280.33002\n",
      "iteration 1800 loss 280.28513\n",
      "iteration 1900 loss 280.23935\n",
      "iteration 2000 loss 280.19196\n",
      "iteration 2100 loss 280.1421\n",
      "iteration 2200 loss 280.08893\n",
      "iteration 2300 loss 280.03162\n",
      "iteration 2400 loss 279.9693\n",
      "iteration 2500 loss 279.9011\n",
      "iteration 2600 loss 279.82602\n",
      "iteration 2700 loss 279.7429\n",
      "iteration 2800 loss 279.6501\n",
      "iteration 2900 loss 279.54578\n",
      "iteration 3000 loss 279.4276\n",
      "iteration 3100 loss 279.3169\n",
      "iteration 3200 loss 279.21313\n",
      "iteration 3300 loss 279.0979\n",
      "iteration 3400 loss 278.9928\n",
      "iteration 3500 loss 278.9394\n",
      "iteration 3600 loss 278.88345\n",
      "iteration 3700 loss 278.82483\n",
      "iteration 3800 loss 278.76324\n",
      "iteration 3900 loss 278.6988\n",
      "iteration 4000 loss 278.6313\n",
      "iteration 4100 loss 278.59848\n",
      "iteration 4200 loss 278.57367\n",
      "iteration 4300 loss 278.54874\n",
      "iteration 4400 loss 278.52368\n",
      "iteration 4500 loss 278.49838\n",
      "iteration 4600 loss 278.4729\n",
      "iteration 4700 loss 278.44702\n",
      "iteration 4800 loss 278.42072\n",
      "iteration 4900 loss 278.39404\n",
      "iteration 5000 loss 278.36908Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         1.         1.         1.         0.99880105 0.99958307\n",
      " 0.9909995  1.         0.         0.54173094 0.         1.\n",
      " 0.998644   1.         0.21501558 1.         0.9975075  1.\n",
      " 1.         0.9988457  1.         0.999884   0.9956742  0.9195208\n",
      " 1.         0.9963483  0.36176145 1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.6950309  1.         0.9995478  1.         0.         1.\n",
      " 1.         0.99273914 1.         1.         0.         1.\n",
      " 1.         0.95145696 0.997434   1.         1.         0.96389264\n",
      " 1.         0.9953955  1.         1.         1.         0.97230226\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.9649062  1.         1.         0.28803197 0.         0.9789379\n",
      " 0.9915531  0.99789435 0.99943966 1.         1.        ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00011533252450328686, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 22.783887374204056\n",
      "\n",
      " Fold 2:\n",
      "\n",
      "iteration 100 loss 191.56827\n",
      "iteration 200 loss 191.37271\n",
      "iteration 300 loss 191.22388\n",
      "iteration 400 loss 191.08173\n",
      "iteration 500 loss 190.91211\n",
      "iteration 600 loss 190.69147\n",
      "iteration 700 loss 190.40324\n",
      "iteration 800 loss 190.03195\n",
      "iteration 900 loss 189.55614\n",
      "iteration 1000 loss 188.94022\n",
      "iteration 1100 loss 188.13196\n",
      "iteration 1200 loss 187.20169\n",
      "iteration 1300 loss 186.7999\n",
      "iteration 1400 loss 186.4802\n",
      "iteration 1500 loss 186.19043\n",
      "iteration 1600 loss 185.89574\n",
      "iteration 1700 loss 185.59784\n",
      "iteration 1800 loss 185.29843\n",
      "iteration 1900 loss 185.02988\n",
      "iteration 2000 loss 184.90108\n",
      "iteration 2100 loss 184.82262\n",
      "iteration 2200 loss 184.77806\n",
      "iteration 2300 loss 184.7341\n",
      "iteration 2400 loss 184.69075\n",
      "iteration 2500 loss 184.64806\n",
      "iteration 2600 loss 184.60605\n",
      "iteration 2700 loss 184.56471\n",
      "iteration 2800 loss 184.5241\n",
      "iteration 2900 loss 184.48416\n",
      "iteration 3000 loss 184.44641\n",
      "iteration 3100 loss 184.42554\n",
      "iteration 3200 loss 184.40521\n",
      "iteration 3300 loss 184.3853\n",
      "iteration 3400 loss 184.36588\n",
      "iteration 3500 loss 184.34685\n",
      "iteration 3600 loss 184.32834\n",
      "iteration 3700 loss 184.31027\n",
      "iteration 3800 loss 184.2926\n",
      "iteration 3900 loss 184.27536\n",
      "iteration 4000 loss 184.26355\n",
      "iteration 4100 loss 184.26065\n",
      "iteration 4200 loss 184.25772\n",
      "iteration 4300 loss 184.25485\n",
      "iteration 4400 loss 184.25201\n",
      "iteration 4500 loss 184.24915\n",
      "iteration 4600 loss 184.2463\n",
      "iteration 4700 loss 184.2435\n",
      "iteration 4800 loss 184.24069\n",
      "iteration 4900 loss 184.23787\n",
      "iteration 5000 loss 184.23508Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [0.98516434 1.         1.         1.         1.         1.\n",
      " 1.         0.1421134  0.363202   0.         0.         1.\n",
      " 1.         1.         1.         1.         0.99137115 1.\n",
      " 1.         0.99407023 1.         0.99977815 0.9829598  0.93773246\n",
      " 0.8934907  1.         0.         1.         0.99785334 1.\n",
      " 1.         0.9646307  1.         1.         1.         1.\n",
      " 0.         1.         1.         1.         0.         1.\n",
      " 1.         0.9627955  1.         0.97303426 1.         1.\n",
      " 0.         1.         0.9879517  0.         1.         1.\n",
      " 0.         0.7924448  1.         1.         0.6338869  1.\n",
      " 1.         1.         1.         1.         0.8664157  1.\n",
      " 0.8654963  1.         1.         0.         0.35815707 0.9081287\n",
      " 0.992328   0.97841513 0.99360216 1.         0.782845  ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00012204660694218252, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 60.38781595032699\n",
      "\n",
      " Fold 3:\n",
      "\n",
      "iteration 100 loss 311.56906\n",
      "iteration 200 loss 311.56805\n",
      "iteration 300 loss 311.56714\n",
      "iteration 400 loss 311.56622\n",
      "iteration 500 loss 311.56543\n",
      "iteration 600 loss 311.56467\n",
      "iteration 700 loss 311.56393\n",
      "iteration 800 loss 311.56323\n",
      "iteration 900 loss 311.56256\n",
      "iteration 1000 loss 311.56192\n",
      "iteration 1100 loss 311.56134\n",
      "iteration 1200 loss 311.5608\n",
      "iteration 1300 loss 311.56024\n",
      "iteration 1400 loss 311.55975\n",
      "iteration 1500 loss 311.5593\n",
      "iteration 1600 loss 311.55884\n",
      "iteration 1700 loss 311.5584\n",
      "iteration 1800 loss 311.558\n",
      "iteration 1900 loss 311.55765\n",
      "iteration 2000 loss 311.55725\n",
      "iteration 2100 loss 311.55695\n",
      "iteration 2200 loss 311.55658\n",
      "iteration 2300 loss 311.55627\n",
      "iteration 2400 loss 311.55597\n",
      "iteration 2500 loss 311.55573\n",
      "iteration 2600 loss 311.55545\n",
      "iteration 2700 loss 311.55515\n",
      "iteration 2800 loss 311.5549\n",
      "iteration 2900 loss 311.55472\n",
      "iteration 3000 loss 311.55444\n",
      "iteration 3100 loss 311.55426\n",
      "iteration 3200 loss 311.55402\n",
      "iteration 3300 loss 311.55386\n",
      "iteration 3400 loss 311.55365\n",
      "iteration 3500 loss 311.5535\n",
      "iteration 3600 loss 311.55328\n",
      "iteration 3700 loss 311.55313\n",
      "iteration 3800 loss 311.55298\n",
      "iteration 3900 loss 311.5528\n",
      "iteration 4000 loss 311.5526\n",
      "iteration 4100 loss 311.55252\n",
      "iteration 4200 loss 311.55237\n",
      "iteration 4300 loss 311.55225\n",
      "iteration 4400 loss 311.55203\n",
      "iteration 4500 loss 311.55194\n",
      "iteration 4600 loss 311.55182\n",
      "iteration 4700 loss 311.55167\n",
      "iteration 4800 loss 311.55157\n",
      "iteration 4900 loss 311.55145\n",
      "iteration 5000 loss 311.55133Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [1.         1.         1.         0.9999651  1.         1.\n",
      " 0.9991406  1.         0.84662014 1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.9992084\n",
      " 0.9993537  1.         1.         1.         1.         0.9876992\n",
      " 1.         0.99987495 1.         1.         1.         1.\n",
      " 1.         1.         0.999328   1.         1.         1.\n",
      " 0.8553921  1.         1.         1.         1.         1.\n",
      " 1.         1.         0.9957688  1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         0.9995754\n",
      " 0.80950075 1.         1.         1.         1.         0.9979257\n",
      " 1.         1.         1.         1.         0.99930996 0.99965024\n",
      " 1.         1.         1.         1.         1.         0.9993553\n",
      " 0.99965364 0.9995493  1.         1.         1.        ]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 0.00013109857736756775, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 10.884689145903426\n",
      "\n",
      " Fold 4:\n",
      "\n",
      "iteration 100 loss 296.41364\n",
      "iteration 200 loss 296.40955\n",
      "iteration 300 loss 296.4049\n",
      "iteration 400 loss 296.39972\n",
      "iteration 500 loss 296.394\n",
      "iteration 600 loss 296.38782\n",
      "iteration 700 loss 296.38132\n",
      "iteration 800 loss 296.37457\n",
      "iteration 900 loss 296.36774\n",
      "iteration 1000 loss 296.36093\n",
      "iteration 1100 loss 296.35413\n",
      "iteration 1200 loss 296.34747\n",
      "iteration 1300 loss 296.341\n",
      "iteration 1400 loss 296.33466\n",
      "iteration 1500 loss 296.3285\n",
      "iteration 1600 loss 296.32248\n",
      "iteration 1700 loss 296.31653\n",
      "iteration 1800 loss 296.31058\n",
      "iteration 1900 loss 296.30457\n",
      "iteration 2000 loss 296.29846\n",
      "iteration 2100 loss 296.29218\n",
      "iteration 2200 loss 296.28564\n",
      "iteration 2300 loss 296.2789\n",
      "iteration 2400 loss 296.27173\n",
      "iteration 2500 loss 296.26422\n",
      "iteration 2600 loss 296.25638\n",
      "iteration 2700 loss 296.248\n",
      "iteration 2800 loss 296.23907\n",
      "iteration 2900 loss 296.22955\n",
      "iteration 3000 loss 296.21936\n",
      "iteration 3100 loss 296.2086\n",
      "iteration 3200 loss 296.19702\n",
      "iteration 3300 loss 296.18472\n",
      "iteration 3400 loss 296.17163\n",
      "iteration 3500 loss 296.15747\n",
      "iteration 3600 loss 296.14255\n",
      "iteration 3700 loss 296.12653\n",
      "iteration 3800 loss 296.10944\n",
      "iteration 3900 loss 296.09116\n",
      "iteration 4000 loss 296.07172\n",
      "iteration 4100 loss 296.05106\n",
      "iteration 4200 loss 296.02905\n",
      "iteration 4300 loss 296.00568\n",
      "iteration 4400 loss 295.98083\n",
      "iteration 4500 loss 295.95456\n",
      "iteration 4600 loss 295.92676\n",
      "iteration 4700 loss 295.8973\n",
      "iteration 4800 loss 295.86624\n",
      "iteration 4900 loss 295.83362\n",
      "iteration 5000 loss 295.79932Train finished\n",
      "Selected features are: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76]\n",
      "Selected weights are: [0.9956216  1.         0.907941   0.9985363  0.9989561  1.\n",
      " 1.         1.         1.         0.95218587 1.         0.60999846\n",
      " 0.999833   1.         1.         1.         0.99817145 1.\n",
      " 1.         0.9998443  1.         0.9999126  0.9983837  0.90803343\n",
      " 0.92704886 1.         1.         1.         0.9987007  1.\n",
      " 1.         0.995715   1.         1.         0.99993104 0.9934331\n",
      " 0.39149696 1.         0.999859   1.         1.         1.\n",
      " 1.         1.         1.         0.92494637 1.         1.\n",
      " 1.         1.         0.99968415 0.2856518  1.         1.\n",
      " 0.00376954 0.98845226 1.         1.         0.7185153  1.\n",
      " 0.9471716  0.9916599  1.         0.8718225  0.986197   0.9991895\n",
      " 0.9961807  1.         0.9980735  0.6703944  1.         0.9957417\n",
      " 1.         0.99997675 0.9987278  0.9997879  0.95751816]\n",
      "Best params obtained by cv: {'alpha': 100000.0, 'coef0': 1, 'degree': 3, 'gamma': 8.133327491744657e-05, 'kernel': 'rbf', 'kernel_params': None}\n",
      "Score at this fold: 18.7859712216978\n",
      "29.29507497131206 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m in [3, 5, 10, 77]:\n",
    "    cv_score(X, Y - Y.mean(0), \"real-valued\", num_features=m, model_name=\"krr\", savepath=\"bmi_final\",\n",
    "             epsilon = 0.1, learning_rate = 0.001, \n",
    "             iterations = 5000, verbose=True,\n",
    "             outer_folds = 5, inner_folds = 5, N_TRIALS=10,\n",
    "             do_alpha=True, no_gamma=False, radial=True, oneSE=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06e1773e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.90191912108356 0.053039353626440376\n",
      "Selected features are: [15 40 48]\n",
      "28.873559945159844 0.037140273387692165\n",
      "Selected features are: [14 15 40 48 58]\n",
      "28.98394520611011 0.11514238873274053\n",
      "Selected features are: [ 9 14 15 40 48 54 56 58 63 69]\n",
      "29.29507497131206 0.29381417600100346\n"
     ]
    }
   ],
   "source": [
    "selections = dict()\n",
    "for m in [3, 5, 10, 77]:\n",
    "    with open(\"./results/real_data/bmi_final/krr_neg_mean_squared_error_{}_1seTrue_radial.pickle\".format(m), \"rb\") as fi:\n",
    "        score, selection = pickle.load(fi)\n",
    "    score_cv = np.mean(score, axis=1)\n",
    "    print(np.mean(score_cv), np.std(score_cv)/np.sqrt(10))\n",
    "    if m == 77:\n",
    "        break\n",
    "    selections[m] = selection_wrapper(selection, 77)\n",
    "    # m = 77 (dim of X) is for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73d26a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 variables\n",
      "\n",
      " 0          OTU_16            OTU_41            OTU_49\n",
      "1        Bacteria          Bacteria          Bacteria\n",
      "2   Bacteroidetes        Firmicutes        Firmicutes\n",
      "3     Bacteroidia        Clostridia        Clostridia\n",
      "4   Bacteroidales     Clostridiales     Clostridiales\n",
      "5   Rikenellaceae   Lachnospiraceae   Ruminococcaceae\n",
      "6       Alistipes             Dorea     Anaerotruncus \n",
      "\n",
      "5 variables\n",
      "\n",
      " 0           OTU_15          OTU_16            OTU_41            OTU_49  \\\n",
      "1         Bacteria        Bacteria          Bacteria          Bacteria   \n",
      "2    Bacteroidetes   Bacteroidetes        Firmicutes        Firmicutes   \n",
      "3      Bacteroidia     Bacteroidia        Clostridia        Clostridia   \n",
      "4    Bacteroidales   Bacteroidales     Clostridiales     Clostridiales   \n",
      "5   Prevotellaceae   Rikenellaceae   Lachnospiraceae   Ruminococcaceae   \n",
      "6       Prevotella       Alistipes             Dorea     Anaerotruncus   \n",
      "\n",
      "0            OTU_59  \n",
      "1          Bacteria  \n",
      "2        Firmicutes  \n",
      "3        Clostridia  \n",
      "4     Clostridiales  \n",
      "5   Veillonellaceae  \n",
      "6         Dialister   \n",
      "\n",
      "10 variables\n",
      "\n",
      " 0               OTU_10           OTU_15          OTU_16            OTU_41  \\\n",
      "1             Bacteria         Bacteria        Bacteria          Bacteria   \n",
      "2        Bacteroidetes    Bacteroidetes   Bacteroidetes        Firmicutes   \n",
      "3          Bacteroidia      Bacteroidia     Bacteroidia        Clostridia   \n",
      "4        Bacteroidales    Bacteroidales   Bacteroidales     Clostridiales   \n",
      "5   Porphyromonadaceae   Prevotellaceae   Rikenellaceae   Lachnospiraceae   \n",
      "6        Butyricimonas       Prevotella       Alistipes             Dorea   \n",
      "\n",
      "0            OTU_49            OTU_55            OTU_57            OTU_59  \\\n",
      "1          Bacteria          Bacteria          Bacteria          Bacteria   \n",
      "2        Firmicutes        Firmicutes        Firmicutes        Firmicutes   \n",
      "3        Clostridia        Clostridia        Clostridia        Clostridia   \n",
      "4     Clostridiales     Clostridiales     Clostridiales     Clostridiales   \n",
      "5   Ruminococcaceae   Ruminococcaceae   Veillonellaceae   Veillonellaceae   \n",
      "6     Anaerotruncus       Sporobacter   Acidaminococcus         Dialister   \n",
      "\n",
      "0             OTU_64                OTU_70  \n",
      "1           Bacteria              Bacteria  \n",
      "2         Firmicutes            Firmicutes  \n",
      "3         Clostridia       Erysipelotrichi  \n",
      "4      Clostridiales    Erysipelotrichales  \n",
      "5    Veillonellaceae   Erysipelotrichaceae  \n",
      "6   Succiniclasticum         Solobacterium   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m in [3, 5, 10]:\n",
    "    print(\"{} variables\\n\\n\".format(m), tax_table.iloc[:, selections[m]['Selected_features']], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13132d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14461f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_score = list()\n",
    "for m in [3, 5, 10, 77]:\n",
    "    with open(\"./results/real_data/bmi_rm/krr_neg_mean_squared_error_{}_1seTrue_radial.pickle\".format(m), \"rb\") as fi:\n",
    "        score, selection = pickle.load(fi)\n",
    "\n",
    "    score = cv_with_selection(X, Y - Y.mean(0), 'real-valued', selection, savepath=\"bmi3\",\n",
    "                              proj_method = 'compo', model_name='krr',\n",
    "                              inner_folds=5, do_alpha=True, no_gamma=False, \n",
    "                              radial=True, oneSE=True)\n",
    "    new_score.append(score)\n",
    "new_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73f5c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.90191912108356 0.053039353626440376 (10,)\n",
      "28.873559945159844 0.037140273387692165 (10,)\n",
      "28.98394520611011 0.11514238873274053 (10,)\n",
      "29.29507497131206 0.29381417600100346 (10,)\n"
     ]
    }
   ],
   "source": [
    "for m in [3, 5, 10, 77]:\n",
    "    with open(\"./results/real_data/bmi3/krr_neg_mean_squared_error_{}_1seTrue_radial.pickle\".format(m), \"rb\") as fi:\n",
    "        score, selection = pickle.load(fi)\n",
    "    score_cv = np.mean(score, axis=1)\n",
    "    print(np.mean(score_cv), np.std(score_cv)/np.sqrt(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec6dd62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e7d5dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "699235fc",
   "metadata": {},
   "source": [
    "## Coda-lasso experiments with the c-lasso module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "206e4df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classo import classo_problem\n",
    "  \n",
    "def learn(Z, Y, C=None, label=None, lam=\"theoretical\"):\n",
    "    # Z: log-transformed compositional data\n",
    "    # Default C: zero sum constraint\n",
    "    problem = classo_problem(Z, Y, C, label=label)\n",
    "    problem.formulation.concomitant = False\n",
    "    problem.formulation.huber = False\n",
    "    problem.formulation.intercept = True\n",
    "\n",
    "    problem.model_selection.CV = False\n",
    "    problem.model_selection.ALO = False\n",
    "    problem.model_selection.StabSel = False\n",
    "\n",
    "    problem.model_selection.LAMfixed = True\n",
    "    problem.model_selection.LAMfixedparameters.lam = lam\n",
    "    problem.solve()\n",
    "    \n",
    "    return problem\n",
    "\n",
    "\n",
    "def predict(model, X_new):\n",
    "    beta = model.solution.LAMfixed.beta\n",
    "    X_new = np.c_[np.ones(len(X_new)), X_new]\n",
    "    return X_new @ beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f84bd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_lasso_regression(X, Y, savepath='bmi_final', C=None, label=None, lamseq=np.geomspace(1e-3, 1, 100),\n",
    "                   outer_folds = 5, inner_folds = 5, oneSE=False, N_TRIALS = 10, **krgs,\n",
    "                   ):\n",
    "    '''\n",
    "    Input X should be zero-replaced count or compositional data\n",
    "    label=None is almost always better.. it returns indices\n",
    "    '''\n",
    "    assert (X > 1e-10).all(), \"Replace zeros or check the data type\"\n",
    "    X = np.log(X)\n",
    "    \n",
    "    if label is None:\n",
    "        label = np.arange(X.shape[1])\n",
    "\n",
    "    score_all = list()\n",
    "    selection_all = list()\n",
    "    \n",
    "    for seed_ in range(1, N_TRIALS + 1):\n",
    "        print(\"\\n\", seed_, \"th cv experiment...\")\n",
    "\n",
    "        # Outer cv folds\n",
    "        Outer_cv = KFold(n_splits=outer_folds, shuffle=True, random_state=seed_)\n",
    "        score = list()\n",
    "        selection = list()\n",
    "        \n",
    "        # Outer cv loop\n",
    "        for i, (train_idx, test_idx) in enumerate(Outer_cv.split(X)):\n",
    "            print(f\"\\n Fold {i}:\")\n",
    "            X_train, Y_train = X[train_idx], Y[train_idx]\n",
    "            X_test, Y_test = X[test_idx], Y[test_idx]\n",
    "\n",
    "            if inner_folds is None:\n",
    "                raise Exception(\"Not yet developed\")\n",
    "            else:\n",
    "                # Do inner cv on X_train to choose lambda parameter\n",
    "                inner_cv = KFold(n_splits=inner_folds, shuffle=True, random_state=seed_ * N_TRIALS + i)\n",
    "                cv_scores = np.zeros((len(lamseq), inner_folds))\n",
    "                \n",
    "                for j, (tr, val_idx) in enumerate(inner_cv.split(X_train)):\n",
    "                    for k in range(len(lamseq)):\n",
    "                        lam = lamseq[k]\n",
    "                        model = learn(X_train[tr], Y_train[tr], lam=lam)\n",
    "                        cv_scores[k, j] = mean_squared_error(Y_train[val_idx],\n",
    "                                                            predict(model, X_train[val_idx]))\n",
    "                \n",
    "                # Parameter selection based on inner_cv result\n",
    "                cv_scores_collected = cv_scores.mean(axis=1)\n",
    "                \n",
    "                if oneSE:\n",
    "                    # Take maximal lambda among the [min, min+1se] scores\n",
    "                    min_plus_1se = np.min(cv_scores_collected) + np.std(cv_scores_collected) / np.sqrt(len(cv_scores_collected))\n",
    "                    mask = cv_scores_collected < min_plus_1se\n",
    "                    lamb = lamseq[mask][-1]\n",
    "                    # print(\"models:\", lamseq[mask])\n",
    "                else:\n",
    "                    # find lambda \"minimizing\" cv-mse\n",
    "                    lamb = lamseq[np.argmin(cv_scores_collected)]\n",
    "                # Refit\n",
    "                refitted_model = learn(X_train, Y_train, label=label, lam=lamb)\n",
    "                \n",
    "            # Get score\n",
    "            refitted_score = mean_squared_error(Y_test,\n",
    "                                                predict(refitted_model, X_test))\n",
    "            selected_feats = label[refitted_model.solution.LAMfixed.selected_param[1:]]\n",
    "            print(\"CV lambda, refitted score:\", lamb, ',', refitted_score)\n",
    "            print(\"Selected feats:\", selected_feats)\n",
    "            score.append(refitted_score)\n",
    "            selection.append(selected_feats)\n",
    "        \n",
    "        # Wrap up the result of _ th iteration\n",
    "        score_all.append(score)\n",
    "        selection_all.append(selection)\n",
    "    \n",
    "    print(np.mean(score_all), \"\\n\")\n",
    "    \n",
    "    # Save\n",
    "    with open(\"./results/real_data/{}/cvlasso_inner{}_1se{}.pickle\".format(savepath, inner_folds, oneSE), 'wb') as f:\n",
    "        pickle.dump((score_all, selection_all), f)\n",
    "    return score_all\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46f93d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a0e901a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "CV lambda, refitted score: 0.49770235643321137 , 40.55090725999977\n",
      "Selected feats: [15 52]\n",
      "\n",
      " Fold 1:\n",
      "CV lambda, refitted score: 0.21544346900318845 , 7.730618565570331\n",
      "Selected feats: [24 52 53]\n",
      "\n",
      " Fold 2:\n",
      "CV lambda, refitted score: 0.26560877829466867 , 38.15574175852951\n",
      "Selected feats: [15 52]\n",
      "\n",
      " Fold 3:\n",
      "CV lambda, refitted score: 0.23101297000831603 , 41.76653427968882\n",
      "Selected feats: [24 52 53]\n",
      "\n",
      " Fold 4:\n",
      "CV lambda, refitted score: 0.23101297000831603 , 15.401680541540708\n",
      "Selected feats: [24 48 52 62]\n",
      "\n",
      " 2 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "CV lambda, refitted score: 0.8697490026177834 , 40.08073394600363\n",
      "Selected feats: []\n",
      "\n",
      " Fold 1:\n",
      "CV lambda, refitted score: 0.3511191734215131 , 21.214318155627268\n",
      "Selected feats: [15 52]\n",
      "\n",
      " Fold 2:\n",
      "CV lambda, refitted score: 0.37649358067924676 , 30.63558627459497\n",
      "Selected feats: [24 52]\n",
      "\n",
      " Fold 3:\n",
      "CV lambda, refitted score: 0.2009233002565048 , 26.0703556539422\n",
      "Selected feats: [15 24 37 52 53]\n",
      "\n",
      " Fold 4:\n",
      "CV lambda, refitted score: 0.2009233002565048 , 32.146656298324494\n",
      "Selected feats: [48 52 53 62]\n",
      "\n",
      " 3 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "CV lambda, refitted score: 0.1519911082952933 , 28.282221749587563\n",
      "Selected feats: [24 37 48 52 55 62]\n",
      "\n",
      " Fold 1:\n",
      "CV lambda, refitted score: 0.37649358067924676 , 40.728216196633404\n",
      "Selected feats: [52]\n",
      "\n",
      " Fold 2:\n",
      "CV lambda, refitted score: 0.21544346900318845 , 14.230316445254008\n",
      "Selected feats: [15 52 55]\n",
      "\n",
      " Fold 3:\n",
      "CV lambda, refitted score: 0.40370172585965536 , 17.850762211088437\n",
      "Selected feats: [15 52]\n",
      "\n",
      " Fold 4:\n",
      "CV lambda, refitted score: 1.0 , 52.77894810313268\n",
      "Selected feats: []\n",
      "\n",
      " 4 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "CV lambda, refitted score: 0.8697490026177834 , 49.79001823285363\n",
      "Selected feats: []\n",
      "\n",
      " Fold 1:\n",
      "CV lambda, refitted score: 0.4641588833612782 , 15.07130757217668\n",
      "Selected feats: [52]\n",
      "\n",
      " Fold 2:\n",
      "CV lambda, refitted score: 0.10722672220103231 , 28.653685626842048\n",
      "Selected feats: [15 24 48 52 55 62 66]\n",
      "\n",
      " Fold 3:\n",
      "CV lambda, refitted score: 0.1873817422860385 , 20.175023945668777\n",
      "Selected feats: [15 24 52 55 62]\n",
      "\n",
      " Fold 4:\n",
      "CV lambda, refitted score: 0.6135907273413176 , 33.205098292361704\n",
      "Selected feats: []\n",
      "\n",
      " 5 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "CV lambda, refitted score: 0.3511191734215131 , 26.090993274349234\n",
      "Selected feats: [52]\n",
      "\n",
      " Fold 1:\n",
      "CV lambda, refitted score: 0.1 , 21.814217952070738\n",
      "Selected feats: [15 24 49 52 53 55 61]\n",
      "\n",
      " Fold 2:\n",
      "CV lambda, refitted score: 1.0 , 51.68432325426316\n",
      "Selected feats: []\n",
      "\n",
      " Fold 3:\n",
      "CV lambda, refitted score: 0.32745491628777285 , 26.72120509735529\n",
      "Selected feats: [24 52]\n",
      "\n",
      " Fold 4:\n",
      "CV lambda, refitted score: 0.40370172585965536 , 24.030108899491378\n",
      "Selected feats: [52]\n",
      "\n",
      " 6 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "CV lambda, refitted score: 0.8697490026177834 , 45.25825292448061\n",
      "Selected feats: []\n",
      "\n",
      " Fold 1:\n",
      "CV lambda, refitted score: 0.24770763559917114 , 31.142228009845894\n",
      "Selected feats: [24 52 53 55 62]\n",
      "\n",
      " Fold 2:\n",
      "CV lambda, refitted score: 0.2009233002565048 , 30.21089605888627\n",
      "Selected feats: [15 24 49 52 53 55]\n",
      "\n",
      " Fold 3:\n",
      "CV lambda, refitted score: 0.3511191734215131 , 9.395862458454548\n",
      "Selected feats: [52]\n",
      "\n",
      " Fold 4:\n",
      "CV lambda, refitted score: 0.30538555088334157 , 24.477913909647977\n",
      "Selected feats: [15 52]\n",
      "\n",
      " 7 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "CV lambda, refitted score: 0.16297508346206452 , 20.54011881103027\n",
      "Selected feats: [15 24 48 49 52 53 55 62]\n",
      "\n",
      " Fold 1:\n",
      "CV lambda, refitted score: 0.4641588833612782 , 27.077764301767797\n",
      "Selected feats: [52]\n",
      "\n",
      " Fold 2:\n",
      "CV lambda, refitted score: 0.3511191734215131 , 33.191871914673676\n",
      "Selected feats: [52]\n",
      "\n",
      " Fold 3:\n",
      "CV lambda, refitted score: 0.1873817422860385 , 29.3684164237305\n",
      "Selected feats: [15 24 37 48 53]\n",
      "\n",
      " Fold 4:\n",
      "CV lambda, refitted score: 0.21544346900318845 , 38.57122906390675\n",
      "Selected feats: [15 24 53 55]\n",
      "\n",
      " 8 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "CV lambda, refitted score: 0.1747528400007685 , 30.3986186390984\n",
      "Selected feats: [15 24 52 53]\n",
      "\n",
      " Fold 1:\n",
      "CV lambda, refitted score: 0.30538555088334157 , 24.75909938042152\n",
      "Selected feats: [48 52]\n",
      "\n",
      " Fold 2:\n",
      "CV lambda, refitted score: 0.6135907273413176 , 43.30045162652417\n",
      "Selected feats: []\n",
      "\n",
      " Fold 3:\n",
      "CV lambda, refitted score: 0.23101297000831603 , 27.23751183540398\n",
      "Selected feats: [15 24 52 53 55 62]\n",
      "\n",
      " Fold 4:\n",
      "CV lambda, refitted score: 1.0 , 18.05856411674996\n",
      "Selected feats: []\n",
      "\n",
      " 9 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "CV lambda, refitted score: 0.16297508346206452 , 21.012251813613403\n",
      "Selected feats: [15 24 52 53]\n",
      "\n",
      " Fold 1:\n",
      "CV lambda, refitted score: 0.40370172585965536 , 50.240664311508965\n",
      "Selected feats: [52]\n",
      "\n",
      " Fold 2:\n",
      "CV lambda, refitted score: 0.2848035868435802 , 33.237956779375025\n",
      "Selected feats: [15 37 52]\n",
      "\n",
      " Fold 3:\n",
      "CV lambda, refitted score: 0.4641588833612782 , 14.417909033007579\n",
      "Selected feats: [52]\n",
      "\n",
      " Fold 4:\n",
      "CV lambda, refitted score: 0.37649358067924676 , 18.895486377292038\n",
      "Selected feats: [52]\n",
      "\n",
      " 10 th cv experiment...\n",
      "\n",
      " Fold 0:\n",
      "CV lambda, refitted score: 0.8111308307896873 , 31.73099124446215\n",
      "Selected feats: []\n",
      "\n",
      " Fold 1:\n",
      "CV lambda, refitted score: 0.2848035868435802 , 21.18219974909895\n",
      "Selected feats: [24 52 56]\n",
      "\n",
      " Fold 2:\n",
      "CV lambda, refitted score: 0.16297508346206452 , 55.862975019020936\n",
      "Selected feats: [15 24 52 55 56]\n",
      "\n",
      " Fold 3:\n",
      "CV lambda, refitted score: 0.1873817422860385 , 20.228996150878697\n",
      "Selected feats: [15 24 37 48 52 62]\n",
      "\n",
      " Fold 4:\n",
      "CV lambda, refitted score: 0.8111308307896873 , 19.774002450630697\n",
      "Selected feats: []\n",
      "29.28863623980922 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[40.55090725999977,\n",
       "  7.730618565570331,\n",
       "  38.15574175852951,\n",
       "  41.76653427968882,\n",
       "  15.401680541540708],\n",
       " [40.08073394600363,\n",
       "  21.214318155627268,\n",
       "  30.63558627459497,\n",
       "  26.0703556539422,\n",
       "  32.146656298324494],\n",
       " [28.282221749587563,\n",
       "  40.728216196633404,\n",
       "  14.230316445254008,\n",
       "  17.850762211088437,\n",
       "  52.77894810313268],\n",
       " [49.79001823285363,\n",
       "  15.07130757217668,\n",
       "  28.653685626842048,\n",
       "  20.175023945668777,\n",
       "  33.205098292361704],\n",
       " [26.090993274349234,\n",
       "  21.814217952070738,\n",
       "  51.68432325426316,\n",
       "  26.72120509735529,\n",
       "  24.030108899491378],\n",
       " [45.25825292448061,\n",
       "  31.142228009845894,\n",
       "  30.21089605888627,\n",
       "  9.395862458454548,\n",
       "  24.477913909647977],\n",
       " [20.54011881103027,\n",
       "  27.077764301767797,\n",
       "  33.191871914673676,\n",
       "  29.3684164237305,\n",
       "  38.57122906390675],\n",
       " [30.3986186390984,\n",
       "  24.75909938042152,\n",
       "  43.30045162652417,\n",
       "  27.23751183540398,\n",
       "  18.05856411674996],\n",
       " [21.012251813613403,\n",
       "  50.240664311508965,\n",
       "  33.237956779375025,\n",
       "  14.417909033007579,\n",
       "  18.895486377292038],\n",
       " [31.73099124446215,\n",
       "  21.18219974909895,\n",
       "  55.862975019020936,\n",
       "  20.228996150878697,\n",
       "  19.774002450630697]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.5min replacement\n",
    "X_replaced = zero_replacement(X, method='min', val=1/2)\n",
    "cv_lasso_regression(X_replaced, Y, savepath='bmi_final', label=None, inner_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1393845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29.28863623980923, 0.2969653109467116)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./results/real_data/{}/cvlasso_inner{}_1se{}_0.5min.pickle\".format(\"bmi_final\", 5, False), 'rb') as fi:\n",
    "    score, selection = pickle.load(fi)\n",
    "mse_through_repeat = np.mean(score, axis=1)\n",
    "mse_through_repeat.mean(), mse_through_repeat.std() / np.sqrt(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30ba3e7",
   "metadata": {},
   "source": [
    "As shown below, applying 1se rule for CV returns almost no selection but intercept (lasso isn't prone to overfitting)  \n",
    "We only record CV result without applying 1se rule for lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8770fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1se applied\n",
    "X_replaced = zero_replacement(X, method='min', val=1/2)\n",
    "cv_lasso_regression(X_replaced, Y, savepath='bmi_final', label=None, inner_folds=5, oneSE=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ea96692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29.58014152071525, 0.16147063989710317)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./results/real_data/{}/cvlasso_inner{}_1se{}_0.5min.pickle\".format(\"bmi_final\", 5, True), 'rb') as fi:\n",
    "    score, selection = pickle.load(fi)\n",
    "mse_through_repeat = np.mean(score, axis=1)\n",
    "mse_through_repeat.mean(), mse_through_repeat.std() / np.sqrt(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7cf35621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([], dtype=int64),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64)],\n",
       " [array([], dtype=int64),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64)],\n",
       " [array([24, 48, 52]),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64)],\n",
       " [array([], dtype=int64),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64)],\n",
       " [array([], dtype=int64),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64)],\n",
       " [array([], dtype=int64),\n",
       "  array([24, 56]),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64)],\n",
       " [array([], dtype=int64),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64)],\n",
       " [array([], dtype=int64),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64)],\n",
       " [array([], dtype=int64),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64)],\n",
       " [array([], dtype=int64),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64),\n",
       "  array([], dtype=int64)]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lasso with 1se rule asserts almost no selection is the best\n",
    "selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6358bf89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2a6a29e",
   "metadata": {},
   "source": [
    "### 1sum replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c99eaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1sum replacement\n",
    "X_replaced = zero_replacement(X, method='sum', val=1.)\n",
    "cv_lasso_regression(X_replaced, Y, savepath='bmi_final', label=None, inner_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3535b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30.516218621834316, 0.37851058876287474)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./results/real_data/{}/cvlasso_inner{}_1se{}_1sum.pickle\".format(\"bmi_final\", 5, False), 'rb') as fi:\n",
    "    score, selection = pickle.load(fi)\n",
    "mse_through_repeat = np.mean(score, axis=1)\n",
    "mse_through_repeat.mean(), mse_through_repeat.std() / np.sqrt(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0bdd62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3506a6f6",
   "metadata": {},
   "source": [
    "### GBM replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72fb1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used R package zCompositions\n",
    "x_gbm = pd.read_csv('./datasets/COMBO_data/complete_data/GeneraCountsGBM.csv')\n",
    "x_gbm = np.array(x_gbm.iloc[:, 1:])\n",
    "cv_lasso_regression(x_gbm, Y, savepath='bmi_final', label=None, inner_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "984aac79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29.052951227592416, 1.391898494377944)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gbm result\n",
    "with open(\"./results/real_data/{}/cvlasso_inner{}_1se{}_gbm.pickle\".format(\"bmi_final\", 5, False), 'rb') as fi:\n",
    "    score, selection = pickle.load(fi)\n",
    "mse_through_repeat = np.mean(score, axis=1)\n",
    "mse_through_repeat.mean(), mse_through_repeat.std() # has to / np.sqrt(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d798a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
